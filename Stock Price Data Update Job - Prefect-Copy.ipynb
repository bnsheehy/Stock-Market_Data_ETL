{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passing-wagon",
   "metadata": {},
   "source": [
    "## Update Stock Prices for each new trading day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlling-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials\n",
    "\n",
    "import json\n",
    "f = open(\"/. .<your file path here> . . /credentials.json\")\n",
    "credentials = json.load(f)\n",
    "\n",
    "file_path = credentials['file_path']\n",
    "intrinio_key = credentials['intrinio_key']\n",
    "aws_key = credentials['aws_access_key']\n",
    "aws_secret_key = credentials['aws_secret_key']\n",
    "rds_host = credentials['rds_host']\n",
    "rds_user = credentials['rds_user']\n",
    "rds_password = credentials['rds_password']\n",
    "rds_database = credentials['rds_database']\n",
    "rds_charset = credentials['rds_charset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Intrinio libraries\n",
    "\n",
    "import time\n",
    "import intrinio_sdk as intrinio\n",
    "from intrinio_sdk.rest import ApiException\n",
    "\n",
    "intrinio.ApiClient().configuration.api_key['api_key'] = intrinio_key\n",
    "\n",
    "# Import Prefect library\n",
    "\n",
    "from prefect.triggers import all_successful, all_failed\n",
    "from prefect import task, Flow\n",
    "import pendulum\n",
    "from prefect.schedules import IntervalSchedule\n",
    "from prefect.schedules.clocks import IntervalClock\n",
    "\n",
    "# Import the usual Python libraries\n",
    "\n",
    "from tqdm.notebook import tqdm, trange  # to be used to track progress in loop iterations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "from datetime import datetime, date, time, timedelta\n",
    "\n",
    "# Import the AWS libraries\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Import SQL libraries\n",
    "\n",
    "import mysql.connector \n",
    "from mysql.connector import errorcode\n",
    "from sqlalchemy import create_engine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "detected-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the last (max) date from the price history column\n",
    "\n",
    "@task\n",
    "def get_max_date():\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    global lastUpdate\n",
    "    global td_days\n",
    "    global todayDate\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT MAX(date) FROM price_data_historical\")\n",
    "\n",
    "    myresult = mycursor.fetchall()[0][0].date()\n",
    "\n",
    "    todayDate = date.today()\n",
    "    lastUpdate = myresult\n",
    "    td = todayDate - lastUpdate\n",
    "    td_days = td.days\n",
    "\n",
    "    print(\"The last day that prices were updated was\", lastUpdate.strftime('%m/%d/%Y'))\n",
    "    print(\"That date was\", td_days, \"days ago.\")\n",
    "\n",
    "    return lastUpdate, td_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "available-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new data for each ticker to append to the price history table\n",
    "\n",
    "@task\n",
    "def get_price_data(lastUpdate, td_days):\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    bad_tickers = []\n",
    "    \n",
    "    global df_prices\n",
    "    global nextDateString\n",
    "    global df_price_update_total\n",
    "    df_price_update_total = pd.DataFrame()\n",
    "\n",
    "    # For each day from the last price update to today, retrieve the new security prices from the Intrinio API.\n",
    "    for updateDate in tqdm(range(1, td_days+1)):\n",
    "\n",
    "        nextDate = lastUpdate + timedelta(updateDate)\n",
    "        nextDateString = nextDate.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        identifier = 'USCOMP'\n",
    "        date = nextDateString\n",
    "        records = 10000\n",
    "        next_page = ''\n",
    "\n",
    "        while next_page != None:\n",
    "\n",
    "            try:\n",
    "\n",
    "                response = intrinio.StockExchangeApi().get_stock_exchange_prices(identifier, date=date, page_size=records, next_page=next_page)\n",
    "                df_prices = pd.DataFrame([x.to_dict() for x in response.stock_prices])\n",
    "\n",
    "                if df_prices.empty:\n",
    "                    print(\"No new prices available for \", nextDate.strftime('%m/%d/%Y'))\n",
    "                    break\n",
    "\n",
    "                df_security = df_prices.security.apply(pd.Series)\n",
    "                df_price_update = pd.concat([df_prices, df_security], axis = 1).drop(['security'], axis = 1)\n",
    "\n",
    "                df_price_update_total = pd.concat([df_price_update_total, df_price_update], ignore_index = True, axis = 0)\n",
    "\n",
    "                next_page = response.next_page\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # If the API returns new prices, drop any duplicates and securities other than stocks, ADRS and ETFs, then\n",
    "    # convert the intraperiod flag to a boolean, rename the adj factor column, set the dates to datetime format\n",
    "    # and reset the column order for uploading to the database.\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "    \n",
    "        df_price_update_total = df_price_update_total.drop_duplicates(subset=['ticker', 'figi', 'date'], keep='last')\n",
    "        df_price_update_total = df_price_update_total[df_price_update_total.code.isin(['EQS', 'DR', 'ETF'])]\n",
    "        df_price_update_total.dropna(subset=['figi'], inplace = True)\n",
    "        df_price_update_total['intraperiod'] = (df_price_update_total['intraperiod'] == 'TRUE').astype(int)\n",
    "        df_price_update_total = df_price_update_total.rename(columns = {'factor':'adj_factor'})\n",
    "        df_price_update_total['date'] = pd.to_datetime(df_price_update_total['date'])\n",
    "        df_price_update_total = df_price_update_total[['ticker', 'figi', 'date', 'open', 'high', 'low', 'close', \n",
    "                                                'volume', 'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume', \n",
    "                                                'adj_factor', 'split_ratio', 'change', 'percent_change', \n",
    "                                                'fifty_two_week_high', 'fifty_two_week_low', 'intraperiod']]\n",
    "\n",
    "        print(\"The initial price update dataframe is retrieved.\")\n",
    "        print(\"The shape of the price update DF is\", df_price_update_total.shape)\n",
    "        print(\"The date range in the update DF goes from \", df_price_update_total.date.min().strftime('%m/%d/%Y'), \" to \", \n",
    "              df_price_update_total.date.max().strftime('%m/%d/%Y'))\n",
    "\n",
    "    return df_price_update_total\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "occupied-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical weighted average diluted shares outstanding for each ticker\n",
    "\n",
    "def get_shares_out(myFigi, myTicker):\n",
    "    \n",
    "    global shares_out_list\n",
    "    global shares_out_lists_combined\n",
    "    \n",
    "    identifier = myFigi\n",
    "    tag = 'adjweightedavedilutedsharesos'\n",
    "    frequency = ''\n",
    "    type = ''\n",
    "    start_date = ''\n",
    "    end_date = ''\n",
    "    sort_order = 'desc'\n",
    "    page_size = 2\n",
    "    next_page = ''\n",
    "\n",
    "    try:\n",
    "        response = intrinio.HistoricalDataApi().get_historical_data(identifier, tag, frequency=frequency, type=type, start_date=start_date, end_date=end_date, sort_order=sort_order, page_size=page_size, next_page=next_page)\n",
    "        shares_out_data = response.historical_data\n",
    "\n",
    "        shares_out_list = []\n",
    "\n",
    "        for item in range(len(shares_out_data)):\n",
    "    \n",
    "            # Add the ticker and figi values to the results\n",
    "            dict_item = shares_out_data[item].to_dict()\n",
    "            dict_item['ticker'] = myTicker\n",
    "            dict_item['figi'] = myFigi\n",
    "            shares_out_list.append(dict_item)\n",
    "            shares_out_lists_combined.extend(shares_out_list)\n",
    "\n",
    "    except:\n",
    "        \n",
    "        # Track any tickers that do not have shares outstanding data available.\n",
    "        bad_tickers.append(myTicker)\n",
    "        pass\n",
    "\n",
    "    return shares_out_lists_combined\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entitled-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shares out data for each ticker.\n",
    "\n",
    "@task\n",
    "def get_shares_out_data(df_price_update_total):\n",
    "\n",
    "    import concurrent.futures\n",
    "\n",
    "    global df_shares_out\n",
    "    global shares_out_lists_combined\n",
    "\n",
    "    df_shares_out = pd.DataFrame()\n",
    "    bad_tickers = []\n",
    "    shares_out_lists_combined = []\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "\n",
    "        # Grab tickers and figis from the price history DF and drop any figi duplicates that might show up.    \n",
    "        arg_list = list(df_price_update_total[['figi', 'ticker']].drop_duplicates().to_records(index = False))\n",
    "\n",
    "        # Use concurrent.futures to use multiple threads to retrieve shares out data.\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "            executor.map(lambda f: get_shares_out(*f), arg_list)\n",
    "\n",
    "        # Comvert the shares out array to a dataframe\n",
    "        df_shares_out = pd.DataFrame(shares_out_lists_combined)\n",
    "\n",
    "        # Drop any duplicates, make sure the date column is in datetime format, rename the shares column and make\n",
    "        # sure zeros are nulled out and any negative values are replaced with absolutes.\n",
    "        df_shares_out = df_shares_out.drop_duplicates(subset=['ticker', 'date'], keep = 'first').copy()\n",
    "        df_shares_out['date']= pd.to_datetime(df_shares_out['date'])\n",
    "        df_shares_out = df_shares_out.rename(columns = {'value':'weighted_avg_shares_out'})\n",
    "        df_shares_out['weighted_avg_shares_out'] = df_shares_out['weighted_avg_shares_out'].replace(0, np.nan)\n",
    "        df_shares_out['weighted_avg_shares_out'] = df_shares_out['weighted_avg_shares_out'].abs()\n",
    "\n",
    "        # Isolate the most recent shares out figures for each ticker\n",
    "        df_shares_out = df_shares_out[df_shares_out.groupby('ticker')['date'].transform('max') == df_shares_out['date']]\n",
    "    \n",
    "    return df_shares_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0734f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the price data with the shares out data to create the final update dataframe.\n",
    "\n",
    "@task\n",
    "def create_complete_update_dataframe(df_shares_out, df_price_update_total):\n",
    "    \n",
    "    global df_price_update_complete\n",
    "    \n",
    "    df_price_update_complete = pd.DataFrame()\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "    \n",
    "        # Use left join to add the shares out data to the stock prices, then calculate market caps and sort by ticker and dates\n",
    "        df_price_update_complete = df_price_update_total.merge(df_shares_out[['weighted_avg_shares_out', 'ticker', 'figi']], on=['ticker', 'figi'], how='left')\n",
    "        df_price_update_complete['market_cap'] = df_price_update_complete['adj_close'] * df_price_update_complete['weighted_avg_shares_out']\n",
    "        df_price_update_complete['date'] = pd.to_datetime(df_price_update_complete['date'])\n",
    "        df_price_update_complete = df_price_update_complete.sort_values(by = ['ticker', 'date'])\n",
    "\n",
    "        # Add unique primary key column, last update date, last corporate action date and re-order columns\n",
    "        df_price_update_complete['key_id'] = df_price_update_complete['ticker'] + df_price_update_complete['figi'] + df_price_update_complete['date'].dt.strftime('%Y-%m-%d')\n",
    "        df_price_update_complete['last_update_date'] = todayDate\n",
    "        df_price_update_complete['last_corp_action_date'] = None\n",
    "        df_price_update_complete = df_price_update_complete[['key_id', 'ticker', 'figi', 'date', 'open', 'high', 'low', 'close', 'volume',\n",
    "               'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume',\n",
    "               'adj_factor', 'split_ratio', 'change', 'percent_change',\n",
    "               'fifty_two_week_high', 'fifty_two_week_low', 'market_cap',\n",
    "               'weighted_avg_shares_out', 'intraperiod', 'last_update_date', 'last_corp_action_date']]\n",
    "\n",
    "        print(\"The shares outstanding are captured and market caps calculated for all tickers that have shares out data available.\")\n",
    "        print(\"The shape of the new DF is \", df_price_update_complete.shape)\n",
    "    \n",
    "    return df_price_update_complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reserved-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataframe to CSV on S3 if you want to use AWS Lambda to take it from there and push it into \n",
    "# the RDS table.\n",
    "\n",
    "@task\n",
    "def push_data_to_S3(df_price_update_complete):\n",
    "\n",
    "    import io\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "\n",
    "        # Create the AWS client\n",
    "        client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id = aws_key,\n",
    "            aws_secret_access_key = aws_secret_key,\n",
    "            region_name = 'us-east-1'\n",
    "        )\n",
    "\n",
    "        myBucket = 'bns-intrinio-data'\n",
    "        myFileLocation = \"price-data-daily/df_price_update_complete_\" + nextDateString + \".csv\"\n",
    "\n",
    "        with io.StringIO() as csv_buffer:\n",
    "            df_price_update_complete.to_csv(csv_buffer, index=False)\n",
    "\n",
    "            response = client.put_object(\n",
    "                Bucket = myBucket, Key = myFileLocation, Body=csv_buffer.getvalue()\n",
    "            )\n",
    "\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "            if status == 200:\n",
    "                print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "            else:\n",
    "                print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faced-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use SQLAlchemy to push the final dataframe into SQL DB on AWS RDS:\n",
    "\n",
    "@task(trigger=all_successful)\n",
    "def push_data_to_RDS(df_price_update_complete):\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "\n",
    "        # Set database credentials.\n",
    "        creds = {'usr': rds_user,\n",
    "                 'pwd': rds_password,\n",
    "                 'hst': rds_host,\n",
    "                 'prt': 3306,\n",
    "                 'dbn': rds_database}\n",
    "\n",
    "        # MySQL conection string.\n",
    "        connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "        # Create sqlalchemy engine for MySQL connection.\n",
    "        engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "        # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "        df_price_update_complete.to_sql(name='price_data_historical', \n",
    "                                              con=engine, \n",
    "                                              if_exists='append', \n",
    "                                              index=False)\n",
    "\n",
    "        print(\"The new data has been appended to RDS. The number of new rows added is\", df_price_update_complete.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a31c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the daily run schedule.\n",
    "\n",
    "schedule = IntervalSchedule(\n",
    "    start_date=pendulum.datetime(2021, 11, 26, 21, 0, 0, tz=\"America/New_York\"),\n",
    "    interval=timedelta(days=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "assumed-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ETL update flow.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with Flow(\"Stock-Data-Update-ETL\", schedule) as flow:\n",
    "\n",
    "        get_max_date = get_max_date()\n",
    "        get_price_data = get_price_data(lastUpdate, td_days, upstream_tasks=[get_max_date])\n",
    "        get_shares_out_data = get_shares_out_data(df_price_update_total, upstream_tasks=[get_price_data])\n",
    "        create_complete_update_dataframe = create_complete_update_dataframe(df_shares_out, \n",
    "                                                                            df_price_update_total, \n",
    "                                                                            upstream_tasks=[get_shares_out_data])\n",
    "        data_to_s3 = push_data_to_S3(df_price_update_complete, upstream_tasks=[data_to_s3])\n",
    "        data_to_rds = push_data_to_RDS(df_price_update_complete, upstream_tasks=[create_complete_update_dataframe])\n",
    "\n",
    "    flow.set_reference_tasks([data_to_rds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29518d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8ee915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last day that prices were updated was 11/24/2021\n",
      "That date was 1 days ago.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f789eed16849efab7e5571869d44d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new prices available for  11/25/2021\n"
     ]
    }
   ],
   "source": [
    "# Test ETL process.\n",
    "\n",
    "get_max_date.run()\n",
    "get_price_data.run(lastUpdate, td_days)\n",
    "get_shares_out_data.run(df_price_update_total)\n",
    "create_complete_update_dataframe.run(df_shares_out, df_price_update_total)\n",
    "push_data_to_S3.run(df_price_update_complete)\n",
    "push_data_to_RDS.run(df_price_update_complete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
