{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "valid-pharmacy",
   "metadata": {},
   "source": [
    "## Adjust the database for recent corporate actions, such as splits and dividends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "confirmed-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials\n",
    "\n",
    "import json\n",
    "f = open(\"/. .<your file path here> . . /credentials.json\")\n",
    "credentials = json.load(f)\n",
    "\n",
    "file_path = list(credentials.values())[0]\n",
    "intrinio_key = list(credentials.values())[1]\n",
    "aws_key = list(credentials.values())[2]\n",
    "aws_secret_key = list(credentials.values())[3]\n",
    "rds_host = list(credentials.values())[4]\n",
    "rds_user = list(credentials.values())[5]\n",
    "rds_password = list(credentials.values())[6]\n",
    "rds_database = list(credentials.values())[7]\n",
    "rds_charset = list(credentials.values())[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fifth-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Intrinio libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import intrinio_sdk as intrinio\n",
    "from intrinio_sdk.rest import ApiException\n",
    "\n",
    "intrinio.ApiClient().configuration.api_key['api_key'] = intrinio_key\n",
    "\n",
    "# Import the usual Python libraries\n",
    "\n",
    "from tqdm.notebook import tqdm, trange  # to be used to track progress in loop iterations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import sys\n",
    "\n",
    "# Import the AWS libraries\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Import SQL libraries\n",
    "\n",
    "import mysql.connector \n",
    "from mysql.connector import errorcode\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Declare the local File Path:\n",
    "\n",
    "global my_path\n",
    "my_path = file_path\n",
    "\n",
    "# Create the AWS client\n",
    "client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id = aws_key,\n",
    "    aws_secret_access_key = aws_secret_key,\n",
    "    region_name = 'us-east-1'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "smaller-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the last (max) date from the price history RDS table\n",
    "\n",
    "def get_max_date():\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "    import mysql.connector\n",
    "\n",
    "    global lastUpdate\n",
    "    global td_days\n",
    "    global todayDate\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT MAX(last_corp_action_date) FROM price_data_historical\")\n",
    "\n",
    "    myresult = mycursor.fetchall()\n",
    "\n",
    "    lastUpdate = myresult[0]\n",
    "    todayDate = date.today()\n",
    "    td = todayDate - lastUpdate[0].date()\n",
    "    td_days = td.days\n",
    "    \n",
    "    # Print the number of days since last script run, or if this script is run too soon, exit the script.\n",
    "    \n",
    "    if td_days > 0:\n",
    "        print(\"The last day that corp actions were updated was\", lastUpdate[0])\n",
    "        print(\"That date was\", td_days, \"day(s) ago.\")\n",
    "    else:\n",
    "        print(\"The last corporate action update was today. Try again tomorrow.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    return lastUpdate, td_days\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "charged-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the list of adjusted tickers and figis\n",
    "\n",
    "def get_adjusted_tickers_figis(lastUpdate, td_days):\n",
    "    \n",
    "    global df_adjusted_tickers_total\n",
    "    global response\n",
    "    global df_security\n",
    "    \n",
    "    df_adjusted_tickers = pd.DataFrame()\n",
    "    df_adjusted_tickers_total = pd.DataFrame()\n",
    "\n",
    "    # For each day since the last corporate actions update, fetch all the tickers/figis with recent corp actions.\n",
    "    for updateDate in tqdm(range(0, td_days)):\n",
    "\n",
    "        nextDate = lastUpdate[0] + timedelta(updateDate)\n",
    "        nextDateString = nextDate.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        identifier = 'USCOMP'\n",
    "        date = nextDateString\n",
    "        page_size = 10000\n",
    "        next_page = ''\n",
    "\n",
    "        try:\n",
    "\n",
    "            response = intrinio.StockExchangeApi().get_stock_exchange_price_adjustments(identifier, date=date, page_size=page_size, next_page=next_page)\n",
    "\n",
    "            if len(response.stock_price_adjustments) > 0:\n",
    "\n",
    "                df_security = pd.DataFrame([x.to_dict() for x in response.stock_price_adjustments]).security.apply(pd.Series)\n",
    "\n",
    "                # If no new adjustments show up, exit the script.\n",
    "                if df_security.empty:\n",
    "                    print(\"No new adjustments available for \", nextDate.strftime('%m/%d/%Y'))\n",
    "                    break\n",
    "\n",
    "                # Filter the data for only stocks, ADRs and ETFs\n",
    "                df_adjusted_tickers = df_security[df_security['code'].isin(['EQS', 'DR','ETF'])][['ticker', 'figi', 'code']]\n",
    "\n",
    "                # Add a date column\n",
    "                df_adjusted_tickers['date'] = nextDateString\n",
    "\n",
    "                # Get the new split ratios and adjustment factors for each ticker/figi\n",
    "                df_data = pd.DataFrame([x.to_dict() for x in response.stock_price_adjustments])[['split_ratio', 'factor']]\n",
    "                df_data['ticker'] = df_security['ticker']\n",
    "\n",
    "                # Merge the data to a single dataframe\n",
    "                df_adjusted_tickers = pd.merge(df_adjusted_tickers, df_data, on = 'ticker', how = 'left')\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Add the daily lists to a total adjustments list, sort by date and ticker, and drop any duplicates or NaNs\n",
    "        df_adjusted_tickers_total = pd.concat([df_adjusted_tickers_total, df_adjusted_tickers], ignore_index = True, axis = 0)\n",
    " \n",
    "    # If there are no tickers to be adjusted, quit the routine, else continue.\n",
    "    if df_adjusted_tickers.shape[0] == 0:\n",
    "        print(\"The number of adjusted EQS, DR or ETF securities is \", df_adjusted_tickers_total.shape[0])\n",
    "        print(\"There is no need to proceed further\")\n",
    "        quit()\n",
    "    else:\n",
    "        \n",
    "        df_adjusted_tickers_total = df_adjusted_tickers_total.sort_values(by = ['date', 'ticker'], ascending = True)\n",
    "        df_adjusted_tickers_total = df_adjusted_tickers_total.drop_duplicates(keep = 'first')\n",
    "        df_adjusted_tickers_total = df_adjusted_tickers_total.dropna(axis=0)\n",
    "        df_adjusted_tickers_total = df_adjusted_tickers_total.rename(columns = {'factor':'adj_factor'})\n",
    "\n",
    "        print(\"The number of adjusted securities is \", df_adjusted_tickers_total.shape[0])\n",
    "        print(\"The date range in the update list DF goes from \", df_adjusted_tickers_total.date.min(), \" to \", \n",
    "              df_adjusted_tickers_total.date.max())\n",
    "        \n",
    "    return df_adjusted_tickers_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "waiting-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the historical adjustment factors and split ratios for the adjusted tickers from the historical price data RDS table\n",
    "\n",
    "def get_adj_factors_splits(df_adjusted_tickers_total):\n",
    "    \n",
    "    import mysql.connector\n",
    "    \n",
    "    global df_splits_factors\n",
    "\n",
    "    figi_list = df_adjusted_tickers_total['figi'].tolist()\n",
    "    \n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT date, ticker, figi, adj_factor, split_ratio FROM price_data_historical WHERE figi in\" + str(tuple(figi_list)))\n",
    "\n",
    "    myresult = mycursor.fetchall()\n",
    "\n",
    "    df_splits_factors = pd.DataFrame(myresult, columns = ['date', 'ticker', 'figi', 'adj_factor', 'split_ratio']).sort_values(by = ['figi', 'date'], ascending = False)\n",
    "\n",
    "    df_splits_factors = pd.concat([df_splits_factors, df_adjusted_tickers_total[['date', 'ticker', 'figi', 'split_ratio', 'adj_factor']].copy()])\n",
    "    df_splits_factors['date'] = pd.to_datetime(df_splits_factors['date'])\n",
    "    df_splits_factors = df_splits_factors.sort_values(by = ['ticker', 'date'], ascending = False)\n",
    "    df_splits_factors = df_splits_factors.drop_duplicates(keep = 'first')\n",
    "\n",
    "    print(\"The shape of the historical splits and adjustments DF is \", df_splits_factors.shape)\n",
    "\n",
    "    return df_splits_factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "divine-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new historical prices for each ticker\n",
    "\n",
    "def get_historical_prices(myFigi, myTicker):\n",
    "    \n",
    "    global adjusted_prices_list\n",
    "    global adjusted_prices_total\n",
    "    \n",
    "    identifier = myFigi\n",
    "    start_date = ''\n",
    "    end_date = todayDate\n",
    "    frequency = 'daily'\n",
    "    page_size = 10000\n",
    "    next_page = ''\n",
    "    \n",
    "    adjusted_prices_list = []\n",
    "    \n",
    "    while next_page != None:\n",
    "        \n",
    "        try:\n",
    "\n",
    "            response = intrinio.SecurityApi().get_security_stock_prices(identifier, start_date=start_date, end_date=end_date, frequency=frequency, page_size=page_size, next_page=next_page)\n",
    "            adjusted_prices = [x.to_dict() for x in response.stock_prices]\n",
    "\n",
    "            for item in range(len(adjusted_prices)):\n",
    "\n",
    "                # Add ticker and figi to the results\n",
    "                dict_item = adjusted_prices[item]\n",
    "                dict_item['ticker'] = response.security.ticker\n",
    "                dict_item['figi'] = response.security.figi\n",
    "                adjusted_prices_list.append(dict_item)\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Track tickers that do not have any price data available.\n",
    "            bad_tickers.append(myTicker)        \n",
    "            pass\n",
    "        \n",
    "        next_page = response.next_page\n",
    "    \n",
    "    # Return adjusted prices\n",
    "    adjusted_prices_total.extend(adjusted_prices_list)\n",
    "    \n",
    "    return adjusted_prices_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "continuing-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch historical prices for adjusted tickers and add some extra calculations to match those in the history table\n",
    "\n",
    "def get_historical_price_data(df_adjusted_tickers_total):\n",
    "\n",
    "    import concurrent.futures\n",
    "\n",
    "    global df_adjusted_prices_total\n",
    "    global arg_list\n",
    "    global adjusted_prices_total\n",
    "\n",
    "    bad_tickers = []\n",
    "    adjusted_prices_total = []\n",
    "\n",
    "    arg_list = list(df_adjusted_tickers_total[['figi', 'ticker']].to_records(index=False))\n",
    "\n",
    "    # Use concurrent.futures to use multiple threads to retrieve price data.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        executor.map(lambda f: get_historical_prices(*f), arg_list)\n",
    "\n",
    "    # Convert the shares out array to a dataframe, drop any duplicates, set the date columnst to datetime format,\n",
    "    # combine the prices with split and adjustment factors, the fill forward any null values.\n",
    "    df_adjusted_prices_total = pd.DataFrame(adjusted_prices_total)\n",
    "    df_adjusted_prices_total['date']= pd.to_datetime(df_adjusted_prices_total['date'])\n",
    "    df_adjusted_prices_total = df_adjusted_prices_total.drop_duplicates(subset=['ticker', 'date'], keep = 'first')\n",
    "    df_adjusted_prices_total = pd.merge(df_adjusted_prices_total, df_splits_factors, on = ['date', 'ticker', 'figi'], how = 'left')\n",
    "    df_adjusted_prices_total[['adj_factor', 'split_ratio']] = df_adjusted_prices_total[['adj_factor', 'split_ratio']].fillna(1)\n",
    "\n",
    "    # Add change, pct_change, and 52 week high/low columns\n",
    "\n",
    "    df_adjusted_prices_total['change'] = df_adjusted_prices_total.sort_values('date').groupby(['ticker']).adj_close.diff()\n",
    "    df_adjusted_prices_total['percent_change'] = df_adjusted_prices_total.sort_values('date').groupby(['ticker']).adj_close.pct_change()\n",
    "\n",
    "    df_adjusted_prices_total['fifty_two_week_high'] = df_adjusted_prices_total.sort_values('date').groupby(['ticker']).adj_close.rolling(window = 260).max().reset_index(0,drop=True)\n",
    "    df_adjusted_prices_total['fifty_two_week_low'] = df_adjusted_prices_total.sort_values('date').groupby(['ticker']).adj_close.rolling(window = 260).min().reset_index(0,drop=True)\n",
    "\n",
    "    print(\"The shape of the historical price data DF is \", df_adjusted_prices_total.shape)\n",
    "    print(\"The earliest date is \", df_adjusted_prices_total['date'].min())\n",
    "\n",
    "    return df_adjusted_prices_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "finnish-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical weighted average diluted shares outstanding for each ticker\n",
    "\n",
    "def get_shares_out(myFigi, myTicker):\n",
    "    \n",
    "    global shares_out_list\n",
    "    global shares_out_lists_combined\n",
    "    \n",
    "    identifier = myFigi\n",
    "    tag = 'adjweightedavedilutedsharesos'\n",
    "    frequency = ''\n",
    "    type = ''\n",
    "    start_date = ''\n",
    "    end_date = ''\n",
    "    sort_order = 'desc'\n",
    "    page_size = 10000\n",
    "    next_page = ''\n",
    "\n",
    "    try:\n",
    "        response = intrinio.HistoricalDataApi().get_historical_data(identifier, tag, frequency=frequency, type=type, start_date=start_date, end_date=end_date, sort_order=sort_order, page_size=page_size, next_page=next_page)\n",
    "        shares_out_data = response.historical_data\n",
    "\n",
    "        shares_out_list = []\n",
    "\n",
    "        for item in range(len(shares_out_data)):\n",
    "    \n",
    "            # Add ticker and figi to the results\n",
    "            dict_item = shares_out_data[item].to_dict()\n",
    "            dict_item['ticker'] = myTicker\n",
    "            dict_item['figi'] = myFigi\n",
    "            shares_out_list.append(dict_item)\n",
    "\n",
    "    except:\n",
    "        \n",
    "        # Track tickers that do not have any shares out data available.\n",
    "        bad_tickers.append(myTicker)\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    #return shares_out_list\n",
    "    shares_out_lists_combined.extend(shares_out_list)\n",
    "    \n",
    "    return shares_out_lists_combined, shares_out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "round-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the historical shares outstanding data for all tickers. Since shares out are reported quarterly, resample\n",
    "# the data to show daily records.\n",
    "\n",
    "def get_shares_out_data(df_adjusted_tickers_total):\n",
    "\n",
    "    import concurrent.futures\n",
    "\n",
    "    global df_shares_out\n",
    "    global shares_out_lists_combined\n",
    "\n",
    "    shares_out_list = []\n",
    "    bad_tickers = []\n",
    "    shares_out_lists_combined = []\n",
    "\n",
    "    arg_list = list(df_adjusted_tickers_total[['figi', 'ticker']].to_records(index=False))\n",
    "\n",
    "    # Use concurrent.futures to use multiple threads to retrieve shares out data.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        executor.map(lambda f: get_shares_out(*f), arg_list)\n",
    "\n",
    "    # Convert the shares out array to a dataframe, drop any duplicates, rename the values column, replace zeros with\n",
    "    # NaNs, and get rid of any negative shares out numbers.\n",
    "    df_shares_out = pd.DataFrame(shares_out_lists_combined)\n",
    "    df_shares_out = df_shares_out.drop_duplicates(subset=['ticker', 'date'], keep = 'first')\n",
    "    df_shares_out['date'] = pd.to_datetime(df_shares_out['date'])\n",
    "    df_shares_out = df_shares_out.rename(columns = {'value':'weighted_avg_shares_out'})\n",
    "    df_shares_out['weighted_avg_shares_out'] = df_shares_out['weighted_avg_shares_out'].replace(0, np.nan)\n",
    "    df_shares_out['weighted_avg_shares_out'] = df_shares_out['weighted_avg_shares_out'].abs()\n",
    "\n",
    "    # Set date as index and convert to daily periods. Since shares out are reported quarterly, we need to resample to\n",
    "    # daily records.\n",
    "    df_shares_resample = df_shares_out.copy()\n",
    "    df_shares_resample = df_shares_resample.set_index('date')\n",
    "    df_shares_resample.index = pd.to_datetime(df_shares_resample.index)\n",
    "    df_shares_resample = df_shares_resample.groupby('ticker').resample('D', convention = 'end').ffill()\n",
    "    df_shares_resample = df_shares_resample.droplevel('ticker')\n",
    "    df_shares_resample = df_shares_resample.reset_index()\n",
    "\n",
    "    df_shares_out = df_shares_resample.copy()\n",
    "\n",
    "    print(\"The shape of the shares out DF is \", df_shares_out.shape)\n",
    "\n",
    "    return df_shares_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "valid-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use left join to add shares out to history dataframe and calculate market cap\n",
    "\n",
    "def combine_transform_adjusted_data(df_adjusted_prices_total, df_shares_out):\n",
    "    \n",
    "    global df_adjusted_prices_complete\n",
    "\n",
    "    df_adjusted_prices_complete = pd.merge(df_adjusted_prices_total, df_shares_out, on=['ticker', 'figi', 'date'], how='left')\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete.sort_values(by = ['ticker', 'date'], ascending = True)\n",
    "\n",
    "    df_adjusted_prices_complete['weighted_avg_shares_out'] = df_adjusted_prices_complete.groupby('ticker')['weighted_avg_shares_out'].transform(lambda x: x.ffill())\n",
    "    df_adjusted_prices_complete['market_cap'] = df_adjusted_prices_complete['adj_close'] * df_adjusted_prices_complete['weighted_avg_shares_out']\n",
    "\n",
    "    # Add last update date and primary key column, reset the data types for each column to be MySQL compliant, \n",
    "    # then reset the column order.\n",
    "\n",
    "    df_adjusted_prices_complete['last_updated_date'] = todayDate\n",
    "    df_adjusted_prices_complete['last_corp_action_date'] = todayDate\n",
    "    df_adjusted_prices_complete['date'] = pd.to_datetime(df_adjusted_prices_complete['date'])\n",
    "    df_adjusted_prices_complete['key_id'] = df_adjusted_prices_complete['ticker'] + df_adjusted_prices_complete['figi'] + df_adjusted_prices_complete['date'].dt.strftime('%Y-%m-%d')\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete.drop_duplicates(subset = ['key_id'], keep = 'first')\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete.where(df_adjusted_prices_complete.notnull(), None)\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete.convert_dtypes()\n",
    "    df_adjusted_prices_complete['date'] = df_adjusted_prices_complete['date'].dt.date\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete.astype({'open':'Float32', 'high':'Float32', 'low':'Float32', 'close':'Float32', 'volume':'Int32', 'adj_open':'Float32', 'adj_high':'Float32', 'adj_low':'Float32', 'adj_close':'Float32', 'adj_volume':'Int32', 'adj_factor':'Float32', 'split_ratio':'Int32', 'change':'Float32', 'percent_change':'Float32', 'fifty_two_week_high':'Float32', 'fifty_two_week_low':'Float32'})\n",
    "\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete[['key_id', 'ticker', 'figi', 'date', 'open', 'high', 'low', 'close', 'volume', 'adj_open', 'adj_high', \n",
    "                                                               'adj_low', 'adj_close', 'adj_volume', 'adj_factor', 'split_ratio', 'change', 'percent_change', \n",
    "                                                               'fifty_two_week_high', 'fifty_two_week_low', 'market_cap', 'weighted_avg_shares_out', 'intraperiod', \n",
    "                                                               'last_updated_date', 'last_corp_action_date']]\n",
    "\n",
    "    print(\"The complete corp actions DF shape is \", df_adjusted_prices_complete.shape)\n",
    "\n",
    "    return df_adjusted_prices_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "signed-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataframe to CSV on S3 for backup and/or archive purposes.\n",
    "\n",
    "def export_data_to_S3(df_adjusted_prices_complete):\n",
    "\n",
    "    import io\n",
    "\n",
    "    myBucket = 'bns-intrinio-data'\n",
    "    myFileLocation = \"price-data-daily/df_adjusted_prices_\" + str(todayDate) + \".csv\"\n",
    "\n",
    "    with io.StringIO() as csv_buffer:\n",
    "        df_adjusted_prices_complete.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        response = client.put_object(\n",
    "            Bucket = myBucket, Key = myFileLocation, Body=csv_buffer.getvalue()\n",
    "        )\n",
    "\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "        if status == 200:\n",
    "            print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "            print(\"New adjusted history data sucessfully posted to S3.\")\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-abraham",
   "metadata": {},
   "source": [
    "### Update History Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "alike-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and save pre-adjusted records in S3 prior to deletion. Just in case we need to undo the operation.\n",
    "\n",
    "def save_preadjusted_records_to_S3(df_adjusted_prices_complete):\n",
    "\n",
    "    import mysql.connector\n",
    "    import io\n",
    "    \n",
    "    global save_records_completion_status\n",
    "\n",
    "    figi_list = df_adjusted_prices_complete['figi'].unique()\n",
    "    preadjusted_records = []\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    def chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "    for chunk in chunker(figi_list, int(len(figi_list)/10)):\n",
    "\n",
    "        mycursor = mydb.cursor()\n",
    "\n",
    "        if len(tuple(chunk)) > 1:\n",
    "            mycursor.execute(\"SELECT * FROM price_data_historical WHERE figi IN \" + str(tuple(chunk)))\n",
    "        else:\n",
    "            mycursor.execute(\"SELECT * FROM price_data_historical WHERE figi IN ('\" + str(chunk[0]) + \"')\")\n",
    "\n",
    "        myresult = mycursor.fetchall()\n",
    "        mycursor.close()\n",
    "        preadjusted_records.extend(myresult)\n",
    "\n",
    "    df_preadjusted_records = pd.DataFrame(preadjusted_records, columns = df_adjusted_prices_complete.columns)\n",
    "\n",
    "    myBucket = 'bns-intrinio-data'\n",
    "    myFileLocation = \"price-data-daily/df_preadjusted_records_\" + str(todayDate) + \".csv\"\n",
    "\n",
    "    with io.StringIO() as csv_buffer:\n",
    "        df_preadjusted_records.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        response = client.put_object(\n",
    "            Bucket = myBucket, Key = myFileLocation, Body=csv_buffer.getvalue()\n",
    "        )\n",
    "\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "        if status == 200:\n",
    "            print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "            print(df_preadjusted_records.shape[0], \"Data record(s) saved on S3.\")\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "            \n",
    "    save_records_completion_status = status\n",
    "    \n",
    "    return save_records_completion_status\n",
    "                                 \n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aboriginal-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete records to be updated from SQL table. The SQL Connector library does not have a reliable \"Upsert\" function,\n",
    "# so we need to delete then replace the data.\n",
    "\n",
    "def delete_preadjusted_records(save_records_completion_status):\n",
    "\n",
    "    import mysql.connector\n",
    "    \n",
    "    global delete_records_status\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    sql_delete_query = \"DELETE FROM price_data_historical WHERE figi IN \" + str(tuple(df_adjusted_prices_complete['figi'].unique()))\n",
    "\n",
    "    mycursor.execute(sql_delete_query)\n",
    "\n",
    "    mydb.commit()\n",
    "\n",
    "    print(mycursor.rowcount, \"Data record(s) deleted.\")\n",
    "    delete_records_status = \"Deleted.\"\n",
    "    \n",
    "    return delete_records_status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pharmaceutical-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new updated records in the SQL history table.\n",
    "\n",
    "def insert_new_records(delete_records_status):\n",
    "\n",
    "    import pymysql.cursors\n",
    "    \n",
    "    global insert_records_status\n",
    "\n",
    "    connection = pymysql.connect(host = rds_host,\n",
    "                                 user = rds_user, \n",
    "                                 password = rds_password, \n",
    "                                 database = rds_database,\n",
    "                                 charset = rds_charset,\n",
    "                                 cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "    mycursor = connection.cursor()\n",
    "\n",
    "    sql_insert_query = \"\"\"\n",
    "    INSERT INTO price_data_historical \n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    data = df_adjusted_prices_complete.values.tolist()\n",
    "\n",
    "    mycursor.executemany(sql_insert_query, data)\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "    print(mycursor.rowcount, \"Data records inserted.\")\n",
    "    insert_records_status = \"Inserted. Done.\"\n",
    "    \n",
    "    return insert_records_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bulgarian-bacon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last day that corp actions were updated was 2021-10-28 00:00:00\n",
      "That date was 30 day(s) ago.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4645f344fd7340b3a801a46ce5fa9412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of adjusted securities is  1131\n",
      "The date range in the update list DF goes from  2021-10-28  to  2021-11-26\n",
      "The shape of the historical splits and adjustments DF is  (4146442, 5)\n",
      "The shape of the historical price data DF is  (3710702, 21)\n",
      "The earliest date is  1962-01-02 00:00:00\n",
      "The shape of the shares out DF is  (1373032, 4)\n",
      "The complete corp actions DF shape is  (3710587, 25)\n",
      "Successful S3 put_object response. Status - 200\n",
      "New adjusted history data sucessfully posted to S3.\n",
      "Successful S3 put_object response. Status - 200\n",
      "3731347 Data record(s) saved on S3.\n",
      "3731347 Data record(s) deleted.\n",
      "3710587 Data records inserted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Inserted. Done.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run ETL process.\n",
    "\n",
    "get_max_date()\n",
    "get_adjusted_tickers_figis(lastUpdate, td_days)\n",
    "get_adj_factors_splits(df_adjusted_tickers_total)\n",
    "get_historical_price_data(df_adjusted_tickers_total)\n",
    "get_shares_out_data(df_adjusted_tickers_total)\n",
    "combine_transform_adjusted_data(df_adjusted_prices_total, df_shares_out)\n",
    "export_data_to_S3(df_adjusted_prices_complete)\n",
    "save_preadjusted_records_to_S3(df_adjusted_prices_complete)\n",
    "delete_preadjusted_records(save_records_completion_status)\n",
    "insert_new_records(delete_records_status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-january",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-glance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "crucial-maple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>figi</th>\n",
       "      <th>code</th>\n",
       "      <th>date</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>adj_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGOX</td>\n",
       "      <td>BBG010WX25T5</td>\n",
       "      <td>ETF</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRBA</td>\n",
       "      <td>BBG000Q92GH5</td>\n",
       "      <td>EQS</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BPRN</td>\n",
       "      <td>BBG00CXM0LX5</td>\n",
       "      <td>EQS</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LVHD</td>\n",
       "      <td>BBG00BRDT880</td>\n",
       "      <td>ETF</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPA</td>\n",
       "      <td>BBG002WKD634</td>\n",
       "      <td>ETF</td>\n",
       "      <td>2021-11-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>XYLD</td>\n",
       "      <td>BBG00MVL3FH5</td>\n",
       "      <td>ETF</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>VABS</td>\n",
       "      <td>BBG00Z6QCLL5</td>\n",
       "      <td>ETF</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>VRIG</td>\n",
       "      <td>BBG00DW0GB49</td>\n",
       "      <td>ETF</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>XYLG</td>\n",
       "      <td>BBG00XH4TSF1</td>\n",
       "      <td>ETF</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>VRP</td>\n",
       "      <td>BBG006F9S8H4</td>\n",
       "      <td>ETF</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker          figi code        date  split_ratio  adj_factor\n",
       "0     AGOX  BBG010WX25T5  ETF  2021-11-04          1.0    0.949429\n",
       "1     FRBA  BBG000Q92GH5  EQS  2021-11-04          1.0    0.995924\n",
       "2     BPRN  BBG00CXM0LX5  EQS  2021-11-04          1.0    0.994061\n",
       "3     LVHD  BBG00BRDT880  ETF  2021-11-04          1.0    0.993915\n",
       "4     MLPA  BBG002WKD634  ETF  2021-11-05          1.0    0.980469\n",
       "..     ...           ...  ...         ...          ...         ...\n",
       "169   XYLD  BBG00MVL3FH5  ETF  2021-11-22          1.0    0.992780\n",
       "170   VABS  BBG00Z6QCLL5  ETF  2021-11-22          1.0    0.998601\n",
       "171   VRIG  BBG00DW0GB49  ETF  2021-11-22          1.0    0.999405\n",
       "172   XYLG  BBG00XH4TSF1  ETF  2021-11-22          1.0    0.996302\n",
       "173    VRP  BBG006F9S8H4  ETF  2021-11-22          1.0    0.996535\n",
       "\n",
       "[166 rows x 6 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the bad tickers that did not pull any results from the price or shares out queries and make \n",
    "# sure they are not well recognized names/tickers. E.g. none should be MSFT or AAPL. Most should be ETFs or very\n",
    "# small cap stocks.\n",
    "\n",
    "df_bad_tickers = pd.DataFrame(bad_tickers, columns=['ticker'])\n",
    "df_bad_tickers = df_bad_tickers.merge(df_adjusted_tickers_total, on=['ticker'], how='left')\n",
    "df_bad_tickers = df_bad_tickers.drop_duplicates(keep = 'first')\n",
    "df_bad_tickers.to_csv(path_or_buf = my_path + \"/df_bad_tickers.csv\", index=False)\n",
    "df_bad_tickers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
