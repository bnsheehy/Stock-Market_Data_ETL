{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a set of modules for downloading historical equity data from Intrinio APIs and converting it into Point & Figure calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import a variety of libraries and credentials to access APIs from Intrinio and AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials\n",
    "\n",
    "import json\n",
    "f = open(\"/. .<your file path here> . . /credentials.json\")\n",
    "credentials = json.load(f)\n",
    "\n",
    "file_path = credentials['file_path']\n",
    "intrinio_key = credentials['intrinio_key']\n",
    "aws_key = credentials['aws_access_key']\n",
    "aws_secret_key = credentials['aws_secret_key']\n",
    "rds_host = credentials['rds_host']\n",
    "rds_user = credentials['rds_user']\n",
    "rds_password = credentials['rds_password']\n",
    "rds_database = credentials['rds_database']\n",
    "rds_charset = credentials['rds_charset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Intrinio libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import intrinio_sdk as intrinio\n",
    "from intrinio_sdk.rest import ApiException\n",
    "\n",
    "intrinio.ApiClient().configuration.api_key['api_key'] = intrinio_key\n",
    "\n",
    "# Import the usual Python libraries\n",
    "\n",
    "from tqdm.notebook import tqdm, trange  # to be used to track progress in loop iterations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Zip file libraries\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import urllib.request as urllib2\n",
    "\n",
    "# Import the AWS libraries\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Declare the local file path to be used for saving CSV outputs.\n",
    "\n",
    "global my_path\n",
    "my_path = file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the historical data from Intrinio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the five Intrinio price history files into one dataframe after downloading them to your hard drive first.\n",
    "\n",
    "def assemble_bulk_history():\n",
    "    \n",
    "    global df_price_history\n",
    "\n",
    "    price_history = np.empty([0,29])\n",
    "\n",
    "    for X in tqdm(range(1, 6)):\n",
    "\n",
    "        ticker_file_path = my_path + \"/\" + \"stock_prices_uscomp_all_file-\" + str(X) + \".zip\"\n",
    "        data = pd.read_csv (ticker_file_path, low_memory = False)\n",
    "        data = np.array(data.values)\n",
    "        price_history = np.concatenate((price_history, data), axis=0)\n",
    "\n",
    "    # Convert price history array to dataframe and do some cleanup\n",
    "\n",
    "    df_price_history = pd.DataFrame(data = price_history, columns = ['security_id', 'company_id', 'name', 'cik', 'ticker', 'figi', 'composite_figi', 'composite_ticker', 'exchange_ticker', 'date', 'type', 'frequency', 'open', 'high', 'low', 'close', 'volume', 'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume', 'adj_factor', 'ex_dividend',  'split_ratio', 'change', 'percent_change', 'fifty_two_week_high', 'fifty_two_week_low'])\n",
    "\n",
    "    # Make sure Date column is in DateTime format, then sort by ticker and date\n",
    "\n",
    "    df_price_history['date'] = pd.to_datetime(df_price_history['date'])\n",
    "    df_price_history = df_price_history.sort_values(by=['ticker', 'date'])\n",
    "\n",
    "    print(\"Price history files assembled.\")    \n",
    "    print(\"The shape of the price history dataframe is \", df_price_history.shape)\n",
    "    \n",
    "    return df_price_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the five Intrinio price history files from their bulk history API and assemble them into one dataframe.\n",
    "\n",
    "def download_bulk_history():\n",
    "    \n",
    "    global df_price_history\n",
    "\n",
    "    price_history = np.empty([0,29])\n",
    "\n",
    "    response = intrinio.BulkDownloadsApi().get_bulk_download_links()\n",
    "\n",
    "    for X in tqdm(range(0, 5)):\n",
    "\n",
    "        url = response.bulk_downloads[1].links[X].url\n",
    "        r = urllib2.urlopen(url).read()\n",
    "        file = ZipFile(BytesIO(r))\n",
    "        data_csv = file.open(\"stock_prices_uscomp_all_file-\" + str(X+1) + \".csv\")\n",
    "        data = pd.read_csv(data_csv, low_memory=False)\n",
    "        data = np.array(data.values)\n",
    "        price_history = np.concatenate((price_history, data), axis=0)\n",
    "\n",
    "    # Convert price history array to dataframe and do some cleanup\n",
    "\n",
    "    df_price_history = pd.DataFrame(data = price_history, columns = ['security_id', 'company_id', 'name', 'cik', 'ticker', 'figi', 'composite_figi', 'composite_ticker', 'exchange_ticker', 'date', 'type', 'frequency', 'open', 'high', 'low', 'close', 'volume', 'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume', 'adj_factor', 'ex_dividend',  'split_ratio', 'change', 'percent_change', 'fifty_two_week_high', 'fifty_two_week_low'])\n",
    "\n",
    "    # Make sure Date column is in DateTime format, then sort by ticker and date\n",
    "\n",
    "    df_price_history['date'] = pd.to_datetime(df_price_history['date'])\n",
    "    df_price_history = df_price_history.sort_values(by=['ticker', 'date'])\n",
    "\n",
    "    print(\"Price history files assembled.\")    \n",
    "    print(\"The shape of the price history dataframe is \", df_price_history.shape)\n",
    "    print(\"df_price_history has \", len(df_price_history), \"records.\")\n",
    "    \n",
    "    return df_price_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the price history for securities classified as code = ['EQS', 'DR', 'ETF'].\n",
    "\n",
    "def filter_price_history(df_price_history):\n",
    "    \n",
    "    df_securities_total = pd.DataFrame()\n",
    "\n",
    "    identifier = 'USCOMP'\n",
    "    page_size = 10000\n",
    "    next_page = ''\n",
    "\n",
    "    # Query Intrinio's Securities by Exchange API to get a current list of securities for US exchanges with\n",
    "    # metadata for each security. The initial list will be larger than the max page size (> 10K records), so you will\n",
    "    # need to use this while loop to paginate the API.\n",
    "    \n",
    "    while next_page != None:\n",
    "\n",
    "        try:\n",
    "\n",
    "            response = intrinio.StockExchangeApi().get_stock_exchange_securities(identifier, page_size=page_size, next_page=next_page)\n",
    "            df_securities = pd.DataFrame([x.to_dict() for x in response.securities])\n",
    "            df_securities_total = pd.concat([df_securities_total, df_securities], ignore_index = True, axis = 0)\n",
    "            next_page = response.next_page\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    code_list = ['EQS', 'DR', 'ETF']  \n",
    "    # This code list corresponds to \"Equities\", \"ADR's\" and \"ETFs\". We filter for these security types, get their figi codes,\n",
    "    # then filter the price history dataframe for those figis.\n",
    "    df_securities_total = df_securities_total[df_securities_total['code'].isin(code_list)]\n",
    "    figi_list = df_securities_total['figi'].tolist()\n",
    "    df_price_history = df_price_history[df_price_history['figi'].isin(figi_list)]\n",
    "\n",
    "    print(\"df_price_history has \", len(df_price_history), \"records.\")\n",
    "\n",
    "    return df_price_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the shares out history dataframe to a CSV file if you want to have a backup.\n",
    "# df_price_history.to_csv(path_or_buf = my_path + \"/df_price_history.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46867581, 29)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # In case the kernel fails later and we have to recreate the market cap history, we can grab the previous CSV file here instead.\n",
    "\n",
    "# ticker_file_path = my_path + \"/\" + \"df_price_history.csv\"\n",
    "# df_price_history = pd.read_csv (ticker_file_path, low_memory=False)\n",
    "# df_price_history.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data and run the calcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the data\n",
    "\n",
    "def clean_up_data(df_price_history):\n",
    "    \n",
    "    global df_price_data\n",
    "\n",
    "    # Take the records out of the duplicate data set where figi is null.\n",
    "\n",
    "    df_price_history = df_price_history.dropna(subset = ['figi'])\n",
    "\n",
    "    # Find duplicates in dataframe on ticker, date and figi\n",
    "\n",
    "    df_duplicate_rows_date_figi = df_price_history[df_price_history.duplicated(subset = ['figi', 'date'], keep = False)]\n",
    "    dupe_tickers_count = df_duplicate_rows_date_figi.groupby('ticker').size().sort_values(ascending=False).to_frame('row_count')\n",
    "\n",
    "    if dupe_tickers_count.empty:\n",
    "        print(\"No dupes found.\")\n",
    "\n",
    "    else:\n",
    "        dupe_tickers_count.to_csv(path_or_buf = my_path + \"/dupe_count_on_tickers_date_figi.csv\", index=True)\n",
    "        print(dupe_tickers_count, \"dupe records found and deleted.\")\n",
    "\n",
    "        df_price_history = df_price_history.drop_duplicates(['figi', 'date'], keep = 'last')\n",
    "\n",
    "    print(df_price_history.shape)\n",
    "    print(list(df_price_history.columns))\n",
    "\n",
    "    df_price_data = df_price_history.copy()\n",
    "    \n",
    "    # Drop columns we don't need, and rename a few of the columns we do need.\n",
    "    df_price_data.drop(['security_id', 'company_id', 'name', 'cik', 'composite_figi', 'composite_ticker', 'exchange_ticker', 'open', \n",
    "               'type', 'frequency', 'high', 'low', 'close', 'volume', 'adj_factor', 'ex_dividend', 'split_ratio', \n",
    "               'fifty_two_week_high', 'fifty_two_week_low'], axis = 1, inplace = True)\n",
    "    df_price_data.rename(columns = {'adj_open':'open', 'adj_high':'high', 'adj_low':'low', 'adj_close': 'close', 'adj_volume':'volume'}, inplace = True)\n",
    "\n",
    "    df_price_data.sort_values(by = ['ticker', 'date'], ascending = True, inplace = True)\n",
    "    df_price_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    print(df_price_data.shape)\n",
    "    print(list(df_price_data.columns))\n",
    "    print(df_price_data.tail())\n",
    "\n",
    "    # Add columns for Ticker, Plot Symbol, Reversal, Signal Name and Percent Change\n",
    "    # Then seed first row for Plot Symbol and Signal Name with \"X\" and \"BUY\" respectively.\n",
    "\n",
    "    df_price_data['plot_symbol'] = np.nan\n",
    "    df_price_data['reversal'] = 0\n",
    "    df_price_data['signal_name'] = np.nan\n",
    "    df_price_data['high_point'] = np.nan\n",
    "    df_price_data['last_high_point'] = np.nan\n",
    "    df_price_data['prev_high_point'] = np.nan\n",
    "    df_price_data['low_point'] = np.nan\n",
    "    df_price_data['last_low_point'] = np.nan\n",
    "    df_price_data['prev_low_point'] = np.nan\n",
    "    df_price_data['entry_x'] = np.nan\n",
    "    df_price_data['entry_o'] = np.nan\n",
    "    df_price_data['next_entry'] = np.nan\n",
    "    df_price_data['stop_loss'] = np.nan\n",
    "    df_price_data['target_price'] = np.nan\n",
    "\n",
    "\n",
    "    df_price_data = df_price_data[['date', 'figi', 'ticker', 'open', 'high', 'low', 'close', 'change', 'percent_change', 'volume', 'plot_symbol', 'reversal', \n",
    "                 'signal_name', 'high_point', 'last_high_point', 'prev_high_point', 'low_point', 'last_low_point', 'prev_low_point', 'entry_x', 'entry_o', \n",
    "                 'next_entry', 'stop_loss', 'target_price']]\n",
    "\n",
    "    print(df_price_data.head())\n",
    "    \n",
    "    return df_price_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_pnf_calcs(myFigi):\n",
    "    \n",
    "    global data\n",
    "    \n",
    "    boxSize = .02\n",
    "    reversalBoxes = 3\n",
    "    reversalAmount = boxSize * reversalBoxes\n",
    "\n",
    "    new_data_list = []\n",
    "    \n",
    "    data = df_price_data.loc[df_price_data['figi'] == myFigi].copy()\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # Set all starting High Points and Low Points equal to the close price on the first day of the time series.\n",
    "\n",
    "    data.loc[0, 'plot_symbol'] = 'X'      #On Day 1, plot_symbol = \"X\"\n",
    "    data.loc[0, 'signal_name'] = 'BUY'    #On Day 1, signal_name = \"BUY\"\n",
    "\n",
    "    high_point = data['close'].iloc[0]     # Set Day 1 values for remaining P&F columns equal to the first close price.\n",
    "    low_point = data['close'].iloc[0]\n",
    "    last_high_point = data['close'].iloc[0]\n",
    "    last_low_point = data['close'].iloc[0]\n",
    "    prev_high_point = data['close'].iloc[0]\n",
    "    prev_low_point = data['close'].iloc[0]\n",
    "    entry_x = data['close'].iloc[0]\n",
    "    entry_o = data['close'].iloc[0]\n",
    "    target_price = data['close'].iloc[0]\n",
    "\n",
    "    # Start the loop on the second day, loop through each day's close price after that.\n",
    "    for i in range(1, len(data)):\n",
    "\n",
    "        if data['plot_symbol'].iloc[i - 1] == 'X':   #If previous Plot Symbol = \"X\", then:\n",
    "\n",
    "            if data['close'].iloc[i] >= data['close'].iloc[i - 1]:     #If current price >= previous price, then:\n",
    "                data.loc[i, 'plot_symbol'] = 'X'        # Today's Plot Symbol = \"X\".\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]    #and copy yesterday's signal to today.\n",
    "\n",
    "                if data['close'].iloc[i] > high_point:    #And if today's price is higher than the most recent high price, \n",
    "                    high_point = data['close'].iloc[i]       #then make today's price the  high price,\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "                if data['close'].iloc[i] > last_high_point:  #And if today's price is higher than the high point from the last X column,\n",
    "                    data.loc[i, 'signal_name'] = \"BUY\"           #then today's signal = \"BUY\".\n",
    "\n",
    "            elif data['close'].iloc[i] < high_point * (1 - reversalAmount):     #Else if today's price is less than the previous high times 1 - reversal,\n",
    "                data.loc[i, 'plot_symbol'] = 'O'                                     #the Plot Symbol reverses to \"O\",\n",
    "                low_point = data['close'].iloc[i]                                   #and the  low point is today's price,\n",
    "                data.loc[i, 'reversal'] = 1                                         #and reversal = 1,\n",
    "                prev_high_point = last_high_point                                        #and prev_high_point = last_high_point, saving this ValueSignal to use in the Target Price calc below\n",
    "                last_high_point = high_point                                               #and last_high_point = most recent high point\n",
    "                entry_o = data['close'].iloc[i - 1]                                 #and entry_o = previous day's closing price, used in next_entry and stop_loss calcs\n",
    "\n",
    "                if data['close'].iloc[i] < last_low_point:   #And if today's price is lower than the low point from the last O column,\n",
    "                    data.loc[i, 'signal_name'] = \"SELL\"          #then today's signal = \"SELL\".\n",
    "                else:\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #Else copy yesterday's signal to today.\n",
    "\n",
    "            else:\n",
    "                data.loc[i, 'plot_symbol'] = 'X'  #Else, Plot Symbol = \"X\" (price is down but not enough to triger a reversal)\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "\n",
    "        if data['plot_symbol'].iloc[i - 1] == 'O':   #If previous Plot Symbol = \"O\", then:\n",
    "\n",
    "            if data['close'].iloc[i] < data['close'].iloc[i - 1]:            #If current price <= previous price, then:\n",
    "                data.loc[i, 'plot_symbol'] = 'O'         # Today's Plot Symbol = \"O\".\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]\n",
    "\n",
    "                if data['close'].iloc[i] < low_point:       #And if today's price is lower than the most recent low price, \n",
    "                    low_point = data['close'].iloc[i]         #then make today's price the  low price.\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "                if data['close'].iloc[i] < last_low_point:   #And if today's price is lower than the low point from the last O column,\n",
    "                    data.loc[i, 'signal_name'] = \"SELL\"         #then today's signal = \"SELL\".\n",
    "\n",
    "\n",
    "            elif data['close'].iloc[i] > low_point * (1 + reversalAmount):       #Else if today's price is greater than the previous high, times 1 + reversal,\n",
    "                data.loc[i, 'plot_symbol'] = 'X'                                       #the Plot Symbol reverses to \"X\",\n",
    "                high_point = data['close'].iloc[i]                                    #and the  high point is today's price,\n",
    "                data.loc[i, 'reversal'] = 1                                           #and reversal = 1,\n",
    "                prev_low_point = last_low_point                                            ##and prev_low_point = last_low_point, saving this ValueSignal to use in the Target Price calc below\n",
    "                last_low_point = low_point                                                   #and last_low_point = most recent low point\n",
    "                entry_x = data['close'].iloc[i - 1]                                   #and entry_x = previous day's closing price, used in next_entry and stop_loss calcs\n",
    "\n",
    "                if data['close'].iloc[i] > last_high_point:  #And if today's price is higher than the high point from the last X column,\n",
    "                    data.loc[i, 'signal_name'] = \"BUY\"          #then today's signal = \"BUY\".\n",
    "\n",
    "                else:\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]     #Else copy yesterday's signal to today.\n",
    "\n",
    "            else:\n",
    "                data.loc[i, 'plot_symbol'] = 'O'  #Else, Plot Symbol = \"O\" (price is up but not enough to triger a reversal)\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "        data.loc[i, 'high_point'] = high_point            #high_point = current \"high_point\"\n",
    "        data.loc[i, 'low_point'] = low_point             #low_point = current \"low_point\"\n",
    "        data.loc[i, 'last_high_point'] = last_high_point  #last_high_point = current \"last_high_point\"\n",
    "        data.loc[i, 'last_low_point'] = last_low_point    #last_low_point = current \"last_low_point\"\n",
    "        data.loc[i, 'prev_high_point'] = prev_high_point  #prev_high_point = current \"prev_high_point\"\n",
    "        data.loc[i, 'prev_low_point'] = prev_low_point    #prev_low_point = current \"prev_low_point\"\n",
    "\n",
    "        if data['signal_name'].iloc[i] == \"BUY\":\n",
    "\n",
    "            next_entry = entry_o * (1 + boxSize)         #Set next_entry at one box up from the price at the last reversal from X to O, which should be near the top of the previous X column\n",
    "            data.loc[i, 'next_entry'] = next_entry\n",
    "            stop_loss = entry_x * (1 - boxSize)          #Set the stop_loss at one box down from the price at the last reversal from O to X, which should be near the bottom of the previous O column\n",
    "            data.loc[i, 'stop_loss'] = stop_loss\n",
    "\n",
    "            if data['signal_name'].iloc[i - 1] == \"SELL\":\n",
    "                target_price = ((last_high_point - prev_low_point) * reversalBoxes) + prev_low_point   #Upon reversal from SELL to BUY, set the target_price equal to the size of the previous X column,\n",
    "                                                                                                # times the box size, added to the bottom of the previous X column. Once calculated, it does not\n",
    "                                                                                                # change for the balance of the current BUY signal.\n",
    "            data.loc[i, 'target_price'] = target_price\n",
    "\n",
    "        else:\n",
    "            next_entry = entry_x * (1 - boxSize)         #Set next_entry at one box down from the price at the last reversal from O to X, which should be near the bottom of the previous O column\n",
    "            data.loc[i, 'next_entry'] = next_entry\n",
    "            stop_loss = entry_o * (1 + boxSize)          #Set the stop_loss at one box up from the price at the last reversal from X to O, which should be near the top of the previous X column\n",
    "            data.loc[i, 'stop_loss'] = stop_loss\n",
    "\n",
    "            if data['signal_name'].iloc[i - 1] == \"BUY\":\n",
    "                target_price = prev_high_point - ((prev_high_point - last_low_point) * reversalBoxes)  #Upon reversal from BUY to SELL, set the target_price equal to the size of the previous O column,\n",
    "                                                                                                # times the box size, subtracted from the top of the previous O column. Once calculated, it does not\n",
    "                                                                                                # change for the balance of the current SELL signal.\n",
    "            data.loc[i, 'target_price'] = target_price\n",
    "            \n",
    "        data.loc[i, 'entry_x'] = entry_x            #entry_x = current \"entry_x\"\n",
    "        data.loc[i, 'entry_o'] = entry_o            #entry_o = current \"entry_o\"\n",
    "\n",
    "    data_list = data.values.tolist()\n",
    "    new_data_list.extend(data_list)\n",
    "    \n",
    "    return new_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the calculations and prepare final dataframe.\n",
    "\n",
    "def run_all_calcs(df_price_data):\n",
    "    \n",
    "    global df_pnf_data_history_complete_load\n",
    "    global df_pnf_data_history\n",
    "\n",
    "    import multiprocessing\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    new_data_list = []\n",
    "\n",
    "    figi_list = df_price_data['figi'].unique().tolist()  # Get the list of figi codes to run P&F calcs on.\n",
    "\n",
    "    p = Pool()\n",
    "    result = p.map(generate_pnf_calcs, figi_list)  # Use multiprocessor pool to spread the work over several processors.\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    end_time = time.time()  # Keep track of the time spent on calcs.\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(\"Elapsed time was\", round(elapsed_time/60, 2), \"minutes.\")\n",
    "\n",
    "    new_data_list = []\n",
    "\n",
    "    for i in range(0, len(figi_list)):\n",
    "        data_list = result[i]\n",
    "        new_data_list.extend(data_list)   # Assemble the data array to be converted to a dataframe below.\n",
    "\n",
    "    myColumns = ['date', 'figi', 'ticker', 'open', 'high', 'low', 'close', 'change', 'percent_change', 'volume', 'plot_symbol', 'reversal', \n",
    "                 'signal_name', 'high_point', 'last_high_point', 'prev_high_point', 'low_point', 'last_low_point', 'prev_low_point', 'entry_x', 'entry_o', \n",
    "                 'next_entry', 'stop_loss', 'target_price']\n",
    "    \n",
    "    df_pnf_data_history = pd.DataFrame(new_data_list, columns = myColumns) # Convert to dataframe and save it to CSV here in case we need to QC it later.\n",
    "\n",
    "    df_pnf_data_history.to_csv(path_or_buf = my_path + \"/df_pnf_data_history.csv\", index=False)\n",
    "\n",
    "    print(df_pnf_data_history.shape)\n",
    "    \n",
    "    \n",
    "    # Add key_id and a few useful date columns to final dataframe.\n",
    "    df_pnf_data_history_complete_load = df_pnf_data_history.copy()\n",
    "    df_pnf_data_history_complete_load['date'] = pd.to_datetime(df_pnf_data_history_complete_load['date'])\n",
    "    df_pnf_data_history_complete_load['last_updated_date'] = pd.to_datetime(df_pnf_data_history_complete_load['date'].max()).normalize()\n",
    "    df_pnf_data_history_complete_load['last_corp_action_date'] = pd.to_datetime(df_pnf_data_history_complete_load['date'].max()).normalize()\n",
    "    df_pnf_data_history_complete_load['key_id'] = df_pnf_data_history_complete_load['ticker'] + df_pnf_data_history_complete_load['figi'] + df_pnf_data_history_complete_load['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    df_pnf_data_history_complete_load = df_pnf_data_history_complete_load[['key_id', 'date', 'figi', 'ticker', 'open', 'high', 'low', 'close', 'change', 'percent_change', 'volume', 'plot_symbol', 'reversal', \n",
    "                 'signal_name', 'high_point', 'last_high_point', 'prev_high_point', 'low_point', 'last_low_point', 'prev_low_point', 'entry_x', 'entry_o', \n",
    "                 'next_entry', 'stop_loss', 'target_price', 'last_updated_date', 'last_corp_action_date']]\n",
    "\n",
    "    print(df_pnf_data_history_complete_load.shape)\n",
    "    \n",
    "    return df_pnf_data_history_complete_load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For backup or archive purposes, save the final dataframe to CSV and/or parquet files and push them to AWS S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the low level functional AWS client\n",
    "\n",
    "def push_data_to_S3(df_pnf_data_history_complete_load):\n",
    "    \n",
    "    client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id = aws_key,\n",
    "        aws_secret_access_key = aws_secret_key,\n",
    "        region_name = 'us-east-1'\n",
    "    )\n",
    "\n",
    "    # Export the price history dataframe to a zipped CSV file then push to AWS S3.\n",
    "    compression_opts = dict(method='zip', archive_name='df_pnf_data_history_complete_load.csv') \n",
    "    df_pnf_data_history_complete_load.to_csv(path_or_buf = my_path + \"/df_pnf_data_history_complete_load.zip\", index=False, compression=compression_opts)\n",
    "    client.upload_file(my_path + \"/df_pnf_data_history_complete_load.zip\", 'bns-intrinio-data', \"price-data-historical/csv_files/df_pnf_data_history_complete_load.zip\")\n",
    "\n",
    "\n",
    "    # Write parquet file to local drive, then push to AWS S3.\n",
    "    local_file = my_path + \"/df_pnf_data_history_complete_load.parquet\"\n",
    "    parquet_table = pa.Table.from_pandas(df_pnf_data_history_complete_load)\n",
    "    pq.write_table(parquet_table, local_file)\n",
    "    client.upload_file(local_file, 'bns-intrinio-data', \"price-data-historical/parquet_files/df_pnf_data_history_complete_load.parquet\")\n",
    "\n",
    "    print(\"Data saved to S3 in zipped CSV and parquet.\")\n",
    "    S3_push_status = \"Done.\"\n",
    "    \n",
    "    return S3_push_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the complete history file if needed\n",
    "\n",
    "def upload_data():\n",
    "    \n",
    "    global df_pnf_data_history_complete_load\n",
    "    \n",
    "    file_path = my_path + \"/\" + \"df_pnf_data_history_complete_load.zip\"\n",
    "    df_pnf_data_history_complete_load = pd.read_csv (file_path, low_memory=False)\n",
    "    df_pnf_data_history_complete_load.shape\n",
    "    \n",
    "    print('Upload done.')\n",
    "    \n",
    "    return df_pnf_data_history_complete_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, create the MySQL price history table in RDS and push the history data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SQL libraries\n",
    "\n",
    "def create_and_fill_RDS_table(df_pnf_data_history_complete_load):\n",
    "\n",
    "    import mysql.connector \n",
    "    from mysql.connector import errorcode\n",
    "\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    # Establish the MySQL connection\n",
    "\n",
    "    connection = mysql.connector.connect(host=rds_host,\n",
    "                                 user=rds_user, \n",
    "                                 password=rds_password, \n",
    "                                 database=rds_database,\n",
    "                                 charset=rds_charset)\n",
    "\n",
    "    mycursor = connection.cursor()\n",
    "\n",
    "    # Create the data table in MySQL with MySQL Connector library\n",
    "\n",
    "    create_pnf_data_history_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS `base_pnf_data_historical` (\n",
    "    `key_id` varchar(40) PRIMARY KEY,\n",
    "    `date` datetime NOT NULL,\n",
    "    `figi` varchar(14) NOT NULL,\n",
    "    `ticker` varchar(8) NOT NULL,\n",
    "    `open` float NULL,\n",
    "    `high` float NULL,\n",
    "    `low` float NULL,\n",
    "    `close` float NULL,\n",
    "    `change` float NULL,\n",
    "    `percent_change` float NULL,\n",
    "    `volume` float NULL,\n",
    "    `plot_symbol` varchar(1) NULL,\n",
    "    `reversal` integer NULL,\n",
    "    `signal_name` varchar(4) NULL,\n",
    "    `high_point` float NULL,\n",
    "    `last_high_point` float NULL,\n",
    "    `prev_high_point` float NULL,\n",
    "    `low_point` float NULL,\n",
    "    `last_low_point` float NULL,\n",
    "    `prev_low_point` float NULL,\n",
    "    `next_entry` float NULL,\n",
    "    `stop_loss` float NULL,\n",
    "    `entry_x` float NULL,\n",
    "    `entry_o` float NULL,\n",
    "    `target_price` float NULL,\n",
    "    `last_updated_date` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "    `last_corp_action_date` datetime NULL\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n",
    "    \"\"\"\n",
    "\n",
    "    mycursor.execute(create_pnf_data_history_table)\n",
    "    \n",
    "    # Create indexes for ticker, figi and dates\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "    print(\"The base_pnf_data_historical table is created in RDS.\")\n",
    "\n",
    "\n",
    "    # Push the final dataframe into SQL DB on AWS RDS.\n",
    "\n",
    "    df = df_pnf_data_history_complete_load.copy()\n",
    "\n",
    "    # Set SQLAlchemy database credentials.\n",
    "    creds = {'usr': rds_user,\n",
    "             'pwd': rds_password,\n",
    "             'hst': rds_host,\n",
    "             'prt': 3306,\n",
    "             'dbn': rds_database}\n",
    "\n",
    "    # MySQL conection string.\n",
    "    connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "    # Create sqlalchemy engine for MySQL connection.\n",
    "    engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "    # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "    chunk = int(len(df) / 1000)\n",
    "    df.to_sql(name='base_pnf_data_historical', \n",
    "                                          con=engine, \n",
    "                                          if_exists='replace', \n",
    "                                          chunksize=chunk,\n",
    "                                          index=False)\n",
    "\n",
    "    # Create indexes for ticker, figi and dates\n",
    "\n",
    "    mycursor.execute(\"CREATE INDEX idx_ticker ON base_pnf_data_historical (ticker(8));\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_figi ON base_pnf_data_historical (figi(14));\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_date ON base_pnf_data_historical (date);\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_update ON base_pnf_data_historical (last_updated_date);\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_corp_action ON base_pnf_data_historical (last_corp_action_date);\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_signal ON base_pnf_data_historical (signal_name(4));\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_plot ON base_pnf_data_historical (plot_symbol(1));\")\n",
    "\n",
    "    print(\"The pnf history data is loaded and the indexes are set.\")\n",
    "    rds_table_status = \"Done.\"\n",
    "    \n",
    "    return rds_table_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to S3 in zipped CSV and parquet.\n",
      "The base_pnf_data_historical table is created in RDS.\n",
      "The pnf history data is loaded and the indexes are set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the ETL process.\n",
    "\n",
    "#assemble_bulk_history()\n",
    "download_bulk_history()\n",
    "filter_price_history(df_price_history)\n",
    "\n",
    "df_price_history.to_csv(path_or_buf = my_path + \"/df_price_history.csv\", index=False)\n",
    "\n",
    "clean_up_data(df_price_history)\n",
    "run_all_calcs(df_price_data)\n",
    "\n",
    "df_pnf_data_history_complete_load.to_csv(path_or_buf = my_path + \"/df_pnf_data_history_complete_load.csv\", index=False)\n",
    "\n",
    "push_data_to_S3(df_pnf_data_history_complete_load)\n",
    "create_and_fill_RDS_table(df_pnf_data_history_complete_load)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few extra modules in case you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [row_count]\n",
      "Index: []\n",
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Double check for duplicates\n",
    "\n",
    "df_duplicate_rows = df_pnf_data_history_complete_load[df_pnf_data_history_complete_load.duplicated(subset = ['key_id'], keep = False)]\n",
    "dupe_tickers_count = df_duplicate_rows.groupby('ticker').size().sort_values(ascending=False).to_frame('row_count')\n",
    "print(dupe_tickers_count)\n",
    "print(dupe_tickers_count.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload done.\n",
      "The base_pnf_data_historical table is created in RDS.\n",
      "The pnf history data is loaded and the indexes are set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "upload_data()\n",
    "create_and_fill_RDS_table(df_pnf_data_history_complete_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
