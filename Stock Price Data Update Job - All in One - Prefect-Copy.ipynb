{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passing-wagon",
   "metadata": {},
   "source": [
    "## Update Stock Prices and Corporate Actions for each new trading day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-plaintiff",
   "metadata": {},
   "source": [
    "This notebook includes modules to update the stock prices, make adjustments to historical data for splits and dividends, and run Point & Figure calculations on the new price data, all in one script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials\n",
    "\n",
    "import json\n",
    "f = open(\"/. .<your file path here> . . /credentials.json\")\n",
    "credentials = json.load(f)\n",
    "\n",
    "file_path = list(credentials.values())[0]\n",
    "intrinio_key = list(credentials.values())[1]\n",
    "aws_key = list(credentials.values())[3]\n",
    "aws_secret_key = list(credentials.values())[4]\n",
    "rds_host = list(credentials.values())[5]\n",
    "rds_user = list(credentials.values())[6]\n",
    "rds_password = list(credentials.values())[7]\n",
    "rds_database = list(credentials.values())[8]\n",
    "rds_charset = list(credentials.values())[9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Intrinio libraries\n",
    "\n",
    "import time\n",
    "import intrinio_sdk as intrinio\n",
    "from intrinio_sdk.rest import ApiException\n",
    "\n",
    "intrinio.ApiClient().configuration.api_key['api_key'] = intrinio_key\n",
    "\n",
    "# Import Prefect library\n",
    "\n",
    "from prefect.triggers import all_successful, all_failed\n",
    "from prefect import task, Flow\n",
    "import pendulum\n",
    "from prefect.schedules import IntervalSchedule\n",
    "from prefect.schedules.clocks import IntervalClock\n",
    "\n",
    "# Import the usual Python libraries\n",
    "\n",
    "from tqdm.notebook import tqdm, trange  # to be used to track progress in loop iterations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import sys\n",
    "\n",
    "# Import SQL libraries\n",
    "\n",
    "import mysql.connector \n",
    "from mysql.connector import errorcode\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Import the AWS libraries\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Create the AWS client\n",
    "client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id = aws_key,\n",
    "    aws_secret_access_key = aws_secret_key,\n",
    "    region_name = 'us-east-1'\n",
    ")\n",
    "\n",
    "# Declare the local File Path:\n",
    "\n",
    "global my_path\n",
    "my_path = file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-importance",
   "metadata": {},
   "source": [
    "## Start price update process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the last (max) date from the price history column\n",
    "\n",
    "@task\n",
    "def get_max_update_date():\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    global lastPriceUpdate\n",
    "    global td_days\n",
    "    global todayDate\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT MAX(date) FROM price_data_historical\")\n",
    "\n",
    "    myresult = mycursor.fetchall()[0][0].date()\n",
    "\n",
    "    todayDate = date.today()\n",
    "    lastPriceUpdate = myresult\n",
    "    td = todayDate - lastPriceUpdate\n",
    "    td_days = td.days\n",
    "\n",
    "    print(\"The last day that prices were updated was\", lastPriceUpdate.strftime('%m/%d/%Y'))\n",
    "    print(\"That date was\", td_days, \"days ago.\")\n",
    "\n",
    "    return lastPriceUpdate, td_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new data for each ticker to append to the price history table\n",
    "\n",
    "@task\n",
    "def get_recent_price_data(lastPriceUpdate, td_days):\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    bad_tickers = []\n",
    "    \n",
    "    global df_prices\n",
    "    global nextDateString\n",
    "    global df_price_update_total\n",
    "    df_price_update_total = pd.DataFrame()\n",
    "\n",
    "    # For each day from the last price update to today, retrieve the new security prices from the Intrinio API.\n",
    "    for updateDate in tqdm(range(1, td_days+1)):\n",
    "\n",
    "        nextDate = lastPriceUpdate + timedelta(updateDate)\n",
    "        nextDateString = nextDate.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        identifier = 'USCOMP'\n",
    "        date = nextDateString\n",
    "        records = 10000\n",
    "        next_page = ''\n",
    "\n",
    "        while next_page != None:\n",
    "\n",
    "            try:\n",
    "\n",
    "                response = intrinio.StockExchangeApi().get_stock_exchange_prices(identifier, date=date, page_size=records, next_page=next_page)\n",
    "                df_prices = pd.DataFrame([x.to_dict() for x in response.stock_prices])\n",
    "\n",
    "                if df_prices.empty:\n",
    "                    print(\"No new prices available for \", nextDate.strftime('%m/%d/%Y'))\n",
    "                    break\n",
    "\n",
    "                df_security = df_prices.security.apply(pd.Series)\n",
    "                df_price_update = pd.concat([df_prices, df_security], axis = 1).drop(['security'], axis = 1)\n",
    "\n",
    "                df_price_update_total = pd.concat([df_price_update_total, df_price_update], ignore_index = True, axis = 0)\n",
    "\n",
    "                next_page = response.next_page\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # If the API returns new prices, drop any duplicates and securities other than stocks, ADRS and ETFs, then\n",
    "    # convert the intraperiod flag to a boolean, rename the adj factor column, set the dates to datetime format\n",
    "    # and reset the column order for uploading to the database.\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "    \n",
    "        df_price_update_total = df_price_update_total.drop_duplicates(subset=['ticker', 'figi', 'date'], keep='last')\n",
    "        df_price_update_total = df_price_update_total[df_price_update_total.code.isin(['EQS', 'DR', 'ETF'])]\n",
    "        df_price_update_total.dropna(subset=['figi'], inplace = True)\n",
    "        df_price_update_total['intraperiod'] = (df_price_update_total['intraperiod'] == 'TRUE').astype(int)\n",
    "        df_price_update_total = df_price_update_total.rename(columns = {'factor':'adj_factor'})\n",
    "        df_price_update_total['date'] = pd.to_datetime(df_price_update_total['date'])\n",
    "        df_price_update_total = df_price_update_total[['ticker', 'figi', 'date', 'open', 'high', 'low', 'close', \n",
    "                                                'volume', 'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume', \n",
    "                                                'adj_factor', 'split_ratio', 'change', 'percent_change', \n",
    "                                                'fifty_two_week_high', 'fifty_two_week_low', 'intraperiod']]\n",
    "\n",
    "        print(\"The initial price update dataframe is retrieved.\")\n",
    "        print(\"The shape of the price update DF is\", df_price_update_total.shape)\n",
    "        print(\"The date range in the update DF goes from \", df_price_update_total.date.min().strftime('%m/%d/%Y'), \" to \", \n",
    "              df_price_update_total.date.max().strftime('%m/%d/%Y'))\n",
    "\n",
    "    return df_price_update_total\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical weighted average diluted shares outstanding for each ticker\n",
    "\n",
    "def get_latest_shares_out_sdk(myFigi, myTicker):\n",
    "    \n",
    "    global shares_out_list\n",
    "    global shares_out_lists_combined\n",
    "    \n",
    "    identifier = myFigi\n",
    "    tag = 'adjweightedavedilutedsharesos'\n",
    "    frequency = ''\n",
    "    type = ''\n",
    "    start_date = ''\n",
    "    end_date = ''\n",
    "    sort_order = 'desc'\n",
    "    page_size = 2\n",
    "    next_page = ''\n",
    "\n",
    "    try:\n",
    "        response = intrinio.HistoricalDataApi().get_historical_data(identifier, tag, frequency=frequency, type=type, start_date=start_date, end_date=end_date, sort_order=sort_order, page_size=page_size, next_page=next_page)\n",
    "        shares_out_data = response.historical_data\n",
    "\n",
    "        shares_out_list = []\n",
    "\n",
    "        for item in range(len(shares_out_data)):\n",
    "    \n",
    "            # Add the ticker and figi values to the results\n",
    "            dict_item = shares_out_data[item].to_dict()\n",
    "            dict_item['ticker'] = myTicker\n",
    "            dict_item['figi'] = myFigi\n",
    "            shares_out_list.append(dict_item)\n",
    "            shares_out_lists_combined.extend(shares_out_list)\n",
    "\n",
    "    except:\n",
    "        \n",
    "        # Track any tickers that do not have shares outstanding data available.\n",
    "        bad_tickers.append(myTicker)\n",
    "        pass\n",
    "\n",
    "    time.sleep( 0.5 )\n",
    "    return shares_out_lists_combined\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method for getting shares oustanding data by using the Intrinio web API instead of their Python SDK.\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_latest_shares_out_webapi(myFigi, myTicker):\n",
    "    \n",
    "    global dict_item\n",
    "    global shares_out_list\n",
    "    global shares_out_lists_combined\n",
    "\n",
    "    identifier = myFigi\n",
    "    tag = 'adjweightedavedilutedsharesos'\n",
    "    pageSize = 2\n",
    "    apiKey = intrinio_key\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"https://api-v2.intrinio.com/historical_data/{identifier}/{tag}?page_size={pageSize}&api_key={apiKey}\")\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            shares_out_list = []\n",
    "            \n",
    "            # Add the ticker and figi values to the results\n",
    "            dict_item = response.json()['historical_data'][0]\n",
    "            dict_item['ticker'] = myTicker\n",
    "            dict_item['figi'] = myFigi\n",
    "            shares_out_list.append(dict_item)\n",
    "            shares_out_lists_combined.extend(shares_out_list)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        # Track any tickers that do not have shares outstanding data available.\n",
    "        bad_tickers.append(myTicker)\n",
    "        pass\n",
    "\n",
    "    time.sleep( 0.5 )\n",
    "    return shares_out_lists_combined, dict_item\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shares out data for each ticker.\n",
    "\n",
    "@task\n",
    "def get_latest_shares_out_data(df_price_update_total):\n",
    "\n",
    "    import concurrent.futures\n",
    "\n",
    "    global df_latest_shares_out\n",
    "    global shares_out_lists_combined\n",
    "\n",
    "    df_latest_shares_out = pd.DataFrame()\n",
    "    bad_tickers = []\n",
    "    shares_out_lists_combined = []\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "\n",
    "        # Grab tickers and figis from the price history DF and drop any figi duplicates that might show up.    \n",
    "        arg_list = list(df_price_update_total[['figi', 'ticker']].drop_duplicates().to_records(index = False))\n",
    "\n",
    "        # Use concurrent.futures to use multiple threads to retrieve shares out data.\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "            executor.map(lambda f: get_latest_shares_out_sdk(*f), arg_list)\n",
    "\n",
    "        # Comvert the shares out array to a dataframe\n",
    "        df_latest_shares_out = pd.DataFrame(shares_out_lists_combined)\n",
    "\n",
    "        # Drop any duplicates, make sure the date column is in datetime format, rename the shares column and make\n",
    "        # sure zeros are nulled out and any negative values are replaced with absolutes.\n",
    "        \n",
    "        # comment this statement out if the web api is used for shares out\n",
    "        df_latest_shares_out = df_latest_shares_out.drop_duplicates(subset=['ticker', 'date'], keep = 'first').copy()\n",
    "        \n",
    "        \n",
    "        df_latest_shares_out['date']= pd.to_datetime(df_latest_shares_out['date'])\n",
    "        df_latest_shares_out = df_latest_shares_out.rename(columns = {'value':'weighted_avg_shares_out'})\n",
    "        df_latest_shares_out['weighted_avg_shares_out'] = df_latest_shares_out['weighted_avg_shares_out'].replace(0, np.nan)\n",
    "        df_latest_shares_out['weighted_avg_shares_out'] = df_latest_shares_out['weighted_avg_shares_out'].abs()\n",
    "\n",
    "        # Isolate the most recent shares out figures for each ticker\n",
    "        df_latest_shares_out = df_latest_shares_out[df_latest_shares_out.groupby('ticker')['date'].transform('max') == df_latest_shares_out['date']]\n",
    "    \n",
    "    print(\"The shape of the shares out DF is \", df_latest_shares_out.shape)\n",
    "    \n",
    "    return df_latest_shares_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the price data with the shares out data to create the final update dataframe.\n",
    "\n",
    "@task\n",
    "def create_complete_update_dataframe(df_latest_shares_out, df_price_update_total):\n",
    "    \n",
    "    global df_price_update_complete\n",
    "    \n",
    "    df_price_update_complete = pd.DataFrame()\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "    \n",
    "        # Use left join to add the shares out data to the stock prices, then calculate market caps and sort by ticker and dates\n",
    "        df_price_update_complete = df_price_update_total.merge(df_latest_shares_out[['weighted_avg_shares_out', 'ticker', 'figi']], on=['ticker', 'figi'], how='left')\n",
    "        df_price_update_complete['market_cap'] = df_price_update_complete['adj_close'] * df_price_update_complete['weighted_avg_shares_out']\n",
    "        df_price_update_complete['date'] = pd.to_datetime(df_price_update_complete['date'])\n",
    "        df_price_update_complete = df_price_update_complete.sort_values(by = ['ticker', 'date'])\n",
    "\n",
    "        # Add unique primary key column, last update date, last corporate action date and re-order columns\n",
    "        df_price_update_complete['key_id'] = df_price_update_complete['ticker'] + df_price_update_complete['figi'] + df_price_update_complete['date'].dt.strftime('%Y-%m-%d')\n",
    "        df_price_update_complete['last_updated_date'] = todayDate\n",
    "        df_price_update_complete['last_corp_action_date'] = None\n",
    "        df_price_update_complete = df_price_update_complete[['key_id', 'ticker', 'figi', 'date', 'open', 'high', 'low', 'close', 'volume',\n",
    "               'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume',\n",
    "               'adj_factor', 'split_ratio', 'change', 'percent_change',\n",
    "               'fifty_two_week_high', 'fifty_two_week_low', 'market_cap',\n",
    "               'weighted_avg_shares_out', 'intraperiod', 'last_updated_date', 'last_corp_action_date']]\n",
    "\n",
    "        print(\"The shares outstanding are captured and market caps calculated for all tickers that have shares out data available.\")\n",
    "        print(\"The shape of the new DF is \", df_price_update_complete.shape)\n",
    "    \n",
    "    return df_price_update_complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the price update DF to a CSV file for troubleshooting if needed.\n",
    "\n",
    "# df_price_update_total.to_csv(path_or_buf = my_path + \"/df_price_update_total.csv\", index=False)\n",
    "\n",
    "# df_price_update_complete.to_csv(path_or_buf = my_path + \"/df_price_update_complete.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataframe to CSV on S3 if you want to use AWS Lambda to take it from there and push it into \n",
    "# the RDS table.\n",
    "\n",
    "@task\n",
    "def push_data_to_S3(df_price_update_complete):\n",
    "\n",
    "    import io\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "\n",
    "        # Create the AWS client\n",
    "        client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id = aws_key,\n",
    "            aws_secret_access_key = aws_secret_key,\n",
    "            region_name = 'us-east-1'\n",
    "        )\n",
    "\n",
    "        myBucket = 'bns-intrinio-data'\n",
    "        myFileLocation = \"price-data-daily/df_price_update_complete_\" + nextDateString + \".csv\"\n",
    "\n",
    "        with io.StringIO() as csv_buffer:\n",
    "            df_price_update_complete.to_csv(csv_buffer, index=False)\n",
    "\n",
    "            response = client.put_object(\n",
    "                Bucket = myBucket, Key = myFileLocation, Body=csv_buffer.getvalue()\n",
    "            )\n",
    "\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "            if status == 200:\n",
    "                print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "            else:\n",
    "                print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use SQLAlchemy to push the final dataframe into SQL DB on AWS RDS:\n",
    "\n",
    "@task(trigger=all_successful)\n",
    "def push_data_to_RDS(df_price_update_complete):\n",
    "    \n",
    "    if len(df_price_update_total) > 0:\n",
    "\n",
    "        # Set database credentials.\n",
    "        creds = {'usr': rds_user,\n",
    "                 'pwd': rds_password,\n",
    "                 'hst': rds_host,\n",
    "                 'prt': 3306,\n",
    "                 'dbn': rds_database}\n",
    "\n",
    "        # MySQL conection string.\n",
    "        connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "        # Create sqlalchemy engine for MySQL connection.\n",
    "        engine = create_engine(connstr.format(**creds))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "        df_price_update_complete.to_sql(name='price_data_historical', \n",
    "                                        con=engine, \n",
    "                                        if_exists='append', \n",
    "                                        index=False\n",
    "                                        #chunksize = int(len(df_price_update_complete)/10)\n",
    "                                       )\n",
    "\n",
    "        print(\"The new data has been appended to RDS. The number of new rows added is\", df_price_update_complete.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-click",
   "metadata": {},
   "source": [
    "## Start corporate actions update process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the last (max) corporate action date from the price history RDS table\n",
    "\n",
    "@task\n",
    "def get_max_corax_date():\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "    import mysql.connector\n",
    "\n",
    "    global lastCoraxUpdate\n",
    "    global td_days\n",
    "    global todayDate\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT MAX(last_corp_action_date) FROM price_data_historical\")\n",
    "\n",
    "    myresult = mycursor.fetchall()\n",
    "\n",
    "    lastCoraxUpdate = myresult[0]\n",
    "    todayDate = date.today()\n",
    "    td = todayDate - lastCoraxUpdate[0].date()\n",
    "    td_days = td.days\n",
    "    \n",
    "    # Print the number of days since last script run, or if this script is run too soon, exit the script.\n",
    "    \n",
    "    if td_days > 0:\n",
    "        print(\"The last day that corp actions were updated was\", lastCoraxUpdate[0])\n",
    "        print(\"That date was\", td_days, \"day(s) ago.\")\n",
    "    else:\n",
    "        print(\"The last corporate action update was today. Try again tomorrow.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    return lastCoraxUpdate, td_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the list of adjusted tickers and figis\n",
    "\n",
    "@task\n",
    "def get_adjusted_tickers_figis(lastCoraxUpdate, td_days):\n",
    "    \n",
    "    global df_adjusted_tickers_total\n",
    "    global response\n",
    "    global df_security\n",
    "    \n",
    "    df_adjusted_tickers = pd.DataFrame()\n",
    "    df_adjusted_tickers_total = pd.DataFrame()\n",
    "\n",
    "    # For each day since the last corporate actions update, fetch all the tickers/figis with recent corp actions.\n",
    "    for updateDate in tqdm(range(0, td_days)):\n",
    "\n",
    "        nextDate = lastCoraxUpdate[0] + timedelta(updateDate)\n",
    "        nextDateString = nextDate.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        identifier = 'USCOMP'\n",
    "        date = nextDateString\n",
    "        page_size = 10000\n",
    "        next_page = ''\n",
    "\n",
    "        try:\n",
    "\n",
    "            response = intrinio.StockExchangeApi().get_stock_exchange_price_adjustments(identifier, date=date, page_size=page_size, next_page=next_page)\n",
    "\n",
    "            if len(response.stock_price_adjustments) > 0:\n",
    "\n",
    "                df_security = pd.DataFrame([x.to_dict() for x in response.stock_price_adjustments]).security.apply(pd.Series)\n",
    "\n",
    "                # If no new adjustments show up, exit the script.\n",
    "                if df_security.empty:\n",
    "                    print(\"No new adjustments available for \", nextDate.strftime('%m/%d/%Y'))\n",
    "                    break\n",
    "\n",
    "                # Filter the data for only stocks, ADRs and ETFs\n",
    "                df_adjusted_tickers = df_security[df_security['code'].isin(['EQS', 'DR','ETF'])][['ticker', 'figi', 'code']]\n",
    "\n",
    "                # Add a date column\n",
    "                df_adjusted_tickers['date'] = nextDateString\n",
    "\n",
    "                # Get the new split ratios and adjustment factors for each ticker/figi\n",
    "                df_data = pd.DataFrame([x.to_dict() for x in response.stock_price_adjustments])[['split_ratio', 'factor']]\n",
    "                df_data['ticker'] = df_security['ticker']\n",
    "\n",
    "                # Merge the data to a single dataframe\n",
    "                df_adjusted_tickers = pd.merge(df_adjusted_tickers, df_data, on = 'ticker', how = 'left')\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Add the daily lists to a total adjustments list, sort by date and ticker, and drop any duplicates or NaNs\n",
    "        df_adjusted_tickers_total = pd.concat([df_adjusted_tickers_total, df_adjusted_tickers], ignore_index = True, axis = 0)\n",
    " \n",
    "    # If there are no tickers to be adjusted, quit the routine, else continue.\n",
    "    if df_adjusted_tickers.shape[0] == 0:\n",
    "        print(\"The number of adjusted EQS, DR or ETF securities is \", df_adjusted_tickers_total.shape[0])\n",
    "        print(\"There is no need to proceed further\")\n",
    "        quit()\n",
    "    else:\n",
    "        \n",
    "        df_adjusted_tickers_total = df_adjusted_tickers_total.sort_values(by = ['date', 'ticker'], ascending = True)\n",
    "        df_adjusted_tickers_total = df_adjusted_tickers_total.drop_duplicates(keep = 'first')\n",
    "        df_adjusted_tickers_total = df_adjusted_tickers_total.dropna(axis=0)\n",
    "        df_adjusted_tickers_total = df_adjusted_tickers_total.rename(columns = {'factor':'adj_factor'})\n",
    "\n",
    "        print(\"The number of adjusted securities is \", df_adjusted_tickers_total.shape[0])\n",
    "        print(\"The date range in the update list DF goes from \", df_adjusted_tickers_total.date.min(), \" to \", \n",
    "              df_adjusted_tickers_total.date.max())\n",
    "        \n",
    "    return df_adjusted_tickers_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the historical adjustment factors and split ratios for the adjusted tickers from the historical price data RDS table\n",
    "\n",
    "@task\n",
    "def get_adj_factors_splits(df_adjusted_tickers_total):\n",
    "    \n",
    "    import mysql.connector\n",
    "    \n",
    "    global df_splits_factors\n",
    "\n",
    "    figi_list = df_adjusted_tickers_total['figi'].tolist()\n",
    "    \n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT date, ticker, figi, adj_factor, split_ratio FROM price_data_historical WHERE figi in\" + str(tuple(figi_list)))\n",
    "\n",
    "    myresult = mycursor.fetchall()\n",
    "\n",
    "    df_splits_factors = pd.DataFrame(myresult, columns = ['date', 'ticker', 'figi', 'adj_factor', 'split_ratio']).sort_values(by = ['figi', 'date'], ascending = False)\n",
    "\n",
    "    df_splits_factors = pd.concat([df_splits_factors, df_adjusted_tickers_total[['date', 'ticker', 'figi', 'split_ratio', 'adj_factor']].copy()])\n",
    "    df_splits_factors['date'] = pd.to_datetime(df_splits_factors['date'])\n",
    "    df_splits_factors = df_splits_factors.sort_values(by = ['ticker', 'date'], ascending = False)\n",
    "    df_splits_factors = df_splits_factors.drop_duplicates(keep = 'first')\n",
    "\n",
    "    print(\"The shape of the historical splits and adjustments DF is \", df_splits_factors.shape)\n",
    "\n",
    "    return df_splits_factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new historical prices for each ticker\n",
    "\n",
    "def get_historical_prices(myFigi, myTicker):\n",
    "    \n",
    "    global adjusted_prices_list\n",
    "    global adjusted_prices_total\n",
    "    \n",
    "    identifier = myFigi\n",
    "    start_date = ''\n",
    "    end_date = todayDate\n",
    "    frequency = 'daily'\n",
    "    page_size = 10000\n",
    "    next_page = ''\n",
    "    \n",
    "    adjusted_prices_list = []\n",
    "    \n",
    "    while next_page != None:\n",
    "        \n",
    "        try:\n",
    "\n",
    "            response = intrinio.SecurityApi().get_security_stock_prices(identifier, start_date=start_date, end_date=end_date, frequency=frequency, page_size=page_size, next_page=next_page)\n",
    "            adjusted_prices = [x.to_dict() for x in response.stock_prices]\n",
    "\n",
    "            for item in range(len(adjusted_prices)):\n",
    "\n",
    "                # Add ticker and figi to the results\n",
    "                dict_item = adjusted_prices[item]\n",
    "                dict_item['ticker'] = response.security.ticker\n",
    "                dict_item['figi'] = response.security.figi\n",
    "                adjusted_prices_list.append(dict_item)\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Track tickers that do not have any price data available.\n",
    "            bad_tickers.append(myTicker)        \n",
    "            pass\n",
    "        \n",
    "        next_page = response.next_page\n",
    "    \n",
    "    # Return adjusted prices\n",
    "    adjusted_prices_total.extend(adjusted_prices_list)\n",
    "    \n",
    "    return adjusted_prices_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch historical prices for adjusted tickers and add some extra calculations to match those in the history table\n",
    "\n",
    "@task\n",
    "def get_adjusted_price_data(df_adjusted_tickers_total):\n",
    "\n",
    "    import concurrent.futures\n",
    "\n",
    "    global df_adjusted_prices_total\n",
    "    global arg_list\n",
    "    global adjusted_prices_total\n",
    "    global df_prices_postiive\n",
    "\n",
    "    bad_tickers = []\n",
    "    adjusted_prices_total = []\n",
    "\n",
    "    arg_list = list(df_adjusted_tickers_total[['figi', 'ticker']].to_records(index=False))\n",
    "\n",
    "    # Use concurrent.futures to use multiple threads to retrieve price data.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        executor.map(lambda f: get_historical_prices(*f), arg_list)\n",
    "\n",
    "    # Convert the shares out array to a dataframe, drop any duplicates, set the date columnst to datetime format,\n",
    "    # combine the prices with split and adjustment factors, the fill forward any null values. Also get rid of any rows\n",
    "    # with negative prices. (Again, market data is always dirty, LOL!)\n",
    "    df_adjusted_prices_total = pd.DataFrame(adjusted_prices_total)\n",
    "    df_adjusted_prices_total['date']= pd.to_datetime(df_adjusted_prices_total['date'])\n",
    "    df_adjusted_prices_total = df_adjusted_prices_total.drop_duplicates(subset=['ticker', 'date'], keep = 'first')\n",
    "    df_adjusted_prices_total = pd.merge(df_adjusted_prices_total, df_splits_factors, on = ['date', 'ticker', 'figi'], how = 'left')\n",
    "    df_adjusted_prices_total[['adj_factor', 'split_ratio']] = df_adjusted_prices_total[['adj_factor', 'split_ratio']].fillna(1)\n",
    "    df_adjusted_prices_total = df_adjusted_prices_total[(df_adjusted_prices_total[['adj_open', 'adj_high', 'adj_low', 'adj_close']] > 0).all(1)]\n",
    "  \n",
    "    # Add change, pct_change, and 52 week high/low columns\n",
    "\n",
    "    df_adjusted_prices_total['change'] = df_adjusted_prices_total.sort_values('date').groupby(['ticker']).adj_close.diff()\n",
    "    df_adjusted_prices_total['percent_change'] = df_adjusted_prices_total.sort_values('date').groupby(['figi']).adj_close.pct_change().replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "    df_adjusted_prices_total['fifty_two_week_high'] = df_adjusted_prices_total.sort_values('date').groupby(['ticker']).adj_close.rolling(window = 260).max().reset_index(0,drop=True)\n",
    "    df_adjusted_prices_total['fifty_two_week_low'] = df_adjusted_prices_total.sort_values('date').groupby(['ticker']).adj_close.rolling(window = 260).min().reset_index(0,drop=True)\n",
    "\n",
    "     \n",
    "    \n",
    "    \n",
    "    print(\"The shape of the historical price data DF is \", df_adjusted_prices_total.shape)\n",
    "    print(\"The earliest date is \", df_adjusted_prices_total['date'].min())\n",
    "\n",
    "    return df_adjusted_prices_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical weighted average diluted shares outstanding for each ticker\n",
    "\n",
    "def get_historical_shares_out(myFigi, myTicker):\n",
    "    \n",
    "    global shares_out_list\n",
    "    global shares_out_lists_combined\n",
    "    \n",
    "    identifier = myFigi\n",
    "    tag = 'adjweightedavedilutedsharesos'\n",
    "    frequency = ''\n",
    "    type = ''\n",
    "    start_date = ''\n",
    "    end_date = ''\n",
    "    sort_order = 'desc'\n",
    "    page_size = 10000\n",
    "    next_page = ''\n",
    "\n",
    "    try:\n",
    "        response = intrinio.HistoricalDataApi().get_historical_data(identifier, tag, frequency=frequency, type=type, start_date=start_date, end_date=end_date, sort_order=sort_order, page_size=page_size, next_page=next_page)\n",
    "        shares_out_data = response.historical_data\n",
    "\n",
    "        shares_out_list = []\n",
    "\n",
    "        for item in range(len(shares_out_data)):\n",
    "    \n",
    "            # Add ticker and figi to the results\n",
    "            dict_item = shares_out_data[item].to_dict()\n",
    "            dict_item['ticker'] = myTicker\n",
    "            dict_item['figi'] = myFigi\n",
    "            shares_out_list.append(dict_item)\n",
    "\n",
    "    except:\n",
    "        \n",
    "        # Track tickers that do not have any shares out data available.\n",
    "        bad_tickers.append(myTicker)\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    #return shares_out_list\n",
    "    shares_out_lists_combined.extend(shares_out_list)\n",
    "    \n",
    "    time.sleep( 0.5 )\n",
    "    return shares_out_lists_combined, shares_out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the historical shares outstanding data for all tickers. Since shares out are reported quarterly, resample\n",
    "# the data to show daily records.\n",
    "\n",
    "@task\n",
    "def get_historical_shares_out_data(df_adjusted_tickers_total):\n",
    "\n",
    "    import concurrent.futures\n",
    "\n",
    "    global df_hist_shares_out\n",
    "    global shares_out_lists_combined\n",
    "\n",
    "    shares_out_list = []\n",
    "    bad_tickers = []\n",
    "    shares_out_lists_combined = []\n",
    "\n",
    "    arg_list = list(df_adjusted_tickers_total[['figi', 'ticker']].to_records(index=False))\n",
    "\n",
    "    # Use concurrent.futures to use multiple threads to retrieve shares out data.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        executor.map(lambda f: get_historical_shares_out(*f), arg_list)\n",
    "\n",
    "    # Convert the shares out array to a dataframe, drop any duplicates, rename the values column, replace zeros with\n",
    "    # NaNs, and get rid of any negative shares out numbers.\n",
    "    df_hist_shares_out = pd.DataFrame(shares_out_lists_combined)\n",
    "    df_hist_shares_out = df_hist_shares_out.drop_duplicates(subset=['ticker', 'date'], keep = 'first')\n",
    "    df_hist_shares_out['date'] = pd.to_datetime(df_hist_shares_out['date'])\n",
    "    df_hist_shares_out = df_hist_shares_out.rename(columns = {'value':'weighted_avg_shares_out'})\n",
    "    df_hist_shares_out['weighted_avg_shares_out'] = df_hist_shares_out['weighted_avg_shares_out'].replace(0, np.nan)\n",
    "    df_hist_shares_out['weighted_avg_shares_out'] = df_hist_shares_out['weighted_avg_shares_out'].abs()\n",
    "\n",
    "    # Set date as index and convert to daily periods. Since shares out are reported quarterly, we need to resample to\n",
    "    # daily records.\n",
    "    df_hist_shares_resample = df_hist_shares_out.copy()\n",
    "    df_hist_shares_resample = df_hist_shares_resample.set_index('date')\n",
    "    df_hist_shares_resample.index = pd.to_datetime(df_hist_shares_resample.index)\n",
    "    df_hist_shares_resample = df_hist_shares_resample.groupby('ticker').resample('D', convention = 'end').ffill()\n",
    "    df_hist_shares_resample = df_hist_shares_resample.droplevel('ticker')\n",
    "    df_hist_shares_resample = df_hist_shares_resample.reset_index()\n",
    "\n",
    "    df_hist_shares_out = df_hist_shares_resample.copy()\n",
    "\n",
    "    print(\"The shape of the shares out DF is \", df_hist_shares_out.shape)\n",
    "\n",
    "    return df_hist_shares_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use left join to add shares out to history dataframe and calculate market cap\n",
    "\n",
    "@task\n",
    "def combine_transform_adjusted_data(df_adjusted_prices_total, df_hist_shares_out):\n",
    "    \n",
    "    global df_adjusted_prices_complete\n",
    "\n",
    "    df_adjusted_prices_complete = pd.merge(df_adjusted_prices_total, df_hist_shares_out, on=['ticker', 'figi', 'date'], how='left')\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete.sort_values(by = ['ticker', 'date'], ascending = True)\n",
    "\n",
    "    df_adjusted_prices_complete['weighted_avg_shares_out'] = df_adjusted_prices_complete.groupby('ticker')['weighted_avg_shares_out'].transform(lambda x: x.ffill())\n",
    "    df_adjusted_prices_complete['market_cap'] = df_adjusted_prices_complete['adj_close'] * df_adjusted_prices_complete['weighted_avg_shares_out']\n",
    "\n",
    "    # Add last update date and primary key column, reset the data types for each column to be MySQL compliant, \n",
    "    # then reset the column order.\n",
    "\n",
    "    df_adjusted_prices_complete['last_updated_date'] = todayDate\n",
    "    df_adjusted_prices_complete['last_corp_action_date'] = todayDate\n",
    "    df_adjusted_prices_complete['date'] = pd.to_datetime(df_adjusted_prices_complete['date'])\n",
    "    df_adjusted_prices_complete['key_id'] = df_adjusted_prices_complete['ticker'] + df_adjusted_prices_complete['figi'] + df_adjusted_prices_complete['date'].dt.strftime('%Y-%m-%d')\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete.drop_duplicates(subset = ['key_id'], keep = 'first')\n",
    "    #df_adjusted_prices_complete = df_adjusted_prices_complete.where(df_adjusted_prices_complete.notnull(), None)\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete.convert_dtypes()\n",
    "    df_adjusted_prices_complete['date'] = df_adjusted_prices_complete['date'].dt.date\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete.astype({'open':'Float32', 'high':'Float32', 'low':'Float32', 'close':'Float32', 'volume':'Int32', 'adj_open':'Float32', 'adj_high':'Float32', 'adj_low':'Float32', 'adj_close':'Float32', 'adj_volume':'Int32', 'adj_factor':'Float32', 'split_ratio':'Int32', 'change':'Float32', 'percent_change':'Float32', 'fifty_two_week_high':'Float32', 'fifty_two_week_low':'Float32'})\n",
    "\n",
    "    df_adjusted_prices_complete = df_adjusted_prices_complete[['key_id', 'ticker', 'figi', 'date', 'open', 'high', 'low', 'close', 'volume', 'adj_open', 'adj_high', \n",
    "                                                               'adj_low', 'adj_close', 'adj_volume', 'adj_factor', 'split_ratio', 'change', 'percent_change', \n",
    "                                                               'fifty_two_week_high', 'fifty_two_week_low', 'market_cap', 'weighted_avg_shares_out', 'intraperiod', \n",
    "                                                               'last_updated_date', 'last_corp_action_date']]\n",
    "\n",
    "    print(\"The complete corp actions DF shape is \", df_adjusted_prices_complete.shape)\n",
    "\n",
    "    return df_adjusted_prices_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_adjusted_prices_complete.to_csv(path_or_buf = my_path + \"/df_adjusted_prices_complete.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataframe to CSV on S3 for backup and/or archive purposes.\n",
    "\n",
    "@task\n",
    "def export_data_to_S3(df_adjusted_prices_complete):\n",
    "\n",
    "    import io\n",
    "\n",
    "    myBucket = 'bns-intrinio-data'\n",
    "    myFileLocation = \"price-data-daily/df_adjusted_prices_\" + str(todayDate) + \".csv\"\n",
    "\n",
    "    with io.StringIO() as csv_buffer:\n",
    "        df_adjusted_prices_complete.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        response = client.put_object(\n",
    "            Bucket = myBucket, Key = myFileLocation, Body=csv_buffer.getvalue()\n",
    "        )\n",
    "\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "        if status == 200:\n",
    "            print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "            print(\"New adjusted history data sucessfully posted to S3.\")\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-paris",
   "metadata": {},
   "source": [
    "### Update the History Table with new adjusted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and save pre-adjusted records in S3 prior to deletion. Just in case we need to undo the operation.\n",
    "\n",
    "@task\n",
    "def save_preadjusted_records_to_S3(df_adjusted_prices_complete):\n",
    "\n",
    "    import mysql.connector\n",
    "    import io\n",
    "    \n",
    "    global save_records_completion_status\n",
    "\n",
    "    figi_list = df_adjusted_prices_complete['figi'].unique()\n",
    "    preadjusted_records = []\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    def chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "    for chunk in chunker(figi_list, int(len(figi_list)/10)):\n",
    "\n",
    "        mycursor = mydb.cursor()\n",
    "\n",
    "        if len(tuple(chunk)) > 1:\n",
    "            mycursor.execute(\"SELECT * FROM price_data_historical WHERE figi IN \" + str(tuple(chunk)))\n",
    "        else:\n",
    "            mycursor.execute(\"SELECT * FROM price_data_historical WHERE figi IN ('\" + str(chunk[0]) + \"')\")\n",
    "\n",
    "        myresult = mycursor.fetchall()\n",
    "        mycursor.close()\n",
    "        preadjusted_records.extend(myresult)\n",
    "\n",
    "    df_preadjusted_records = pd.DataFrame(preadjusted_records, columns = df_adjusted_prices_complete.columns)\n",
    "\n",
    "    myBucket = 'bns-intrinio-data'\n",
    "    myFileLocation = \"price-data-daily/df_preadjusted_records_\" + str(todayDate) + \".csv\"\n",
    "\n",
    "    with io.StringIO() as csv_buffer:\n",
    "        df_preadjusted_records.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        response = client.put_object(\n",
    "            Bucket = myBucket, Key = myFileLocation, Body=csv_buffer.getvalue()\n",
    "        )\n",
    "\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "        if status == 200:\n",
    "            print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "            print(df_preadjusted_records.shape[0], \"Data record(s) saved on S3.\")\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "            \n",
    "    save_records_completion_status = status\n",
    "    \n",
    "    return save_records_completion_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete records to be updated from SQL table. The SQL Connector library does not have a reliable \"Upsert\" function,\n",
    "# so we need to delete then replace the data.\n",
    "\n",
    "@task\n",
    "def delete_preadjusted_records(save_records_completion_status):\n",
    "\n",
    "    import mysql.connector\n",
    "    \n",
    "    global delete_records_status\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    sql_delete_query = \"DELETE FROM price_data_historical WHERE figi IN \" + str(tuple(df_adjusted_prices_complete['figi'].unique()))\n",
    "\n",
    "    mycursor.execute(sql_delete_query)\n",
    "\n",
    "    mydb.commit()\n",
    "\n",
    "    print(mycursor.rowcount, \"Data record(s) deleted.\")\n",
    "    delete_records_status = \"Deleted.\"\n",
    "    \n",
    "    return delete_records_status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new updated records in the SQL history table.\n",
    "\n",
    "@task\n",
    "def insert_new_records(delete_records_status):\n",
    "\n",
    "    import pymysql.cursors\n",
    "    \n",
    "    global insert_records_status\n",
    "\n",
    "    rowCount = 0\n",
    "\n",
    "    connection = pymysql.connect(host = rds_host,\n",
    "                                 user = rds_user, \n",
    "                                 password = rds_password, \n",
    "                                 database = rds_database,\n",
    "                                 charset = rds_charset,\n",
    "                                 cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "    mycursor = connection.cursor()\n",
    "\n",
    "    sql_insert_query = \"\"\"\n",
    "    INSERT INTO price_data_historical \n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_adjusted_prices_complete.copy()\n",
    "\n",
    "    def chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "    for chunk in tqdm(chunker(df, int(len(df)/10))):\n",
    "\n",
    "        data = chunk.values.tolist()\n",
    "        mycursor.executemany(sql_insert_query, data)\n",
    "        connection.commit()\n",
    "        rowCount = rowCount + mycursor.rowcount\n",
    "\n",
    "    print(rowCount, \"Data records inserted.\")\n",
    "    insert_records_status = \"Inserted. Done.\"\n",
    "\n",
    "    \n",
    "    return insert_records_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ea5e0",
   "metadata": {},
   "source": [
    "## Start the P&F update process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the last (max) date from the Point & Figure history table\n",
    "\n",
    "@task\n",
    "def get_max_pnf_date():\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    global lastPnFUpdate\n",
    "    global td_days\n",
    "    global todayDate\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT MAX(date) FROM base_pnf_data_historical\")\n",
    "\n",
    "    myResultDate = mycursor.fetchall()[0][0].date()\n",
    "\n",
    "    todayDate = date.today()       # Save today's date\n",
    "    lastPnFUpdate = myResultDate   # Save the last trading date from the historical data table\n",
    "    td = todayDate - lastPnFUpdate # Calculate the number of days since the last trading date\n",
    "    td_days = td.days              # Save the date difference calculation\n",
    "\n",
    "    print(\"The last day that prices were updated was\", lastPnFUpdate.strftime('%m/%d/%Y'))\n",
    "    print(\"That date was\", td_days, \"days ago.\")\n",
    "\n",
    "    return lastPnFUpdate, td_days, todayDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new data from the Price History table for each ticker to append to the P&F history table.\n",
    "\n",
    "@task\n",
    "def get_price_data(todayDate, lastPnFUpdate):\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    bad_tickers = []\n",
    "    \n",
    "    global df_price_data\n",
    "    global nextDateString\n",
    "    global df_price_update_total\n",
    "    global myResultData\n",
    "    \n",
    "    df_price_update_total = pd.DataFrame()\n",
    "\n",
    "    # For each day from the last price update to today, retrieve the new security prices from the Price History table.\n",
    "    \n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "    \n",
    "    mycursor.execute(\"SELECT * FROM price_data_historical WHERE date BETWEEN '\" + lastPnFUpdate.strftime('%Y-%m-%d') + \\\n",
    "                     \"' AND '\" + todayDate.strftime('%Y-%m-%d') + \"';\")\n",
    "    \n",
    "    myResultData = mycursor.fetchall()\n",
    "\n",
    "    columns = ['key_id', 'ticker', 'figi', 'date', 'open', 'high', 'low', 'close', 'volume', 'adj_open', 'adj_high', \n",
    "               'adj_low', 'adj_close', 'adj_volume', 'adj_factor', 'split_ratio', 'change', 'percent_change', \n",
    "               'fifty_two_week_high', 'fifty_two_week_low', 'market_cap', 'weighted_avg_shares_out', 'intraperiod', \n",
    "               'last_updated_date', 'last_corp_action_date']\n",
    "\n",
    "    df_price_data = pd.DataFrame(myResultData, columns = columns)  # Save the records from the price history table\n",
    "                                                                   # that we will apply P&F calculations to.\n",
    "    \n",
    "    # Add columns for Plot Symbol, Reversal, Signal Name and Percent Change and other P&F calcs\n",
    "    \n",
    "    df_price_data['plot_symbol'] = np.nan\n",
    "    df_price_data['reversal'] = 0\n",
    "    df_price_data['signal_name'] = np.nan\n",
    "    df_price_data['high_point'] = np.nan\n",
    "    df_price_data['last_high_point'] = np.nan\n",
    "    df_price_data['prev_high_point'] = np.nan\n",
    "    df_price_data['low_point'] = np.nan\n",
    "    df_price_data['last_low_point'] = np.nan\n",
    "    df_price_data['prev_low_point'] = np.nan\n",
    "    df_price_data['entry_x'] = np.nan\n",
    "    df_price_data['entry_o'] = np.nan\n",
    "    df_price_data['next_entry'] = np.nan\n",
    "    df_price_data['stop_loss'] = np.nan\n",
    "    df_price_data['target_price'] = np.nan\n",
    "\n",
    "    # Reorder the columns\n",
    "    \n",
    "    df_price_data = df_price_data[['key_id', 'date', 'figi', 'ticker', 'open', 'high', 'low', 'close', 'change', 'percent_change', 'volume', 'plot_symbol', 'reversal', \n",
    "                 'signal_name', 'high_point', 'last_high_point', 'prev_high_point', 'low_point', 'last_low_point', 'prev_low_point', 'entry_x', 'entry_o', \n",
    "                 'next_entry', 'stop_loss', 'target_price', 'last_updated_date', 'last_corp_action_date']]\n",
    "\n",
    "    print(\"The shape of the new price data DF is\", df_price_data.shape)\n",
    "    \n",
    "    return df_price_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last record for each stock from the historical database and append them to the update DF.\n",
    "\n",
    "@task\n",
    "def get_last_records(lastPnFUpdate):\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    global df_last_records\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT * FROM base_pnf_data_historical WHERE date = '\" + lastPnFUpdate.strftime('%Y-%m-%d') + \"'\")\n",
    "\n",
    "    myresult = mycursor.fetchall()\n",
    "    \n",
    "    myColumns = ['key_id', 'date', 'figi', 'ticker', 'open', 'high', 'low', 'close', 'change', 'percent_change', 'volume', 'plot_symbol', 'reversal', \n",
    "                 'signal_name', 'high_point', 'last_high_point', 'prev_high_point', 'low_point', 'last_low_point', 'prev_low_point', 'entry_x', 'entry_o', \n",
    "                 'next_entry', 'stop_loss', 'target_price', 'last_updated_date','last_corp_action_date']\n",
    "    \n",
    "    df_last_records = pd.DataFrame(myresult, columns = myColumns) # Save the P&F records from the last trading date\n",
    "    \n",
    "    print(\"The shape of the last active records DF is\", df_last_records.shape)\n",
    "\n",
    "    return df_last_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the new price data with the last active records from the history database to get the starting P&F values\n",
    "# for the new data.\n",
    "\n",
    "@task\n",
    "def join_records(df_price_data, df_last_records):\n",
    "    \n",
    "    global df_pnf_update\n",
    "\n",
    "    df_pnf_update = pd.concat([df_price_data, df_last_records])\n",
    "    df_pnf_update.sort_values(by = ['date', 'ticker', 'plot_symbol'], inplace = True)\n",
    "    df_pnf_update.drop_duplicates(subset=['key_id'], keep = 'first', inplace = True)\n",
    "    df_pnf_update.sort_values(by = ['ticker', 'date'], inplace = True)\n",
    "    \n",
    "    print(\"The shape of the new combined DF is\", df_pnf_update.shape)\n",
    "    \n",
    "    return df_pnf_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c348b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pnf_calcs(myFigi):\n",
    "    \n",
    "    boxSize = .02\n",
    "    reversalBoxes = 3\n",
    "    reversalAmount = boxSize * reversalBoxes\n",
    "\n",
    "    new_data_list = []\n",
    "    \n",
    "    data = df_pnf_update.loc[df_pnf_update['figi'] == myFigi].copy()\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # Set all starting High Points and Low Points equal to the last record for each ticker from the historical data table.\n",
    "\n",
    "    high_point = data['high_point'].iloc[0]\n",
    "    low_point = data['low_point'].iloc[0]\n",
    "    last_high_point = data['last_high_point'].iloc[0]\n",
    "    last_low_point = data['last_low_point'].iloc[0]\n",
    "    entry_x = data['entry_x'].iloc[0]\n",
    "    entry_o = data['entry_o'].iloc[0]\n",
    "    prev_high_point = data['prev_high_point'].iloc[0]\n",
    "    prev_low_point = data['prev_low_point'].iloc[0]\n",
    "    target_price = data['target_price'].iloc[0]\n",
    "\n",
    "    # Start the loop on the second day, loop through each day's close price after that.\n",
    "    for i in range(1, len(data)):\n",
    "\n",
    "        if data['plot_symbol'].iloc[i - 1] == 'X':   #If previous Plot Symbol = \"X\", then:\n",
    "\n",
    "            if data['close'].iloc[i] >= data['close'].iloc[i - 1]:     #If current price >= previous price, then:\n",
    "                data.loc[i, 'plot_symbol'] = 'X'        # Today's Plot Symbol = \"X\".\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]    #and copy yesterday's signal to today.\n",
    "\n",
    "                if data['close'].iloc[i] > high_point:    #And if today's price is higher than the most recent high price, \n",
    "                    high_point = data['close'].iloc[i]       #then make today's price the  high price,\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "                if data['close'].iloc[i] > last_high_point:  #And if today's price is higher than the high point from the last X column,\n",
    "                    data.loc[i, 'signal_name'] = \"BUY\"           #then today's signal = \"BUY\".\n",
    "\n",
    "            elif data['close'].iloc[i] < high_point * (1 - reversalAmount):     #Else if today's price is less than the previous high times 1 - reversal,\n",
    "                data.loc[i, 'plot_symbol'] = 'O'                                     #the Plot Symbol reverses to \"O\",\n",
    "                low_point = data['close'].iloc[i]                                   #and the  low point is today's price,\n",
    "                data.loc[i, 'reversal'] = 1                                         #and reversal = 1,\n",
    "                prev_high_point = last_high_point                                        #and prev_high_point = last_high_point, saving this ValueSignal to use in the Target Price calc below\n",
    "                last_high_point = high_point                                               #and last_high_point = most recent high point\n",
    "                entry_o = data['close'].iloc[i - 1]                                 #and entry_o = previous day's closing price, used in next_entry and stop_loss calcs\n",
    "\n",
    "                if data['close'].iloc[i] < last_low_point:   #And if today's price is lower than the low point from the last O column,\n",
    "                    data.loc[i, 'signal_name'] = \"SELL\"          #then today's signal = \"SELL\".\n",
    "                else:\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #Else copy yesterday's signal to today.\n",
    "\n",
    "            else:\n",
    "                data.loc[i, 'plot_symbol'] = 'X'  #Else, Plot Symbol = \"X\" (price is down but not enough to triger a reversal)\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "\n",
    "        if data['plot_symbol'].iloc[i - 1] == 'O':   #If previous Plot Symbol = \"O\", then:\n",
    "\n",
    "            if data['close'].iloc[i] < data['close'].iloc[i - 1]:            #If current price <= previous price, then:\n",
    "                data.loc[i, 'plot_symbol'] = 'O'         # Today's Plot Symbol = \"O\".\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]\n",
    "\n",
    "                if data['close'].iloc[i] < low_point:       #And if today's price is lower than the most recent low price, \n",
    "                    low_point = data['close'].iloc[i]         #then make today's price the  low price.\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "                if data['close'].iloc[i] < last_low_point:   #And if today's price is lower than the low point from the last O column,\n",
    "                    data.loc[i, 'signal_name'] = \"SELL\"         #then today's signal = \"SELL\".\n",
    "\n",
    "\n",
    "            elif data['close'].iloc[i] > low_point * (1 + reversalAmount):       #Else if today's price is greater than the previous high, times 1 + reversal,\n",
    "                data.loc[i, 'plot_symbol'] = 'X'                                       #the Plot Symbol reverses to \"X\",\n",
    "                high_point = data['close'].iloc[i]                                    #and the  high point is today's price,\n",
    "                data.loc[i, 'reversal'] = 1                                           #and reversal = 1,\n",
    "                prev_low_point = last_low_point                                            ##and prev_low_point = last_low_point, saving this ValueSignal to use in the Target Price calc below\n",
    "                last_low_point = low_point                                                   #and last_low_point = most recent low point\n",
    "                entry_x = data['close'].iloc[i - 1]                                   #and entry_x = previous day's closing price, used in next_entry and stop_loss calcs\n",
    "\n",
    "                if data['close'].iloc[i] > last_high_point:  #And if today's price is higher than the high point from the last X column,\n",
    "                    data.loc[i, 'signal_name'] = \"BUY\"          #then today's signal = \"BUY\".\n",
    "\n",
    "                else:\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]     #Else copy yesterday's signal to today.\n",
    "\n",
    "            else:\n",
    "                data.loc[i, 'plot_symbol'] = 'O'  #Else, Plot Symbol = \"O\" (price is up but not enough to triger a reversal)\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "        data.loc[i, 'high_point'] = high_point            #high_point = current \"high_point\"\n",
    "        data.loc[i, 'low_point'] = low_point             #low_point = current \"low_point\"\n",
    "        data.loc[i, 'last_high_point'] = last_high_point  #last_high_point = current \"last_high_point\"\n",
    "        data.loc[i, 'last_low_point'] = last_low_point    #last_low_point = current \"last_low_point\"\n",
    "        data.loc[i, 'prev_high_point'] = prev_high_point  #prev_high_point = current \"prev_high_point\"\n",
    "        data.loc[i, 'prev_low_point'] = prev_low_point    #prev_low_point = current \"prev_low_point\"\n",
    "\n",
    "        if data['signal_name'].iloc[i] == \"BUY\":\n",
    "\n",
    "            next_entry = entry_o * (1 + boxSize)         #Set next_entry at one box up from the price at the last reversal from X to O, which should be near the top of the previous X column\n",
    "            data.loc[i, 'next_entry'] = next_entry\n",
    "            stop_loss = entry_x * (1 - boxSize)          #Set the stop_loss at one box down from the price at the last reversal from O to X, which should be near the bottom of the previous O column\n",
    "            data.loc[i, 'stop_loss'] = stop_loss\n",
    "\n",
    "            if data['signal_name'].iloc[i - 1] == \"SELL\":\n",
    "                target_price = ((last_high_point - prev_low_point) * reversalBoxes) + prev_low_point   #Upon reversal from SELL to BUY, set the target_price equal to the size of the previous X column,\n",
    "                                                                                                # times the box size, added to the bottom of the previous X column. Once calculated, it does not\n",
    "                                                                                                # change for the balance of the current BUY signal.\n",
    "            data.loc[i, 'target_price'] = target_price\n",
    "\n",
    "        else:\n",
    "            next_entry = entry_x * (1 - boxSize)         #Set next_entry at one box down from the price at the last reversal from O to X, which should be near the bottom of the previous O column\n",
    "            data.loc[i, 'next_entry'] = next_entry\n",
    "            stop_loss = entry_o * (1 + boxSize)          #Set the stop_loss at one box up from the price at the last reversal from X to O, which should be near the top of the previous X column\n",
    "            data.loc[i, 'stop_loss'] = stop_loss\n",
    "\n",
    "            if data['signal_name'].iloc[i - 1] == \"BUY\":\n",
    "                target_price = prev_high_point - ((prev_high_point - last_low_point) * reversalBoxes)  #Upon reversal from BUY to SELL, set the target_price equal to the size of the previous O column,\n",
    "                                                                                                # times the box size, subtracted from the top of the previous O column. Once calculated, it does not\n",
    "                                                                                                # change for the balance of the current SELL signal.\n",
    "            data.loc[i, 'target_price'] = target_price\n",
    "            \n",
    "        data.loc[i, 'entry_x'] = entry_x            #entry_x = current \"entry_x\"\n",
    "        data.loc[i, 'entry_o'] = entry_o            #entry_o = current \"entry_o\"\n",
    "\n",
    "    data_list = data.values.tolist()\n",
    "    new_data_list.extend(data_list)\n",
    "    \n",
    "    return new_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the calculations and prepare final dataframe.\n",
    "\n",
    "@task\n",
    "def run_all_calcs(df_pnf_update):\n",
    "    \n",
    "    global df_pnf_update_load\n",
    "\n",
    "    import multiprocessing\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    new_data_list = []\n",
    "\n",
    "    figi_list = df_pnf_update['figi'].unique().tolist() # Get the list of FIGI codes to run the calculations against.\n",
    "\n",
    "    p = Pool()\n",
    "    result = p.map(generate_pnf_calcs, figi_list)  # Use multiprocessing pool to spread the work over all available chip cores\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(\"Elapsed time was\", round(elapsed_time/60, 2), \"minutes.\")\n",
    "\n",
    "    new_data_list = []\n",
    "\n",
    "    for i in range(0, len(figi_list)):  # Convert the MP pool results to a list of values\n",
    "        data_list = result[i]\n",
    "        new_data_list.extend(data_list)\n",
    "\n",
    "    myColumns = ['key_id', 'date', 'figi', 'ticker', 'open', 'high', 'low', 'close', 'change', 'percent_change', 'volume', 'plot_symbol', 'reversal', \n",
    "                 'signal_name', 'high_point', 'last_high_point', 'prev_high_point', 'low_point', 'last_low_point', 'prev_low_point', 'entry_x', 'entry_o', \n",
    "                 'next_entry', 'stop_loss', 'target_price', 'last_updated_date','last_corp_action_date']\n",
    "\n",
    "    df_pnf_data = pd.DataFrame(new_data_list, columns = myColumns)  # Save the pool results list to a dataframe\n",
    "\n",
    "    # Save the dataframe to a CSV file in case you need to refer to it later.\n",
    "    df_pnf_data.to_csv(path_or_buf = my_path + \"/df_pnf_data_update_\" + todayDate.strftime('%Y-%m-%d') + \".csv\", index=False)\n",
    "\n",
    "    print(\"The intermediate dataframe shape is \", df_pnf_data.shape)\n",
    "    \n",
    "    # Make sure the date column is in datetime format and remove the records from the last trading day so there is no overlap with the database.\n",
    "    df_pnf_update_load = df_pnf_data.copy()  \n",
    "    df_pnf_update_load['date'] = pd.to_datetime(df_pnf_update_load['date'])\n",
    "    df_pnf_update_load['last_updated_date'] = pd.to_datetime(df_pnf_update_load['date'].max()).normalize()\n",
    "    df_pnf_update_load = df_pnf_update_load[df_pnf_update_load['date'] != lastPnFUpdate.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    print(\"The shape of the dataframe to load is \", df_pnf_update_load.shape)\n",
    "    \n",
    "    # Confirm that the date range for the new data is what you expect to see.\n",
    "    startDate = df_pnf_update_load['date'].min().strftime('%Y-%m-%d')\n",
    "    endDate = df_pnf_update_load['date'].max().strftime('%Y-%m-%d')\n",
    "    print(\"The date range of the dataframe to load goes from \", startDate, \" to \", endDate)\n",
    "    \n",
    "    return df_pnf_update_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataframe to CSV on S3 if you want to use AWS Lambda to take it from there and push it into \n",
    "# the RDS table.\n",
    "\n",
    "@task\n",
    "def push_pnf_data_to_S3(df_pnf_update_load):\n",
    "\n",
    "    import io\n",
    "    \n",
    "    if len(df_pnf_update_load) > 0:\n",
    "\n",
    "        # Create the AWS client\n",
    "        client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id = aws_key,\n",
    "            aws_secret_access_key = aws_secret_key,\n",
    "            region_name = 'us-east-1'\n",
    "        )\n",
    "\n",
    "        myBucket = 'bns-intrinio-data'\n",
    "        myFileLocation = \"price-data-daily/df_pnf_update_load_\" + todayDate.strftime('%Y-%m-%d') + \".csv\"\n",
    "\n",
    "        with io.StringIO() as csv_buffer:\n",
    "            df_pnf_update_load.to_csv(csv_buffer, index=False)\n",
    "\n",
    "            response = client.put_object(\n",
    "                Bucket = myBucket, Key = myFileLocation, Body=csv_buffer.getvalue()\n",
    "            )\n",
    "\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "            if status == 200:\n",
    "                print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "            else:\n",
    "                print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQLAlchemy to push the final dataframe into SQL DB on AWS RDS:\n",
    "\n",
    "@task\n",
    "def push_pnf_data_to_RDS(df_pnf_update_load):\n",
    "    \n",
    "    if len(df_pnf_update_load) > 0:\n",
    "\n",
    "        # Set database credentials.\n",
    "        creds = {'usr': rds_user,\n",
    "                 'pwd': rds_password,\n",
    "                 'hst': rds_host,\n",
    "                 'prt': 3306,\n",
    "                 'dbn': rds_database}\n",
    "\n",
    "        # MySQL conection string.\n",
    "        connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "        # Create sqlalchemy engine for MySQL connection.\n",
    "        engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "        # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "        df_pnf_update_load.to_sql(name='base_pnf_data_historical', \n",
    "                                              con=engine, \n",
    "                                              if_exists='append', \n",
    "                                              index=False)\n",
    "\n",
    "        print(\"The new data has been appended to RDS. The number of new rows added is\", df_pnf_update_load.shape[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-catch",
   "metadata": {},
   "source": [
    "## Run the Prefect schedule process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a31c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the daily run schedule.\n",
    "\n",
    "schedule = IntervalSchedule(\n",
    "    start_date=pendulum.datetime(2021, 12, 21, 21, 0, 0, tz=\"America/New_York\"),\n",
    "    interval=timedelta(days=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ETL update flow.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with Flow(\"Stock-Data-Update-ETL\", schedule) as flow:\n",
    "\n",
    "        # Stock price updates\n",
    "        get_max_update_date = get_max_update_date()\n",
    "        get_recent_price_data = get_recent_price_data(lastPriceUpdate, td_days, upstream_tasks=[get_max_update_date])\n",
    "        get_latest_shares_out_data = get_latest_shares_out_data(df_price_update_total, upstream_tasks=[get_recent_price_data])\n",
    "        create_complete_update_dataframe = create_complete_update_dataframe(df_latest_shares_out, \n",
    "                                                                            df_price_update_total, \n",
    "                                                                            upstream_tasks=[get_latest_shares_out_data])\n",
    "        data_to_s3 = push_data_to_S3(df_price_update_complete, upstream_tasks=[create_complete_update_dataframe])\n",
    "        data_to_rds = push_data_to_RDS(df_price_update_complete, upstream_tasks=[create_complete_update_dataframe])\n",
    "        \n",
    "        # Corp actions updates\n",
    "        get_max_corax_date = get_max_corax_date(upstream_tasks=[data_to_rds])\n",
    "        get_adjusted_tickers_figis = get_adjusted_tickers_figis(lastCoraxUpdate, td_days, upstream_tasks=[get_max_corax_date])\n",
    "        get_adj_factors_splits = get_adj_factors_splits(df_adjusted_tickers_total,upstream_tasks=[get_adjusted_tickers_figis])\n",
    "        get_adjusted_price_data = get_adjusted_price_data(df_adjusted_tickers_total, upstream_tasks=[get_adjusted_tickers_figis])\n",
    "        get_historical_shares_out_data = get_historical_shares_out_data(df_adjusted_tickers_total, upstream_tasks=[get_adjusted_price_data])\n",
    "        combine_transform_adjusted_data = combine_transform_adjusted_data(df_adjusted_prices_total, df_hist_shares_out, \n",
    "                                                                          upstream_tasks=[get_historical_shares_out_data])\n",
    "        export_data_to_S3 = export_data_to_S3(df_adjusted_prices_complete, upstream_tasks=[combine_transform_adjusted_data])\n",
    "        save_preadjusted_records_to_S3 = save_preadjusted_records_to_S3(df_adjusted_prices_complete, upstream_tasks=[export_data_to_S3])\n",
    "        delete_preadjusted_records = delete_preadjusted_records(save_records_completion_status, upstream_tasks=[save_preadjusted_records_to_S3])\n",
    "        insert_new_records = insert_new_records(delete_records_status, upstream_tasks=[delete_preadjusted_records])\n",
    "\n",
    "        # Point & Figure data update\n",
    "        get_max_pnf_date = get_max_pnf_date(upstream_tasks=[insert_new_records])\n",
    "        get_price_data = get_price_data(todayDate, lastPnFUpdate, upstream_tasks=[get_max_pnf_date])\n",
    "        get_last_records = get_last_records(lastPnFUpdate, upstream_tasks=[get_price_data])\n",
    "        join_records = join_records(df_price_data, df_last_records, upstream_tasks=[get_last_records])\n",
    "        run_all_calcs = run_all_calcs(df_pnf_update, upstream_tasks=[join_records])\n",
    "\n",
    "        push_pnf_data_to_S3 = push_pnf_data_to_S3(df_pnf_update_load,upstream_tasks=[run_all_calcs])\n",
    "        push_pnf_data_to_RDS = push_pnf_data_to_RDS(df_pnf_update_load, upstream_tasks=[run_all_calcs])\n",
    "        \n",
    "    flow.set_reference_tasks([data_to_rds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-snake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "turkish-instrumentation",
   "metadata": {},
   "source": [
    "## Extra or alternative functions if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_new_records.run(delete_records_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ee915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Price Update ETL process.\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Stock price updates\n",
    "get_max_update_date.run()\n",
    "get_recent_price_data.run(lastPriceUpdate, td_days)\n",
    "get_latest_shares_out_data.run(df_price_update_total)\n",
    "create_complete_update_dataframe.run(df_latest_shares_out, df_price_update_total)\n",
    "push_data_to_S3.run(df_price_update_complete)\n",
    "push_data_to_RDS.run(df_price_update_complete)\n",
    "\n",
    "# Corporate actions updates\n",
    "get_max_corax_date.run()\n",
    "get_adjusted_tickers_figis.run(lastCoraxUpdate, td_days)\n",
    "get_adj_factors_splits.run(df_adjusted_tickers_total)\n",
    "get_adjusted_price_data.run(df_adjusted_tickers_total)\n",
    "get_historical_shares_out_data.run(df_adjusted_tickers_total)\n",
    "combine_transform_adjusted_data.run(df_adjusted_prices_total, df_hist_shares_out)\n",
    "export_data_to_S3.run(df_adjusted_prices_complete)\n",
    "save_preadjusted_records_to_S3.run(df_adjusted_prices_complete)\n",
    "delete_preadjusted_records.run(save_records_completion_status)\n",
    "insert_new_records.run(delete_records_status)\n",
    "\n",
    "# P&F data update\n",
    "get_max_pnf_date.run()\n",
    "get_price_data.run(todayDate, lastPnFUpdate)\n",
    "get_last_records.run(lastPnFUpdate)\n",
    "join_records.run(df_price_data, df_last_records)\n",
    "run_all_calcs.run(df_pnf_update)\n",
    "push_pnf_data_to_S3.run(df_pnf_update_load)\n",
    "push_pnf_data_to_RDS.run(df_pnf_update_load)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time was\", round(elapsed_time/60, 2), \"minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshoot get shares out.\n",
    "\n",
    "arg_list = list(df_price_update_total[['figi', 'ticker']].drop_duplicates().to_records(index = False))[0:500]\n",
    "\n",
    "global shares_out_list\n",
    "global shares_out_lists_combined\n",
    "\n",
    "shares_out_lists_combined = []\n",
    "bad_tickers = []\n",
    "\n",
    "#identifier = myFigi\n",
    "tag = 'adjweightedavedilutedsharesos'\n",
    "frequency = ''\n",
    "type = ''\n",
    "start_date = ''\n",
    "end_date = ''\n",
    "sort_order = 'desc'\n",
    "page_size = 2\n",
    "next_page = ''\n",
    "\n",
    "for item in arg_list:\n",
    "    \n",
    "    myFigi = item[0]\n",
    "    myTicker = item[1]\n",
    "    identifier = myFigi    \n",
    "    \n",
    "    try:\n",
    "        response = intrinio.HistoricalDataApi().get_historical_data(identifier, tag, frequency=frequency, type=type, start_date=start_date, end_date=end_date, sort_order=sort_order, page_size=page_size, next_page=next_page)\n",
    "        shares_out_data = response.historical_data\n",
    "\n",
    "        shares_out_list = []\n",
    "\n",
    "        for item in range(len(shares_out_data)):\n",
    "\n",
    "            # Add the ticker and figi values to the results\n",
    "            dict_item = shares_out_data[item].to_dict()\n",
    "            dict_item['ticker'] = myTicker\n",
    "            dict_item['figi'] = myFigi\n",
    "            shares_out_list.append(dict_item)\n",
    "            shares_out_lists_combined.extend(shares_out_list)\n",
    "\n",
    "    except:\n",
    "\n",
    "        # Track any tickers that do not have shares outstanding data available.\n",
    "        bad_tickers.append(myTicker)\n",
    "        pass\n",
    "\n",
    "    time.sleep( 0.5 )\n",
    "\n",
    "pd.DataFrame(shares_out_lists_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method for inserting new updated records in the SQL history table.\n",
    "\n",
    "def insert_new_records(df_price_update_complete):\n",
    "\n",
    "    import pymysql.cursors\n",
    "    \n",
    "    global insert_records_status\n",
    "\n",
    "    rowCount = 0\n",
    "\n",
    "    connection = pymysql.connect(host = rds_host,\n",
    "                                 user = rds_user, \n",
    "                                 password = rds_password, \n",
    "                                 database = rds_database,\n",
    "                                 charset = rds_charset,\n",
    "                                 cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "    mycursor = connection.cursor()\n",
    "\n",
    "    sql_insert_query = \"\"\"\n",
    "    INSERT INTO price_data_historical \n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_price_update_complete.copy()\n",
    "\n",
    "    def chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "    for chunk in tqdm(chunker(df, int(len(df)/10))):\n",
    "\n",
    "        data = chunk.values.tolist()\n",
    "        mycursor.executemany(sql_insert_query, data)\n",
    "        connection.commit()\n",
    "        rowCount = rowCount + mycursor.rowcount\n",
    "\n",
    "    print(rowCount, \"Data records inserted.\")\n",
    "    insert_records_status = \"Inserted. Done.\"\n",
    "\n",
    "    \n",
    "    return insert_records_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-automation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
