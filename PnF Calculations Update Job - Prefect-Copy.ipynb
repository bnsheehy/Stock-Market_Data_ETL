{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passing-wagon",
   "metadata": {},
   "source": [
    "## Update Stock Prices and run PnF calcs for each new trading day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controlling-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials\n",
    "\n",
    "import json\n",
    "f = open(\"/. .<your file path here> . . /credentials.json\")\n",
    "credentials = json.load(f)\n",
    "\n",
    "file_path = credentials['file_path']\n",
    "intrinio_key = credentials['intrinio_key']\n",
    "aws_key = credentials['aws_access_key']\n",
    "aws_secret_key = credentials['aws_secret_key']\n",
    "rds_host = credentials['rds_host']\n",
    "rds_user = credentials['rds_user']\n",
    "rds_password = credentials['rds_password']\n",
    "rds_database = credentials['rds_database']\n",
    "rds_charset = credentials['rds_charset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "former-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Intrinio libraries\n",
    "\n",
    "import time\n",
    "import intrinio_sdk as intrinio\n",
    "from intrinio_sdk.rest import ApiException\n",
    "\n",
    "intrinio.ApiClient().configuration.api_key['api_key'] = intrinio_key\n",
    "\n",
    "# Import the usual Python libraries\n",
    "\n",
    "from tqdm.notebook import tqdm, trange  # to be used to track progress in loop iterations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "from datetime import datetime, date, time, timedelta\n",
    "\n",
    "# Import Prefect library\n",
    "\n",
    "from prefect.triggers import all_successful, all_failed\n",
    "from prefect import task, Flow\n",
    "import pendulum\n",
    "from prefect.schedules import IntervalSchedule\n",
    "from prefect.schedules.clocks import IntervalClock\n",
    "\n",
    "# Import the AWS libraries\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Import SQL libraries\n",
    "\n",
    "import mysql.connector \n",
    "from mysql.connector import errorcode\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Declare the local File Path:\n",
    "\n",
    "global my_path\n",
    "my_path = file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "detected-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the last (max) date from the Point & Figure history table\n",
    "\n",
    "@task\n",
    "def get_max_pnf_date():\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    global lastPnFUpdate\n",
    "    global td_days\n",
    "    global todayDate\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT MAX(date) FROM base_pnf_data_historical\")\n",
    "\n",
    "    myResultDate = mycursor.fetchall()[0][0].date()\n",
    "\n",
    "    todayDate = date.today()       # Save today's date\n",
    "    lastPnFUpdate = myResultDate   # Save the last trading date from the historical data table\n",
    "    td = todayDate - lastPnFUpdate # Calculate the number of days since the last trading date\n",
    "    td_days = td.days              # Save the date difference calculation\n",
    "\n",
    "    print(\"The last day that prices were updated was\", lastPnFUpdate.strftime('%m/%d/%Y'))\n",
    "    print(\"That date was\", td_days, \"days ago.\")\n",
    "\n",
    "    return lastPnFUpdate, td_days, todayDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "available-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new data from the Price History table for each ticker to append to the P&F history table.\n",
    "\n",
    "@task\n",
    "def get_price_data(todayDate, lastPnFUpdate):\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    bad_tickers = []\n",
    "    \n",
    "    global df_price_data\n",
    "    global nextDateString\n",
    "    global df_price_update_total\n",
    "    global myResultData\n",
    "    \n",
    "    df_price_update_total = pd.DataFrame()\n",
    "\n",
    "    # For each day from the last price update to today, retrieve the new security prices from the Price History table.\n",
    "    \n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "    \n",
    "    mycursor.execute(\"SELECT * FROM price_data_historical WHERE date BETWEEN '\" + lastPnFUpdate.strftime('%Y-%m-%d') + \\\n",
    "                     \"' AND '\" + todayDate.strftime('%Y-%m-%d') + \"';\")\n",
    "    \n",
    "    myResultData = mycursor.fetchall()\n",
    "\n",
    "    columns = ['key_id', 'ticker', 'figi', 'date', 'open', 'high', 'low', 'close', 'volume', 'adj_open', 'adj_high', \n",
    "               'adj_low', 'adj_close', 'adj_volume', 'adj_factor', 'split_ratio', 'change', 'percent_change', \n",
    "               'fifty_two_week_high', 'fifty_two_week_low', 'market_cap', 'weighted_avg_shares_out', 'intraperiod', \n",
    "               'last_updated_date', 'last_corp_action_date']\n",
    "\n",
    "    df_price_data = pd.DataFrame(myResultData, columns = columns)  # Save the records from the price history table\n",
    "                                                                   # that we will apply P&F calculations to.\n",
    "    \n",
    "    # Add columns for Plot Symbol, Reversal, Signal Name and Percent Change and other P&F calcs\n",
    "    \n",
    "    df_price_data['plot_symbol'] = np.nan\n",
    "    df_price_data['reversal'] = 0\n",
    "    df_price_data['signal_name'] = np.nan\n",
    "    df_price_data['high_point'] = np.nan\n",
    "    df_price_data['last_high_point'] = np.nan\n",
    "    df_price_data['prev_high_point'] = np.nan\n",
    "    df_price_data['low_point'] = np.nan\n",
    "    df_price_data['last_low_point'] = np.nan\n",
    "    df_price_data['prev_low_point'] = np.nan\n",
    "    df_price_data['entry_x'] = np.nan\n",
    "    df_price_data['entry_o'] = np.nan\n",
    "    df_price_data['next_entry'] = np.nan\n",
    "    df_price_data['stop_loss'] = np.nan\n",
    "    df_price_data['target_price'] = np.nan\n",
    "\n",
    "    # Reorder the columns\n",
    "    \n",
    "    df_price_data = df_price_data[['key_id', 'date', 'figi', 'ticker', 'open', 'high', 'low', 'close', 'change', 'percent_change', 'volume', 'plot_symbol', 'reversal', \n",
    "                 'signal_name', 'high_point', 'last_high_point', 'prev_high_point', 'low_point', 'last_low_point', 'prev_low_point', 'entry_x', 'entry_o', \n",
    "                 'next_entry', 'stop_loss', 'target_price', 'last_updated_date', 'last_corp_action_date']]\n",
    "\n",
    "    print(\"The shape of the new price data DF is\", df_price_data.shape)\n",
    "    \n",
    "    return df_price_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "altered-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last record for each stock from the historical database and append them to the update DF.\n",
    "\n",
    "@task\n",
    "def get_last_records(lastPnFUpdate):\n",
    "\n",
    "    from datetime import datetime, date, time, timedelta\n",
    "\n",
    "    global df_last_records\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT * FROM base_pnf_data_historical WHERE date = '\" + lastPnFUpdate.strftime('%Y-%m-%d') + \"'\")\n",
    "\n",
    "    myresult = mycursor.fetchall()\n",
    "    \n",
    "    myColumns = ['key_id', 'date', 'figi', 'ticker', 'open', 'high', 'low', 'close', 'change', 'percent_change', 'volume', 'plot_symbol', 'reversal', \n",
    "                 'signal_name', 'high_point', 'last_high_point', 'prev_high_point', 'low_point', 'last_low_point', 'prev_low_point', 'entry_x', 'entry_o', \n",
    "                 'next_entry', 'stop_loss', 'target_price', 'last_updated_date','last_corp_action_date']\n",
    "    \n",
    "    df_last_records = pd.DataFrame(myresult, columns = myColumns) # Save the P&F records from the last trading date\n",
    "    \n",
    "    print(\"The shape of the last active records DF is\", df_last_records.shape)\n",
    "\n",
    "    return df_last_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intense-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the new price data with the last active records from the history database to get the starting P&F values\n",
    "# for the new data.\n",
    "\n",
    "@task\n",
    "def join_records(df_price_data, df_last_records):\n",
    "    \n",
    "    global df_pnf_update\n",
    "\n",
    "    df_pnf_update = pd.concat([df_price_data, df_last_records])\n",
    "    df_pnf_update.sort_values(by = ['date', 'ticker', 'plot_symbol'], inplace = True)\n",
    "    df_pnf_update.drop_duplicates(subset=['key_id'], keep = 'first', inplace = True)\n",
    "    df_pnf_update.sort_values(by = ['ticker', 'date'], inplace = True)\n",
    "    \n",
    "    print(\"The shape of the new combined DF is\", df_pnf_update.shape)\n",
    "    \n",
    "    return df_pnf_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pending-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pnf_calcs(myFigi):\n",
    "    \n",
    "    boxSize = .02\n",
    "    reversalBoxes = 3\n",
    "    reversalAmount = boxSize * reversalBoxes\n",
    "\n",
    "    new_data_list = []\n",
    "    \n",
    "    data = df_pnf_update.loc[df_pnf_update['figi'] == myFigi].copy()\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # Set all starting High Points and Low Points equal to the last record for each ticker from the historical data table.\n",
    "\n",
    "    high_point = data['high_point'].iloc[0]\n",
    "    low_point = data['low_point'].iloc[0]\n",
    "    last_high_point = data['last_high_point'].iloc[0]\n",
    "    last_low_point = data['last_low_point'].iloc[0]\n",
    "    entry_x = data['entry_x'].iloc[0]\n",
    "    entry_o = data['entry_o'].iloc[0]\n",
    "    prev_high_point = data['prev_high_point'].iloc[0]\n",
    "    prev_low_point = data['prev_low_point'].iloc[0]\n",
    "    target_price = data['target_price'].iloc[0]\n",
    "\n",
    "    # Start the loop on the second day, loop through each day's close price after that.\n",
    "    for i in range(1, len(data)):\n",
    "\n",
    "        if data['plot_symbol'].iloc[i - 1] == 'X':   #If previous Plot Symbol = \"X\", then:\n",
    "\n",
    "            if data['close'].iloc[i] >= data['close'].iloc[i - 1]:     #If current price >= previous price, then:\n",
    "                data.loc[i, 'plot_symbol'] = 'X'        # Today's Plot Symbol = \"X\".\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]    #and copy yesterday's signal to today.\n",
    "\n",
    "                if data['close'].iloc[i] > high_point:    #And if today's price is higher than the most recent high price, \n",
    "                    high_point = data['close'].iloc[i]       #then make today's price the  high price,\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "                if data['close'].iloc[i] > last_high_point:  #And if today's price is higher than the high point from the last X column,\n",
    "                    data.loc[i, 'signal_name'] = \"BUY\"           #then today's signal = \"BUY\".\n",
    "\n",
    "            elif data['close'].iloc[i] < high_point * (1 - reversalAmount):     #Else if today's price is less than the previous high times 1 - reversal,\n",
    "                data.loc[i, 'plot_symbol'] = 'O'                                     #the Plot Symbol reverses to \"O\",\n",
    "                low_point = data['close'].iloc[i]                                   #and the  low point is today's price,\n",
    "                data.loc[i, 'reversal'] = 1                                         #and reversal = 1,\n",
    "                prev_high_point = last_high_point                                        #and prev_high_point = last_high_point, saving this ValueSignal to use in the Target Price calc below\n",
    "                last_high_point = high_point                                               #and last_high_point = most recent high point\n",
    "                entry_o = data['close'].iloc[i - 1]                                 #and entry_o = previous day's closing price, used in next_entry and stop_loss calcs\n",
    "\n",
    "                if data['close'].iloc[i] < last_low_point:   #And if today's price is lower than the low point from the last O column,\n",
    "                    data.loc[i, 'signal_name'] = \"SELL\"          #then today's signal = \"SELL\".\n",
    "                else:\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #Else copy yesterday's signal to today.\n",
    "\n",
    "            else:\n",
    "                data.loc[i, 'plot_symbol'] = 'X'  #Else, Plot Symbol = \"X\" (price is down but not enough to triger a reversal)\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "\n",
    "        if data['plot_symbol'].iloc[i - 1] == 'O':   #If previous Plot Symbol = \"O\", then:\n",
    "\n",
    "            if data['close'].iloc[i] < data['close'].iloc[i - 1]:            #If current price <= previous price, then:\n",
    "                data.loc[i, 'plot_symbol'] = 'O'         # Today's Plot Symbol = \"O\".\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]\n",
    "\n",
    "                if data['close'].iloc[i] < low_point:       #And if today's price is lower than the most recent low price, \n",
    "                    low_point = data['close'].iloc[i]         #then make today's price the  low price.\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "                if data['close'].iloc[i] < last_low_point:   #And if today's price is lower than the low point from the last O column,\n",
    "                    data.loc[i, 'signal_name'] = \"SELL\"         #then today's signal = \"SELL\".\n",
    "\n",
    "\n",
    "            elif data['close'].iloc[i] > low_point * (1 + reversalAmount):       #Else if today's price is greater than the previous high, times 1 + reversal,\n",
    "                data.loc[i, 'plot_symbol'] = 'X'                                       #the Plot Symbol reverses to \"X\",\n",
    "                high_point = data['close'].iloc[i]                                    #and the  high point is today's price,\n",
    "                data.loc[i, 'reversal'] = 1                                           #and reversal = 1,\n",
    "                prev_low_point = last_low_point                                            ##and prev_low_point = last_low_point, saving this ValueSignal to use in the Target Price calc below\n",
    "                last_low_point = low_point                                                   #and last_low_point = most recent low point\n",
    "                entry_x = data['close'].iloc[i - 1]                                   #and entry_x = previous day's closing price, used in next_entry and stop_loss calcs\n",
    "\n",
    "                if data['close'].iloc[i] > last_high_point:  #And if today's price is higher than the high point from the last X column,\n",
    "                    data.loc[i, 'signal_name'] = \"BUY\"          #then today's signal = \"BUY\".\n",
    "\n",
    "                else:\n",
    "                    data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]     #Else copy yesterday's signal to today.\n",
    "\n",
    "            else:\n",
    "                data.loc[i, 'plot_symbol'] = 'O'  #Else, Plot Symbol = \"O\" (price is up but not enough to triger a reversal)\n",
    "                data.loc[i, 'signal_name'] = data['signal_name'].iloc[i - 1]   #and copy yesterday's signal to today.\n",
    "\n",
    "        data.loc[i, 'high_point'] = high_point            #high_point = current \"high_point\"\n",
    "        data.loc[i, 'low_point'] = low_point             #low_point = current \"low_point\"\n",
    "        data.loc[i, 'last_high_point'] = last_high_point  #last_high_point = current \"last_high_point\"\n",
    "        data.loc[i, 'last_low_point'] = last_low_point    #last_low_point = current \"last_low_point\"\n",
    "        data.loc[i, 'prev_high_point'] = prev_high_point  #prev_high_point = current \"prev_high_point\"\n",
    "        data.loc[i, 'prev_low_point'] = prev_low_point    #prev_low_point = current \"prev_low_point\"\n",
    "\n",
    "        if data['signal_name'].iloc[i] == \"BUY\":\n",
    "\n",
    "            next_entry = entry_o * (1 + boxSize)         #Set next_entry at one box up from the price at the last reversal from X to O, which should be near the top of the previous X column\n",
    "            data.loc[i, 'next_entry'] = next_entry\n",
    "            stop_loss = entry_x * (1 - boxSize)          #Set the stop_loss at one box down from the price at the last reversal from O to X, which should be near the bottom of the previous O column\n",
    "            data.loc[i, 'stop_loss'] = stop_loss\n",
    "\n",
    "            if data['signal_name'].iloc[i - 1] == \"SELL\":\n",
    "                target_price = ((last_high_point - prev_low_point) * reversalBoxes) + prev_low_point   #Upon reversal from SELL to BUY, set the target_price equal to the size of the previous X column,\n",
    "                                                                                                # times the box size, added to the bottom of the previous X column. Once calculated, it does not\n",
    "                                                                                                # change for the balance of the current BUY signal.\n",
    "            data.loc[i, 'target_price'] = target_price\n",
    "\n",
    "        else:\n",
    "            next_entry = entry_x * (1 - boxSize)         #Set next_entry at one box down from the price at the last reversal from O to X, which should be near the bottom of the previous O column\n",
    "            data.loc[i, 'next_entry'] = next_entry\n",
    "            stop_loss = entry_o * (1 + boxSize)          #Set the stop_loss at one box up from the price at the last reversal from X to O, which should be near the top of the previous X column\n",
    "            data.loc[i, 'stop_loss'] = stop_loss\n",
    "\n",
    "            if data['signal_name'].iloc[i - 1] == \"BUY\":\n",
    "                target_price = prev_high_point - ((prev_high_point - last_low_point) * reversalBoxes)  #Upon reversal from BUY to SELL, set the target_price equal to the size of the previous O column,\n",
    "                                                                                                # times the box size, subtracted from the top of the previous O column. Once calculated, it does not\n",
    "                                                                                                # change for the balance of the current SELL signal.\n",
    "            data.loc[i, 'target_price'] = target_price\n",
    "            \n",
    "        data.loc[i, 'entry_x'] = entry_x            #entry_x = current \"entry_x\"\n",
    "        data.loc[i, 'entry_o'] = entry_o            #entry_o = current \"entry_o\"\n",
    "\n",
    "    data_list = data.values.tolist()\n",
    "    new_data_list.extend(data_list)\n",
    "    \n",
    "    return new_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incomplete-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the calculations and prepare final dataframe.\n",
    "\n",
    "@task\n",
    "def run_all_calcs(df_pnf_update):\n",
    "    \n",
    "    global df_pnf_update_load\n",
    "\n",
    "    import multiprocessing\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    new_data_list = []\n",
    "\n",
    "    figi_list = df_pnf_update['figi'].unique().tolist() # Get the list of FIGI codes to run the calculations against.\n",
    "\n",
    "    p = Pool()\n",
    "    result = p.map(generate_pnf_calcs, figi_list)  # Use multiprocessing pool to spread the work over all available chip cores\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(\"Elapsed time was\", round(elapsed_time/60, 2), \"minutes.\")\n",
    "\n",
    "    new_data_list = []\n",
    "\n",
    "    for i in range(0, len(figi_list)):  # Convert the MP pool results to a list of values\n",
    "        data_list = result[i]\n",
    "        new_data_list.extend(data_list)\n",
    "\n",
    "    myColumns = ['key_id', 'date', 'figi', 'ticker', 'open', 'high', 'low', 'close', 'change', 'percent_change', 'volume', 'plot_symbol', 'reversal', \n",
    "                 'signal_name', 'high_point', 'last_high_point', 'prev_high_point', 'low_point', 'last_low_point', 'prev_low_point', 'entry_x', 'entry_o', \n",
    "                 'next_entry', 'stop_loss', 'target_price', 'last_updated_date','last_corp_action_date']\n",
    "\n",
    "    df_pnf_data = pd.DataFrame(new_data_list, columns = myColumns)  # Save the pool results list to a dataframe\n",
    "\n",
    "    # Save the dataframe to a CSV file in case you need to refer to it later.\n",
    "    df_pnf_data.to_csv(path_or_buf = my_path + \"/df_pnf_data_update_\" + todayDate.strftime('%Y-%m-%d') + \".csv\", index=False)\n",
    "\n",
    "    print(\"The intermediate dataframe shape is \", df_pnf_data.shape)\n",
    "    \n",
    "    # Make sure the date column is in datetime format and remove the records from the last trading day so there is no overlap with the database.\n",
    "    df_pnf_update_load = df_pnf_data.copy()  \n",
    "    df_pnf_update_load['date'] = pd.to_datetime(df_pnf_update_load['date'])\n",
    "    df_pnf_update_load['last_updated_date'] = pd.to_datetime(df_pnf_update_load['date'].max()).normalize()\n",
    "    df_pnf_update_load = df_pnf_update_load[df_pnf_update_load['date'] != lastPnFUpdate.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    print(\"The shape of the dataframe to load is \", df_pnf_update_load.shape)\n",
    "    \n",
    "    # Confirm that the date range for the new data is what you expect to see.\n",
    "    startDate = df_pnf_update_load['date'].min().strftime('%Y-%m-%d')\n",
    "    endDate = df_pnf_update_load['date'].max().strftime('%Y-%m-%d')\n",
    "    print(\"The date range of the dataframe to load goes from \", startDate, \" to \", endDate)\n",
    "    \n",
    "    return df_pnf_update_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reserved-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataframe to CSV on S3 if you want to use AWS Lambda to take it from there and push it into \n",
    "# the RDS table.\n",
    "\n",
    "@task\n",
    "def push_data_to_S3(df_pnf_update_load):\n",
    "\n",
    "    import io\n",
    "    \n",
    "    if len(df_pnf_update_load) > 0:\n",
    "\n",
    "        # Create the AWS client\n",
    "        client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id = aws_key,\n",
    "            aws_secret_access_key = aws_secret_key,\n",
    "            region_name = 'us-east-1'\n",
    "        )\n",
    "\n",
    "        myBucket = 'bns-intrinio-data'\n",
    "        myFileLocation = \"price-data-daily/df_pnf_update_load_\" + todayDate.strftime('%Y-%m-%d') + \".csv\"\n",
    "\n",
    "        with io.StringIO() as csv_buffer:\n",
    "            df_pnf_update_load.to_csv(csv_buffer, index=False)\n",
    "\n",
    "            response = client.put_object(\n",
    "                Bucket = myBucket, Key = myFileLocation, Body=csv_buffer.getvalue()\n",
    "            )\n",
    "\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "            if status == 200:\n",
    "                print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "            else:\n",
    "                print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faced-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQLAlchemy to push the final dataframe into SQL DB on AWS RDS:\n",
    "\n",
    "@task\n",
    "def push_data_to_RDS(df_pnf_update_load):\n",
    "    \n",
    "    if len(df_pnf_update_load) > 0:\n",
    "\n",
    "        # Set database credentials.\n",
    "        creds = {'usr': rds_user,\n",
    "                 'pwd': rds_password,\n",
    "                 'hst': rds_host,\n",
    "                 'prt': 3306,\n",
    "                 'dbn': rds_database}\n",
    "\n",
    "        # MySQL conection string.\n",
    "        connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "        # Create sqlalchemy engine for MySQL connection.\n",
    "        engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "        # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "        df_pnf_update_load.to_sql(name='base_pnf_data_historical', \n",
    "                                              con=engine, \n",
    "                                              if_exists='append', \n",
    "                                              index=False)\n",
    "\n",
    "        print(\"The new data has been appended to RDS. The number of new rows added is\", df_pnf_update_load.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da14b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the daily run schedule.\n",
    "\n",
    "schedule = IntervalSchedule(\n",
    "    start_date=pendulum.datetime(2021, 12, 19, 21, 0, 0, tz=\"America/New_York\"),\n",
    "    interval=timedelta(days=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82830fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ETL update flow.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with Flow(\"Stock-Data-Update-ETL\", schedule) as flow:\n",
    "        \n",
    "        get_max_pnf_date = get_max_pnf_date()\n",
    "        get_price_data = get_price_data(todayDate, lastPnFUpdate, upstream_tasks=[get_max_pnf_date])\n",
    "        get_last_records = get_last_records(lastPnFUpdate, upstream_tasks=[get_price_data])\n",
    "        join_records = join_records(df_price_data, df_last_records, upstream_tasks=[get_last_records])\n",
    "        run_all_calcs = run_all_calcs(df_pnf_update, upstream_tasks=[join_records])\n",
    "\n",
    "        push_data_to_S3 = push_data_to_S3(df_pnf_update_load,upstream_tasks=[run_all_calcs])\n",
    "        push_data_to_RDS = push_data_to_RDS(df_pnf_update_load, upstream_tasks=[run_all_calcs])\n",
    "\n",
    "    flow.set_reference_tasks([push_data_to_RDS])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45206c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33edbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the update process\n",
    "\n",
    "get_max_pnf_date.run()\n",
    "get_price_data.run(todayDate, lastPnFUpdate)\n",
    "get_last_records.run(lastPnFUpdate)\n",
    "join_records.run(df_price_data, df_last_records)\n",
    "run_all_calcs.run(df_pnf_update)\n",
    "\n",
    "push_data_to_S3.run(df_pnf_update_load)\n",
    "push_data_to_RDS.run(df_pnf_update_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-revision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
