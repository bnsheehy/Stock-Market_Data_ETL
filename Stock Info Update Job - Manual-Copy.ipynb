{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "concrete-thailand",
   "metadata": {},
   "source": [
    "### This module downloads stock and company metadata from Intrinio to fill out and maintain a Stock Info table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aboriginal-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials\n",
    "\n",
    "import json\n",
    "f = open(\"/. .<your file path here> . . /credentials.json\")\n",
    "credentials = json.load(f)\n",
    "\n",
    "file_path = list(credentials.values())[0]\n",
    "intrinio_key = list(credentials.values())[1]\n",
    "aws_key = list(credentials.values())[2]\n",
    "aws_secret_key = list(credentials.values())[3]\n",
    "rds_host = list(credentials.values())[4]\n",
    "rds_user = list(credentials.values())[5]\n",
    "rds_password = list(credentials.values())[6]\n",
    "rds_database = list(credentials.values())[7]\n",
    "rds_charset = list(credentials.values())[8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "careful-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Intrinio libraries and set API key\n",
    "\n",
    "from __future__ import print_function\n",
    "import intrinio_sdk as intrinio\n",
    "from intrinio_sdk.rest import ApiException\n",
    "\n",
    "intrinio.ApiClient().configuration.api_key['api_key'] = intrinio_key\n",
    "\n",
    "# Import needed Python libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm, trange  # to be used in loop iterations\n",
    "\n",
    "# Import FinViz library for getting sector, industry and other metadata\n",
    "\n",
    "from finvizfinance.quote import finvizfinance\n",
    "\n",
    "# Import the AWS libraries\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Import Zip file libraries\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import urllib.request as urllib2\n",
    "\n",
    "# Import SQL connection libraries\n",
    "\n",
    "import mysql.connector \n",
    "from mysql.connector import errorcode\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the low level functional AWS client\n",
    "\n",
    "client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id = aws_key,\n",
    "    aws_secret_access_key = aws_secret_key,\n",
    "    region_name = 'us-east-1'\n",
    ")\n",
    "\n",
    "# Set local path.\n",
    "\n",
    "global my_path\n",
    "my_path = file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "early-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the active stock (EQS & DR) tickers and figis. This will be needed to retrieve market cap and shares outstanding\n",
    "# data for active stocks and ADRs.\n",
    "\n",
    "def get_active_tickers():\n",
    "    \n",
    "    global df_active_tickers\n",
    "    global df_active_EQSDR_tickers\n",
    "    \n",
    "    active = True\n",
    "    delisted = False\n",
    "    currency = ''\n",
    "    ticker = ''\n",
    "    name = ''\n",
    "    composite_mic = 'USCOMP'\n",
    "    exchange_mic = ''\n",
    "    stock_prices_after = ''\n",
    "    stock_prices_before = ''\n",
    "    cik = ''\n",
    "    figi = ''\n",
    "    composite_figi = ''\n",
    "    share_class_figi = ''\n",
    "    figi_unique_id = ''\n",
    "    include_non_figi = False\n",
    "    page_size = 10000\n",
    "    primary_listing = True\n",
    "    next_page = ''\n",
    "\n",
    "    # Get active EQS ticker/figi list and convert to dataframe\n",
    "    code = 'EQS'\n",
    "    response = intrinio.SecurityApi().get_all_securities(active=active, delisted=delisted, code=code, composite_mic=composite_mic, page_size=page_size, primary_listing=primary_listing, next_page=next_page)\n",
    "    df_active_EQS = pd.DataFrame([x.to_dict() for x in response\n",
    "                                  .securities])[['ticker', 'figi', 'name', 'company_id', 'id', 'composite_ticker', 'share_class_figi', \n",
    "                                                'composite_figi', 'currency']].sort_values('ticker')\n",
    "    df_active_EQS['instrument_type_name'] = \"Equity Shares\"\n",
    "    df_active_EQS['instrument_type_code'] = \"EQS\"\n",
    "    df_active_EQS['active_status'] = 1\n",
    "\n",
    "    # Get the active ADR tickers and figis\n",
    "    code = 'DR'\n",
    "    response = intrinio.SecurityApi().get_all_securities(active=active, delisted=delisted, code=code, composite_mic=composite_mic, page_size=page_size, primary_listing=primary_listing, next_page=next_page)\n",
    "    df_active_DR = pd.DataFrame([x.to_dict() for x in response\n",
    "                                 .securities])[['ticker', 'figi', 'name', 'company_id', 'id', 'composite_ticker', 'share_class_figi', \n",
    "                                                'composite_figi', 'currency']].sort_values('ticker')\n",
    "    df_active_DR['instrument_type_name'] = \"Depository Receipts\"\n",
    "    df_active_DR['instrument_type_code'] = \"DR\"\n",
    "    df_active_DR['active_status'] = 1\n",
    "    \n",
    "    # Get the active ADR tickers and figis\n",
    "    code = 'ETF'\n",
    "    response = intrinio.SecurityApi().get_all_securities(active=active, delisted=delisted, code=code, composite_mic=composite_mic, page_size=page_size, primary_listing=primary_listing, next_page=next_page)\n",
    "    df_active_ETF = pd.DataFrame([x.to_dict() for x in response\n",
    "                                  .securities])[['ticker', 'figi', 'name', 'company_id', 'id', 'composite_ticker', 'share_class_figi', \n",
    "                                                'composite_figi', 'currency']].sort_values('ticker')\n",
    "    df_active_ETF['instrument_type_name'] = \"Exchange Traded Fund\"\n",
    "    df_active_ETF['instrument_type_code'] = \"ETF\"\n",
    "    df_active_ETF['active_status'] = 1\n",
    "    \n",
    "    # Assemble the EQS, DR and ETF ticker lists to one dataframe\n",
    "    df_active_tickers = pd.concat([df_active_EQS, df_active_DR, df_active_ETF], ignore_index = True)\n",
    "    \n",
    "    # Assemble just the EQS and DR ticker lists to one dataframe\n",
    "    df_active_EQSDR_tickers = pd.concat([df_active_EQS, df_active_DR], ignore_index = True)\n",
    "    \n",
    "    print(\"There are \", df_active_tickers.shape[0], \" currently active tickers in total, including ETFs.\")\n",
    "    print(\"There are \", df_active_EQSDR_tickers.shape[0], \" equity and ADR tickers currently active.\")\n",
    "    \n",
    "    return df_active_tickers, df_active_EQSDR_tickers\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "developing-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get company metadata from Intrinio API.\n",
    "\n",
    "def get_company_metadata():\n",
    "    \n",
    "    global df_company_metadata\n",
    "\n",
    "    response = intrinio.BulkDownloadsApi().get_bulk_download_links()\n",
    "\n",
    "    url = response.bulk_downloads[0].links[0].url\n",
    "\n",
    "    r = urllib2.urlopen(url).read()\n",
    "    file = ZipFile(BytesIO(r))\n",
    "    companies_csv = file.open(\"companies.csv\")\n",
    "    df_company_metadata = pd.read_csv(companies_csv)\n",
    "    \n",
    "    df_company_metadata.columns = map(str.lower, df_company_metadata.columns)\n",
    "    df_company_metadata = df_company_metadata[['ticker', 'id','lei', 'country', 'sector_name', 'industry_category_name', 'industry_group_name', 'sic', 'stock_exchange', 'short_description', 'long_description', 'legal_name', 'ceo', 'company_url', 'business_address', 'employees', 'cik', 'first_stock_price_date', 'last_stock_price_date', 'standardized_active', 'first_fundamental_date', 'last_fundamental_date', 'latest_filing_date', 'statement_template']]\n",
    "    df_company_metadata = df_company_metadata.rename(columns={'id': 'company_id'})\n",
    "    \n",
    "    print(\"The shape of the metadata DF is \", df_company_metadata.shape)\n",
    "    \n",
    "    return df_company_metadata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "declared-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Finviz data downloader for sector and industry mapping.\n",
    "\n",
    "def finviz_sectors(myTicker):\n",
    "    \n",
    "    global df_sector_info\n",
    "    global bad_tickers_finviz\n",
    "    global stock_sector_info\n",
    "    \n",
    "    bad_tickers_finviz = []\n",
    "\n",
    "    try:\n",
    "        ticker = myTicker\n",
    "\n",
    "        result = finvizfinance(ticker).TickerFundament()\n",
    "\n",
    "        sector = [result[key] for key in ['Sector']][0]\n",
    "        industry = [result[key] for key in ['Industry']][0]\n",
    "\n",
    "        data = [ticker, sector, industry]\n",
    "\n",
    "    except:\n",
    "        bad_tickers_finviz.append(ticker)\n",
    "        pass\n",
    "\n",
    "    stock_sector_info.append(data)\n",
    "    \n",
    "    return stock_sector_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "round-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FinViz to get sector and industry for each ticker\n",
    "\n",
    "def get_finviz_sector_info(df_active_EQSDR_tickers):\n",
    "    \n",
    "    import concurrent.futures\n",
    "    \n",
    "    global arg_list\n",
    "    global df_sector_info\n",
    "    global stock_sector_info\n",
    "\n",
    "    arg_list = list(df_active_EQSDR_tickers['ticker'])\n",
    "    stock_sector_info = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        executor.map(finviz_sectors, arg_list)\n",
    "\n",
    "    df_sector_info = pd.DataFrame(data = stock_sector_info, columns = ['ticker', 'fv_sector', 'fv_industry'])\n",
    "    df_sector_info = df_sector_info.drop_duplicates(subset=['ticker'], keep = 'first')    \n",
    "\n",
    "    #Push dataframe to CSV\n",
    "    df_sector_info.to_csv(path_or_buf = my_path + \"/df_company_info.csv\", index=False)\n",
    "\n",
    "    print(\"The sector info dataframe shape is \", df_sector_info.shape)\n",
    "    \n",
    "    return df_sector_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "awful-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the metadata with the original active ticker list\n",
    "\n",
    "def merge_data(df_active_tickers, df_company_metadata, df_sector_info):\n",
    "    \n",
    "    global df_active_tickers_info\n",
    "\n",
    "    df_active_tickers_info = df_active_tickers.merge(df_company_metadata, how = 'left', on = ['ticker', 'company_id'])\n",
    "    df_active_tickers_info = df_active_tickers_info.merge(df_sector_info, how = 'left', on = 'ticker')\n",
    "\n",
    "    df_active_tickers_info = df_active_tickers_info.drop_duplicates(subset=['ticker'], keep = 'first')\n",
    "    df_active_tickers_info = df_active_tickers_info.sort_values(by = ['ticker'], ascending = True)\n",
    "\n",
    "    # Add last update column and change boolean columns to integers (1/0)\n",
    "\n",
    "    df_active_tickers_info['key_id'] = df_active_tickers_info['ticker'] + df_active_tickers_info['figi']\n",
    "    df_active_tickers_info['last_updated_date'] = pd.to_datetime('today').normalize()\n",
    "\n",
    "    df_active_tickers_info = df_active_tickers_info[['key_id', 'ticker', 'figi', 'name', 'company_id', 'id', 'composite_ticker',\n",
    "           'share_class_figi', 'composite_figi', 'currency',\n",
    "           'instrument_type_name', 'instrument_type_code', 'active_status', 'lei',\n",
    "           'country', 'sector_name', 'industry_category_name',\n",
    "           'industry_group_name', 'sic', 'stock_exchange', 'short_description',\n",
    "           'long_description', 'legal_name', 'ceo', 'company_url',\n",
    "           'business_address', 'employees', 'cik', 'first_stock_price_date',\n",
    "           'last_stock_price_date', 'standardized_active',\n",
    "           'first_fundamental_date', 'last_fundamental_date', 'latest_filing_date',\n",
    "           'statement_template', 'fv_sector', 'fv_industry', 'last_updated_date']]\n",
    "\n",
    "    print(\"The shape of the new active ticker info DF is \", df_active_tickers_info.shape)\n",
    "    \n",
    "    return df_active_tickers_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "central-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataframe to CSV on S3\n",
    "\n",
    "def push_new_data_to_S3(df_active_tickers_info):\n",
    "\n",
    "    import io\n",
    "    \n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    myBucket = 'bns-intrinio-data'\n",
    "    myKey = \"security_info/df_active_tickers_info_\" + today + \".csv\"\n",
    "\n",
    "    with io.StringIO() as csv_buffer:\n",
    "        df_active_tickers_info.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        response = client.put_object(\n",
    "            Bucket = myBucket, Key = myKey, Body=csv_buffer.getvalue()\n",
    "        )\n",
    "\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "        if status == 200:\n",
    "            print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-greece",
   "metadata": {},
   "source": [
    "### Copy Old Table and Update Existing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "herbal-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MySQL Connector to get all records from the existing Security_Info table\n",
    "\n",
    "def get_old_info_records():\n",
    "    \n",
    "    import mysql.connector\n",
    "    \n",
    "    global df_previous_active_tickers_info\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT * FROM security_info\")\n",
    "\n",
    "    myresult = mycursor.fetchall()\n",
    "\n",
    "    df_previous_active_tickers_info = pd.DataFrame(myresult, columns = df_active_tickers_info.columns)\n",
    "\n",
    "    print(\"There are\", df_previous_active_tickers_info.shape[0], \"records in the current securities_info table.\")\n",
    "    \n",
    "    return df_previous_active_tickers_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specified-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL Alchemy to push existing data from Security_Info table to Security_Info_old (backup) table in \n",
    "# SQL DB on AWS RDS:\n",
    "\n",
    "def push_old_data_to_backup_table(df_previous_active_tickers_info):\n",
    "    \n",
    "    global old_data_table_update_status\n",
    "\n",
    "    # Set database credentials.\n",
    "    creds = {'usr': rds_user,\n",
    "             'pwd': rds_password,\n",
    "             'hst': rds_host,\n",
    "             'prt': 3306,\n",
    "             'dbn': rds_database}\n",
    "\n",
    "    # Generate MySQL conection string.\n",
    "    connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "    # Create sqlalchemy engine for MySQL connection.\n",
    "    engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "    # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "\n",
    "    df_previous_active_tickers_info.to_sql(name='security_info_old', \n",
    "                                          con=engine, \n",
    "                                          if_exists='replace', \n",
    "                                          index=False)\n",
    "\n",
    "    print(\"The current securities list has been copied to the security_info_old table.\")\n",
    "    \n",
    "    old_data_table_update_status = \"Done\"\n",
    "    \n",
    "    return old_data_table_update_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stunning-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare new Active Tickers dataframe to current Security_Info table to isolate records that are no longer in\n",
    "# the new data set and therefore no longer actively traded.\n",
    "\n",
    "def transform_new_data_records(df_active_tickers_info, df_previous_active_tickers_info):\n",
    "    \n",
    "    global df_ticker_info\n",
    "\n",
    "    df_ticker_diff = df_previous_active_tickers_info.merge(df_active_tickers_info.drop_duplicates(), on=['ticker'], \n",
    "                       how='left', indicator=True)\n",
    "    df_dead_tickers = df_ticker_diff[df_ticker_diff['_merge'] == 'left_only']\n",
    "    df_dead_tickers = df_dead_tickers.iloc[: , :38]\n",
    "    df_dead_tickers.columns = df_previous_active_tickers_info.columns\n",
    "\n",
    "    # df_dead_tickers.to_csv(my_path + \"/dead_tickers.csv\")\n",
    "\n",
    "    print(\"There are\", df_dead_tickers.shape[0], \"records in the old securities list \" \\\n",
    "          \"where the ticker is no longer trading.\")\n",
    "\n",
    "    # With old records, set 'Active_Status' to 0 and then join to new data set and configure date columns to be\n",
    "    # MySQL friendly. Also check for dupe tickers.\n",
    "\n",
    "    df_dead_tickers['active_status'] = 0\n",
    "\n",
    "    df_ticker_info = pd.concat([df_active_tickers_info, df_dead_tickers])\n",
    "    dupes = df_ticker_info[df_ticker_info.duplicated(subset = ['ticker'], keep=False)]\n",
    "    df_ticker_info['first_stock_price_date'] = pd.to_datetime(df_ticker_info['first_stock_price_date'])\n",
    "    df_ticker_info['last_stock_price_date'] = pd.to_datetime(df_ticker_info['last_stock_price_date'])\n",
    "    df_ticker_info['first_fundamental_date'] = pd.to_datetime(df_ticker_info['first_fundamental_date'])\n",
    "    df_ticker_info['last_fundamental_date'] = pd.to_datetime(df_ticker_info['last_fundamental_date'])\n",
    "    df_ticker_info['latest_filing_date'] = pd.to_datetime(df_ticker_info['latest_filing_date'])\n",
    "    df_ticker_info['last_updated_date'] = pd.to_datetime('today').normalize()\n",
    "    df_ticker_info.convert_dtypes().dtypes\n",
    "\n",
    "    print(\"The shape of the new ticker info DF is\", df_ticker_info.shape)\n",
    "    print(\"There are\", dupes.shape[0], \"duplicated tickers in this set.\" )\n",
    "    \n",
    "    return df_ticker_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "african-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the final dataframe into SQL DB on AWS RDS:\n",
    "\n",
    "def push_new_data_to_info_table(df_ticker_info):\n",
    "    \n",
    "    # Set database credentials.\n",
    "    creds = {'usr': rds_user,\n",
    "             'pwd': rds_password,\n",
    "             'hst': rds_host,\n",
    "             'prt': 3306,\n",
    "             'dbn': rds_database}\n",
    "\n",
    "    \n",
    "    # MySQL conection string.\n",
    "    connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "    # Create sqlalchemy engine for MySQL connection.\n",
    "    engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "    # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "    chunk = int(len(df_ticker_info) / 1000)\n",
    "\n",
    "    df_ticker_info.to_sql(\n",
    "        name='security_info', \n",
    "        con=engine, \n",
    "        if_exists='replace', \n",
    "        chunksize=chunk, \n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    print(\"The security_info table has been updated.\")\n",
    "    \n",
    "    new_data_update_status = \"Update done.\"\n",
    "    \n",
    "    return new_data_update_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vanilla-falls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  8414  currently active tickers in total, including ETFs.\n",
      "There are  5763  equity and ADR tickers currently active.\n",
      "The shape of the metadata DF is  (18467, 24)\n",
      "The sector info dataframe shape is  (5291, 3)\n",
      "The shape of the new active ticker info DF is  (8412, 38)\n",
      "Successful S3 put_object response. Status - 200\n"
     ]
    }
   ],
   "source": [
    "# Run ETL process.\n",
    "\n",
    "get_active_tickers()\n",
    "get_company_metadata()\n",
    "get_finviz_sector_info(df_active_EQSDR_tickers)\n",
    "merge_data(df_active_tickers, df_company_metadata, df_sector_info)\n",
    "push_new_data_to_S3(df_active_tickers_info)\n",
    "get_old_info_records()\n",
    "push_old_data_to_backup_table(df_previous_active_tickers_info)\n",
    "transform_new_data_records(df_active_tickers_info, df_previous_active_tickers_info)\n",
    "push_new_data_to_info_table(df_ticker_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce591c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "functional-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push all four info dataframes to CSV files.\n",
    "\n",
    "df_active_tickers.to_csv(path_or_buf = my_path + \"/df_active_tickers.csv\", index=False)\n",
    "df_active_EQSDR_tickers.to_csv(path_or_buf = my_path + \"/df_active_EQSDR_tickers.csv\", index=False)\n",
    "df_company_metadata.to_csv(path_or_buf = my_path + \"/df_company_metadata.csv\", index=False)\n",
    "df_sector_info.to_csv(path_or_buf = my_path + \"/df_sector_info.csv\", index=False)\n",
    "\n",
    "# Push bad figis/tickers to CSV for further analysis\n",
    "\n",
    "df_bad_tickers_finviz = pd.DataFrame(bad_tickers_finviz)\n",
    "df_bad_tickers_finviz.to_csv(path_or_buf = my_path + \"/df_bad_tickers_finviz.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push final dataframe to CSV with today's date added to the file name.\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "df_active_tickers_info.to_csv(path_or_buf = my_path + \"/df_active_tickers_info_\" + today + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
