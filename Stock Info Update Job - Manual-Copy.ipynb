{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "concrete-thailand",
   "metadata": {},
   "source": [
    "### This module downloads stock and company metadata from Intrinio to fill out and maintain a Stock Info table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials\n",
    "\n",
    "import json\n",
    "f = open(\"/. .<your file path here> . . /credentials.json\")\n",
    "credentials = json.load(f)\n",
    "\n",
    "file_path = credentials['file_path']\n",
    "intrinio_key = credentials['intrinio_key']\n",
    "aws_key = credentials['aws_access_key']\n",
    "aws_secret_key = credentials['aws_secret_key']\n",
    "rds_host = credentials['rds_host']\n",
    "rds_user = credentials['rds_user']\n",
    "rds_password = credentials['rds_password']\n",
    "rds_database = credentials['rds_database']\n",
    "rds_charset = credentials['rds_charset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Intrinio libraries and set API key\n",
    "\n",
    "from __future__ import print_function\n",
    "import intrinio_sdk as intrinio\n",
    "from intrinio_sdk.rest import ApiException\n",
    "\n",
    "intrinio.ApiClient().configuration.api_key['api_key'] = intrinio_key\n",
    "\n",
    "# Import needed Python libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm, trange  # to be used in loop iterations\n",
    "\n",
    "# Import Prefect library\n",
    "\n",
    "from prefect import task, Flow\n",
    "import pendulum\n",
    "from prefect.schedules import IntervalSchedule\n",
    "from prefect.schedules.clocks import IntervalClock\n",
    "\n",
    "# Import FinViz library for getting sector, industry and other metadata\n",
    "\n",
    "import finviz\n",
    "from finvizfinance.quote import finvizfinance\n",
    "\n",
    "# Import the AWS libraries\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Import Zip file libraries\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import urllib.request as urllib2\n",
    "\n",
    "# Import SQL connection libraries\n",
    "\n",
    "import mysql.connector \n",
    "from mysql.connector import errorcode\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the low level functional AWS client\n",
    "\n",
    "client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id = aws_key,\n",
    "    aws_secret_access_key = aws_secret_key,\n",
    "    region_name = 'us-east-1'\n",
    ")\n",
    "\n",
    "# Set local path.\n",
    "\n",
    "global my_path\n",
    "my_path = file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the active stock (EQS & DR) tickers and figis. This will be needed to retrieve market cap and shares outstanding\n",
    "# data for active stocks and ADRs.\n",
    "\n",
    "def get_active_tickers():\n",
    "    \n",
    "    global df_active_tickers\n",
    "    global df_active_EQSDR_tickers\n",
    "    \n",
    "    active = True\n",
    "    delisted = False\n",
    "    currency = ''\n",
    "    ticker = ''\n",
    "    name = ''\n",
    "    composite_mic = 'USCOMP'\n",
    "    exchange_mic = ''\n",
    "    stock_prices_after = ''\n",
    "    stock_prices_before = ''\n",
    "    cik = ''\n",
    "    figi = ''\n",
    "    composite_figi = ''\n",
    "    share_class_figi = ''\n",
    "    figi_unique_id = ''\n",
    "    include_non_figi = False\n",
    "    page_size = 10000\n",
    "    primary_listing = True\n",
    "    next_page = ''\n",
    "\n",
    "    # Get active EQS ticker/figi list and convert to dataframe\n",
    "    code = 'EQS'\n",
    "    response = intrinio.SecurityApi().get_all_securities(active=active, delisted=delisted, code=code, composite_mic=composite_mic, page_size=page_size, primary_listing=primary_listing, next_page=next_page)\n",
    "    df_active_EQS = pd.DataFrame([x.to_dict() for x in response\n",
    "                                  .securities])[['ticker', 'figi', 'name', 'company_id', 'id', 'composite_ticker', 'share_class_figi', \n",
    "                                                'composite_figi', 'currency']].sort_values('ticker')\n",
    "    df_active_EQS['instrument_type_name'] = \"Equity Shares\"\n",
    "    df_active_EQS['instrument_type_code'] = \"EQS\"\n",
    "    df_active_EQS['active_status'] = 1\n",
    "\n",
    "    # Get the active ADR tickers and figis\n",
    "    code = 'DR'\n",
    "    response = intrinio.SecurityApi().get_all_securities(active=active, delisted=delisted, code=code, composite_mic=composite_mic, page_size=page_size, primary_listing=primary_listing, next_page=next_page)\n",
    "    df_active_DR = pd.DataFrame([x.to_dict() for x in response\n",
    "                                 .securities])[['ticker', 'figi', 'name', 'company_id', 'id', 'composite_ticker', 'share_class_figi', \n",
    "                                                'composite_figi', 'currency']].sort_values('ticker')\n",
    "    df_active_DR['instrument_type_name'] = \"Depository Receipts\"\n",
    "    df_active_DR['instrument_type_code'] = \"DR\"\n",
    "    df_active_DR['active_status'] = 1\n",
    "    \n",
    "    # Get the active ADR tickers and figis\n",
    "    code = 'ETF'\n",
    "    response = intrinio.SecurityApi().get_all_securities(active=active, delisted=delisted, code=code, composite_mic=composite_mic, page_size=page_size, primary_listing=primary_listing, next_page=next_page)\n",
    "    df_active_ETF = pd.DataFrame([x.to_dict() for x in response\n",
    "                                  .securities])[['ticker', 'figi', 'name', 'company_id', 'id', 'composite_ticker', 'share_class_figi', \n",
    "                                                'composite_figi', 'currency']].sort_values('ticker')\n",
    "    df_active_ETF['instrument_type_name'] = \"Exchange Traded Fund\"\n",
    "    df_active_ETF['instrument_type_code'] = \"ETF\"\n",
    "    df_active_ETF['active_status'] = 1\n",
    "    \n",
    "    # Assemble the EQS, DR and ETF ticker lists to one dataframe\n",
    "    df_active_tickers = pd.concat([df_active_EQS, df_active_DR, df_active_ETF], ignore_index = True)\n",
    "    \n",
    "    # Assemble just the EQS and DR ticker lists to one dataframe\n",
    "    df_active_EQSDR_tickers = pd.concat([df_active_EQS, df_active_DR], ignore_index = True)\n",
    "    \n",
    "    print(\"There are \", df_active_tickers.shape[0], \" currently active tickers in total, including ETFs.\")\n",
    "    print(\"There are \", df_active_EQSDR_tickers.shape[0], \" equity and ADR tickers currently active.\")\n",
    "    \n",
    "    return df_active_tickers, df_active_EQSDR_tickers\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get company metadata from Intrinio API.\n",
    "\n",
    "def get_company_metadata():\n",
    "    \n",
    "    global df_company_metadata\n",
    "\n",
    "    response = intrinio.BulkDownloadsApi().get_bulk_download_links()\n",
    "\n",
    "    url = response.bulk_downloads[0].links[0].url\n",
    "\n",
    "    r = urllib2.urlopen(url).read()\n",
    "    file = ZipFile(BytesIO(r))\n",
    "    companies_csv = file.open(\"companies.csv\")\n",
    "    df_company_metadata = pd.read_csv(companies_csv)\n",
    "    \n",
    "    df_company_metadata.columns = map(str.lower, df_company_metadata.columns)\n",
    "    df_company_metadata = df_company_metadata[['ticker', 'id','lei', 'country', 'sector_name', 'industry_category_name', 'industry_group_name', 'sic', 'stock_exchange', 'short_description', 'long_description', 'legal_name', 'ceo', 'company_url', 'business_address', 'employees', 'cik', 'first_stock_price_date', 'last_stock_price_date', 'standardized_active', 'first_fundamental_date', 'last_fundamental_date', 'latest_filing_date', 'statement_template']]\n",
    "    df_company_metadata = df_company_metadata.rename(columns={'id': 'company_id'})\n",
    "    \n",
    "    print(\"The shape of the metadata DF is \", df_company_metadata.shape)\n",
    "    \n",
    "    return df_company_metadata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Finviz data downloader for sector and industry mapping.\n",
    "\n",
    "def finviz_sectors(myTicker):\n",
    "    \n",
    "    global df_sector_info\n",
    "    global bad_tickers_finviz\n",
    "    global stock_sector_info\n",
    "    \n",
    "    bad_tickers_finviz = []\n",
    "\n",
    "    try:\n",
    "        ticker = myTicker\n",
    "\n",
    "        result = finviz.get_stock(ticker)\n",
    "\n",
    "        sector = result['Sector']\n",
    "        industry = result['Industry']\n",
    "\n",
    "        data = [ticker, sector, industry]\n",
    "\n",
    "    except:\n",
    "        bad_tickers_finviz.append(ticker)\n",
    "        pass\n",
    "\n",
    "    stock_sector_info.append(data)\n",
    "    \n",
    "    return stock_sector_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FinViz to get sector and industry for each ticker\n",
    "\n",
    "def get_finviz_sector_info(df_active_EQSDR_tickers):\n",
    "    \n",
    "    import concurrent.futures\n",
    "    \n",
    "    global arg_list\n",
    "    global df_sector_info\n",
    "    global stock_sector_info\n",
    "\n",
    "    arg_list = list(df_active_EQSDR_tickers['ticker'])\n",
    "    stock_sector_info = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        executor.map(finviz_sectors, arg_list)\n",
    "\n",
    "    df_sector_info = pd.DataFrame(data = stock_sector_info, columns = ['ticker', 'fv_sector', 'fv_industry'])\n",
    "    df_sector_info = df_sector_info.drop_duplicates(subset=['ticker'], keep = 'first')    \n",
    "\n",
    "    #Push dataframe to CSV\n",
    "    df_sector_info.to_csv(path_or_buf = my_path + \"/df_company_info.csv\", index=False)\n",
    "\n",
    "    print(\"The sector info dataframe shape is \", df_sector_info.shape)\n",
    "    \n",
    "    return df_sector_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the metadata with the original active ticker list\n",
    "\n",
    "def merge_data(df_active_tickers, df_company_metadata, df_sector_info):\n",
    "    \n",
    "    global df_active_tickers_info\n",
    "\n",
    "    df_active_tickers_info = df_active_tickers.merge(df_company_metadata, how = 'left', on = ['ticker', 'company_id'])\n",
    "    df_active_tickers_info = df_active_tickers_info.merge(df_sector_info, how = 'left', on = 'ticker')\n",
    "\n",
    "    df_active_tickers_info = df_active_tickers_info.drop_duplicates(subset=['ticker'], keep = 'first')\n",
    "    df_active_tickers_info = df_active_tickers_info.sort_values(by = ['ticker'], ascending = True)\n",
    "\n",
    "    # Add last update column and change boolean columns to integers (1/0)\n",
    "\n",
    "    df_active_tickers_info['key_id'] = df_active_tickers_info['ticker'] + df_active_tickers_info['figi']\n",
    "    df_active_tickers_info['last_updated_date'] = pd.to_datetime('today').normalize()\n",
    "\n",
    "    df_active_tickers_info = df_active_tickers_info[['key_id', 'ticker', 'figi', 'name', 'company_id', 'id', 'composite_ticker',\n",
    "           'share_class_figi', 'composite_figi', 'currency',\n",
    "           'instrument_type_name', 'instrument_type_code', 'active_status', 'lei',\n",
    "           'country', 'sector_name', 'industry_category_name',\n",
    "           'industry_group_name', 'sic', 'stock_exchange', 'short_description',\n",
    "           'long_description', 'legal_name', 'ceo', 'company_url',\n",
    "           'business_address', 'employees', 'cik', 'first_stock_price_date',\n",
    "           'last_stock_price_date', 'standardized_active',\n",
    "           'first_fundamental_date', 'last_fundamental_date', 'latest_filing_date',\n",
    "           'statement_template', 'fv_sector', 'fv_industry', 'last_updated_date']]\n",
    "\n",
    "    print(\"The shape of the new active ticker info DF is \", df_active_tickers_info.shape)\n",
    "    \n",
    "    return df_active_tickers_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataframe to CSV on S3\n",
    "\n",
    "def push_new_data_to_S3(df_active_tickers_info):\n",
    "\n",
    "    import io\n",
    "    \n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    myBucket = 'bns-intrinio-data'\n",
    "    myKey = \"security_info/df_active_tickers_info_\" + today + \".csv\"\n",
    "\n",
    "    with io.StringIO() as csv_buffer:\n",
    "        df_active_tickers_info.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        response = client.put_object(\n",
    "            Bucket = myBucket, Key = myKey, Body=csv_buffer.getvalue()\n",
    "        )\n",
    "\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "        if status == 200:\n",
    "            print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-greece",
   "metadata": {},
   "source": [
    "### Copy Old Table and Update Existing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MySQL Connector to get all records from the existing Security_Info table\n",
    "\n",
    "def get_old_info_records():\n",
    "    \n",
    "    import mysql.connector\n",
    "    \n",
    "    global df_previous_active_tickers_info\n",
    "\n",
    "    mydb = mysql.connector.connect(\n",
    "      host = rds_host,\n",
    "      user = rds_user,\n",
    "      password = rds_password,\n",
    "      database = rds_database\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    mycursor.execute(\"SELECT * FROM security_info\")\n",
    "\n",
    "    myresult = mycursor.fetchall()\n",
    "\n",
    "    df_previous_active_tickers_info = pd.DataFrame(myresult, columns = df_active_tickers_info.columns)\n",
    "\n",
    "    print(\"There are\", df_previous_active_tickers_info.shape[0], \"records in the current securities_info table.\")\n",
    "    \n",
    "    return df_previous_active_tickers_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL Alchemy to push existing data from Security_Info table to Security_Info_old (backup) table in \n",
    "# SQL DB on AWS RDS:\n",
    "\n",
    "def push_old_data_to_backup_table(df_previous_active_tickers_info):\n",
    "    \n",
    "    global old_data_table_update_status\n",
    "\n",
    "    # Set database credentials.\n",
    "    creds = {'usr': rds_user,\n",
    "             'pwd': rds_password,\n",
    "             'hst': rds_host,\n",
    "             'prt': 3306,\n",
    "             'dbn': rds_database}\n",
    "\n",
    "    # Generate MySQL conection string.\n",
    "    connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "    # Create sqlalchemy engine for MySQL connection.\n",
    "    engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "    # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "\n",
    "    df_previous_active_tickers_info.to_sql(name='security_info_old', \n",
    "                                          con=engine, \n",
    "                                          if_exists='replace', \n",
    "                                          index=False)\n",
    "\n",
    "    print(\"The current securities list has been copied to the security_info_old table.\")\n",
    "    \n",
    "    old_data_table_update_status = \"Done\"\n",
    "    \n",
    "    return old_data_table_update_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare new Active Tickers dataframe to current Security_Info table to isolate records that are no longer in\n",
    "# the new data set and therefore no longer actively traded.\n",
    "\n",
    "def transform_new_data_records(df_active_tickers_info, df_previous_active_tickers_info):\n",
    "    \n",
    "    global df_ticker_info\n",
    "\n",
    "    df_ticker_diff = df_previous_active_tickers_info.merge(df_active_tickers_info.drop_duplicates(), on=['ticker'], \n",
    "                       how='left', indicator=True)\n",
    "    df_dead_tickers = df_ticker_diff[df_ticker_diff['_merge'] == 'left_only']\n",
    "    df_dead_tickers = df_dead_tickers.iloc[: , :38]\n",
    "    df_dead_tickers.columns = df_previous_active_tickers_info.columns\n",
    "\n",
    "    # df_dead_tickers.to_csv(my_path + \"/dead_tickers.csv\")\n",
    "\n",
    "    print(\"There are\", df_dead_tickers.shape[0], \"records in the old securities list \" \\\n",
    "          \"where the ticker is no longer trading.\")\n",
    "\n",
    "    # With old records, set 'Active_Status' to 0 and then join to new data set and configure date columns to be\n",
    "    # MySQL friendly. Also check for dupe tickers.\n",
    "\n",
    "    df_dead_tickers['active_status'] = 0\n",
    "\n",
    "    df_ticker_info = pd.concat([df_active_tickers_info, df_dead_tickers])\n",
    "    dupes = df_ticker_info[df_ticker_info.duplicated(subset = ['ticker'], keep=False)]\n",
    "    df_ticker_info['first_stock_price_date'] = pd.to_datetime(df_ticker_info['first_stock_price_date'], errors = 'coerce')\n",
    "    df_ticker_info['last_stock_price_date'] = pd.to_datetime(df_ticker_info['last_stock_price_date'], errors = 'coerce')\n",
    "    df_ticker_info['first_fundamental_date'] = pd.to_datetime(df_ticker_info['first_fundamental_date'], errors = 'coerce')\n",
    "    df_ticker_info['last_fundamental_date'] = pd.to_datetime(df_ticker_info['last_fundamental_date'], errors = 'coerce')\n",
    "    df_ticker_info['latest_filing_date'] = pd.to_datetime(df_ticker_info['latest_filing_date'], errors = 'coerce')\n",
    "    df_ticker_info['last_updated_date'] = pd.to_datetime('today').normalize()\n",
    "    \n",
    "    df_ticker_info['standardized_active'].replace({0: 'FALSE', 1: 'TRUE'})\n",
    "    df_ticker_info.fillna('', inplace=True)\n",
    "    df_ticker_info = df_ticker_info.replace('NaT', '')\n",
    "#     df_ticker_info.convert_dtypes().dtypes\n",
    "\n",
    "    print(\"The shape of the new ticker info DF is\", df_ticker_info.shape)\n",
    "    print(\"There are\", dupes.shape[0], \"duplicated tickers in this set.\" )\n",
    "    \n",
    "    return df_ticker_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the final dataframe into SQL DB on AWS RDS:\n",
    "\n",
    "def push_new_data_to_info_table(df_ticker_info):\n",
    "    \n",
    "    df = df_ticker_info.copy()\n",
    "\n",
    "    # Set SQLAlchemy database credentials.\n",
    "    creds = {'usr': rds_user,\n",
    "             'pwd': rds_password,\n",
    "             'hst': rds_host,\n",
    "             'prt': 3306,\n",
    "             'dbn': rds_database}\n",
    "\n",
    "    # MySQL conection string.\n",
    "    connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "    # Create sqlalchemy engine for MySQL connection.\n",
    "    engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "    # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "    chunk = int(len(df) / 10)\n",
    "    df.to_sql(\n",
    "        name='security_info', \n",
    "        con=engine, \n",
    "        if_exists='replace', \n",
    "        chunksize=chunk, \n",
    "        index=False)\n",
    "\n",
    "\n",
    "    print(\"The security_info table has been updated.\")\n",
    "    \n",
    "    new_data_update_status = \"Update done.\"\n",
    "    \n",
    "    return new_data_update_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_new_data_to_info_table2(df_ticker_info):\n",
    "\n",
    "    import pymysql.cursors\n",
    "    \n",
    "    global insert_records_status\n",
    "\n",
    "    rowCount = 0\n",
    "\n",
    "    connection = pymysql.connect(host = rds_host,\n",
    "                                 user = rds_user, \n",
    "                                 password = rds_password, \n",
    "                                 database = rds_database,\n",
    "                                 charset = rds_charset,\n",
    "                                 cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "    mycursor = connection.cursor()\n",
    "\n",
    "    sql_insert_query = \"\"\"\n",
    "    INSERT INTO security_info \n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, \\\n",
    "    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_ticker_info.copy()\n",
    "\n",
    "    def chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "    for chunk in tqdm(chunker(df, int(len(df)/10))):\n",
    "\n",
    "        data = chunk.values.tolist()\n",
    "        mycursor.executemany(sql_insert_query, data)\n",
    "        connection.commit()\n",
    "        rowCount = rowCount + mycursor.rowcount\n",
    "\n",
    "    print(rowCount, \"data records inserted.\")\n",
    "    \n",
    "    new_data_update_status = \"Update done.\"\n",
    "\n",
    "    \n",
    "    return new_data_update_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ETL process>\n",
    "\n",
    "get_active_tickers()\n",
    "get_company_metadata()\n",
    "get_finviz_sector_info(df_active_EQSDR_tickers)\n",
    "merge_data(df_active_tickers, df_company_metadata, df_sector_info)\n",
    "push_new_data_to_S3(df_active_tickers_info)\n",
    "get_old_info_records()\n",
    "push_old_data_to_backup_table(df_previous_active_tickers_info)\n",
    "transform_new_data_records(df_active_tickers_info, df_previous_active_tickers_info)\n",
    "\n",
    "try: \n",
    "    push_new_data_to_info_table(df_ticker_info)\n",
    "except:\n",
    "    push_new_data_to_info_table2(df_ticker_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push all info dataframes to CSV files.\n",
    "\n",
    "df_active_tickers.to_csv(path_or_buf = my_path + \"/df_active_tickers.csv\", index=False)\n",
    "df_active_EQSDR_tickers.to_csv(path_or_buf = my_path + \"/df_active_EQSDR_tickers.csv\", index=False)\n",
    "df_company_metadata.to_csv(path_or_buf = my_path + \"/df_company_metadata.csv\", index=False)\n",
    "df_sector_info.to_csv(path_or_buf = my_path + \"/df_sector_info.csv\", index=False)\n",
    "df_ticker_info.to_csv(path_or_buf = my_path + \"/df_ticker_info.csv\", index=False)\n",
    "\n",
    "# Push bad figis/tickers to CSV for further analysis\n",
    "\n",
    "df_bad_tickers_finviz = pd.DataFrame(bad_tickers_finviz)\n",
    "df_bad_tickers_finviz.to_csv(path_or_buf = my_path + \"/df_bad_tickers_finviz.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push final dataframe to CSV with today's date added to the file name.\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "df_active_tickers_info.to_csv(path_or_buf = my_path + \"/df_active_tickers_info_\" + today + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-ladder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
