{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a set of modules for downloading historical equity data from Intrinio APIs and pushing it into an AWS RDS database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import a variety of libraries and credentials to access APIs from Intrinio and AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials\n",
    "\n",
    "import json\n",
    "f = open(\"/. .<your file path here> . . /credentials.json\")\n",
    "credentials = json.load(f)\n",
    "\n",
    "file_path = list(credentials.values())[0]\n",
    "intrinio_key = list(credentials.values())[1]\n",
    "aws_key = list(credentials.values())[2]\n",
    "aws_secret_key = list(credentials.values())[3]\n",
    "rds_host = list(credentials.values())[4]\n",
    "rds_user = list(credentials.values())[5]\n",
    "rds_password = list(credentials.values())[6]\n",
    "rds_database = list(credentials.values())[7]\n",
    "rds_charset = list(credentials.values())[8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Intrinio libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import intrinio_sdk as intrinio\n",
    "from intrinio_sdk.rest import ApiException\n",
    "\n",
    "intrinio.ApiClient().configuration.api_key['api_key'] = intrinio_key\n",
    "\n",
    "# Import the usual Python libraries\n",
    "\n",
    "from tqdm.notebook import tqdm, trange  # to be used to track progress in loop iterations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Zip file libraries\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import urllib.request as urllib2\n",
    "\n",
    "# Import the AWS libraries\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Declare the local file path to be used for saving CSV outputs.\n",
    "\n",
    "global my_path\n",
    "my_path = file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the historical data from Intrinio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after manually downloading the historical bulk files from Intrinio, we roll them up into a single dataframe for transformation into the shape we need for the DB table. The source files are in a zip format, which is fine. The pd.read_csv command will unzip them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the five Intrinio price history files into one dataframe.\n",
    "\n",
    "def assemble_bulk_history():\n",
    "    \n",
    "    global df_price_history\n",
    "\n",
    "    price_history = np.empty([0,29])\n",
    "\n",
    "    for X in tqdm(range(1, 6)):\n",
    "\n",
    "        ticker_file_path = my_path + \"/\" + \"stock_prices_uscomp_all_file-\" + str(X) + \".zip\"\n",
    "        data = pd.read_csv (ticker_file_path, low_memory = False)\n",
    "        data = np.array(data.values)\n",
    "        price_history = np.concatenate((price_history, data), axis=0)\n",
    "\n",
    "    # Convert price history array to dataframe and do some cleanup\n",
    "\n",
    "    df_price_history = pd.DataFrame(data = price_history, columns = ['security_id', 'company_id', 'name', 'cik', 'ticker', 'figi', 'composite_figi', 'composite_ticker', 'exchange_ticker', 'date', 'type', 'frequency', 'open', 'high', 'low', 'close', 'volume', 'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume', 'adj_factor', 'ex_dividend',  'split_ratio', 'change', 'percent_change', 'fifty_two_week_high', 'fifty_two_week_low'])\n",
    "\n",
    "    # Make sure Date column is in DateTime format, then sort by ticker and date\n",
    "\n",
    "    df_price_history['date'] = pd.to_datetime(df_price_history['date'])\n",
    "    df_price_history = df_price_history.sort_values(by=['ticker', 'date'])\n",
    "\n",
    "    print(\"Price history files assembled.\")    \n",
    "    print(\"The shape of the price history dataframe is \", df_price_history.shape)\n",
    "    \n",
    "    return df_price_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the five Intrinio price history files and assemble them into one dataframe.\n",
    "\n",
    "def download_bulk_history():\n",
    "    \n",
    "    global df_price_history\n",
    "\n",
    "    price_history = np.empty([0,29])\n",
    "\n",
    "    response = intrinio.BulkDownloadsApi().get_bulk_download_links()\n",
    "\n",
    "    for X in tqdm(range(0, 5)):\n",
    "\n",
    "        url = response.bulk_downloads[1].links[X].url\n",
    "        r = urllib2.urlopen(url).read()\n",
    "        file = ZipFile(BytesIO(r))\n",
    "        data_csv = file.open(\"stock_prices_uscomp_all_file-\" + str(X+1) + \".csv\")\n",
    "        data = pd.read_csv(data_csv, low_memory=False)\n",
    "        data = np.array(data.values)\n",
    "        price_history = np.concatenate((price_history, data), axis=0)\n",
    "\n",
    "    # Convert price history array to dataframe and do some cleanup\n",
    "\n",
    "    df_price_history = pd.DataFrame(data = price_history, columns = ['security_id', 'company_id', 'name', 'cik', 'ticker', 'figi', 'composite_figi', 'composite_ticker', 'exchange_ticker', 'date', 'type', 'frequency', 'open', 'high', 'low', 'close', 'volume', 'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume', 'adj_factor', 'ex_dividend',  'split_ratio', 'change', 'percent_change', 'fifty_two_week_high', 'fifty_two_week_low'])\n",
    "\n",
    "    # Make sure Date column is in DateTime format, then sort by ticker and date\n",
    "\n",
    "    df_price_history['date'] = pd.to_datetime(df_price_history['date'])\n",
    "    df_price_history = df_price_history.sort_values(by=['ticker', 'date'])\n",
    "\n",
    "    print(\"Price history files assembled.\")    \n",
    "    print(\"The shape of the price history dataframe is \", df_price_history.shape)\n",
    "    \n",
    "    return df_price_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical weighted average diluted shares outstanding for each ticker\n",
    "\n",
    "def get_shares_out(myFigi, myTicker):\n",
    "    \n",
    "    global shares_out_list\n",
    "    global shares_out_lists_combined\n",
    "    \n",
    "    identifier = myFigi\n",
    "    tag = 'adjweightedavedilutedsharesos'\n",
    "    frequency = ''\n",
    "    type = ''\n",
    "    start_date = ''\n",
    "    end_date = ''\n",
    "    sort_order = 'desc'\n",
    "    page_size = 10000\n",
    "    next_page = ''\n",
    "\n",
    "    try:\n",
    "        response = intrinio.HistoricalDataApi().get_historical_data(identifier, tag, frequency=frequency, type=type, start_date=start_date, end_date=end_date, sort_order=sort_order, page_size=page_size, next_page=next_page)\n",
    "        shares_out_data = response.historical_data\n",
    "\n",
    "        shares_out_list = []\n",
    "\n",
    "        for item in range(len(shares_out_data)):\n",
    "            \n",
    "            # Append ticker and figi to results\n",
    "            dict_item = shares_out_data[item].to_dict()\n",
    "            dict_item['ticker'] = myTicker\n",
    "            dict_item['figi'] = myFigi\n",
    "            shares_out_list.append(dict_item)\n",
    "\n",
    "    except:\n",
    "        \n",
    "        # Track tickers that do not have a shares out figure available.\n",
    "        bad_tickers.append(myTicker, myFigi)\n",
    "        pass\n",
    "\n",
    "    shares_out_lists_combined.extend(shares_out_list)\n",
    "    \n",
    "    return shares_out_lists_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the shares outstanding data and resample it to daily frequency.\n",
    "\n",
    "def get_shares_out_data(df_price_history):\n",
    "\n",
    "    import concurrent.futures\n",
    "\n",
    "    mkt_cap_list = []\n",
    "    shares_out_list = []\n",
    "    bad_tickers = []\n",
    "    shares_out_lists_combined = []\n",
    "\n",
    "    # Grab tickers and figis from the price history DF and drop any figi duplicates that might show up.\n",
    "    df_price_history_figis = df_price_history[['figi', 'ticker']].drop_duplicates(subset = ['figi'])\n",
    "    arg_list = list(df_price_history_figis.to_records(index = False))\n",
    "\n",
    "    # Use concurrent.futures to use multiple threads to retrieve shares out data.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 25) as executor:\n",
    "        executor.map(lambda f: get_shares_out(*f), arg_list)\n",
    "\n",
    "    # Convert the shares out array to a dataframe, drop any duplicates, rename the values column, replace zeros with\n",
    "    # NaNs, and get rid of any negative shares out numbers.\n",
    "    df_shares_out = pd.DataFrame(shares_out_lists_combined)\n",
    "    df_shares_out = df_shares_out.drop_duplicates(subset=['ticker', 'date'], keep = 'first')\n",
    "    df_shares_out['date']= pd.to_datetime(df_shares_out['date'])\n",
    "    df_shares_out = df_shares_out.rename(columns = {'value':'weighted_avg_shares_out'})\n",
    "    df_shares_out['weighted_avg_shares_out'] = df_shares_out['weighted_avg_shares_out'].replace(0, np.nan)\n",
    "    df_shares_out['weighted_avg_shares_out'] = df_shares_out['weighted_avg_shares_out'].abs()\n",
    "\n",
    "    # set date as index and convert to daily periods. Since shares out are reported quarterly, we need to resample to\n",
    "    # daily records.\n",
    "    df_shares_resample = df_shares_out.copy()\n",
    "    df_shares_resample = df_shares_resample.set_index('date')\n",
    "    df_shares_resample.index = pd.to_datetime(df_shares_resample.index)\n",
    "    df_shares_resample = df_shares_resample.groupby('ticker').resample('D', convention = 'end').ffill()\n",
    "    df_shares_resample = df_shares_resample.droplevel('ticker')\n",
    "    df_shares_resample = df_shares_resample.reset_index()\n",
    "\n",
    "    df_shares_out = df_shares_resample.copy()\n",
    "\n",
    "    # Take a look at the bad tickers that did not pull any results from the market cap query and make sure they are\n",
    "    # not well recognized names/tickers. E.g. none should be MSFT or AAPL. For the most part, they should be ETFs or very\n",
    "    # small cap stocks.\n",
    "\n",
    "    df_bad_tickers = pd.DataFrame(bad_tickers, columns=['ticker'])\n",
    "    df_bad_tickers = df_bad_tickers.drop_duplicates(keep = 'first')\n",
    "    df_bad_tickers.to_csv(path_or_buf = my_path + \"/df_bad_tickers.csv\", index=False)\n",
    "\n",
    "    print(\"The shape of the shares_out DF is \", df_shares_out.shape)\n",
    "    print(\"Bad tickers that did not generate any shares out figures are pushed to CSV.\")\n",
    "    \n",
    "    return df_shares_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the shares out history dataframe to a CSV file if you want to have a backup.\n",
    "# df_shares_out.to_csv(path_or_buf = my_path + \"/df_shares_out.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case the kernel fails later and we have to recreate the market cap history, we can grab the previous CSV file here instead.\n",
    "\n",
    "# ticker_file_path = my_path + \"/\" + \"df_shares_out.csv\"\n",
    "# df_shares_out = pd.read_csv (ticker_file_path, low_memory=False)\n",
    "# df_shares_out.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish transforming the data by adding a Market Cap calculation and several date fields to keep track of updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use left join to add shares out to history dataframe, then calculate market cap for each row.\n",
    "# Make sure the date columns are in datetime format, do the merge, forward fill the shares out figures to from the\n",
    "# last report date to today, then calculate market cap figures.\n",
    "\n",
    "def join_dataframes(df_price_history, df_shares_out):\n",
    "    \n",
    "    global df_price_history_complete_load\n",
    "\n",
    "    df_shares_out['date']= pd.to_datetime(df_shares_out['date'])\n",
    "    df_price_history['date']= pd.to_datetime(df_price_history['date'])\n",
    "    df_price_history_complete = pd.merge(df_price_history, df_shares_out, on=['ticker', 'figi', 'date'], how='left')\n",
    "    df_price_history_complete = df_price_history_complete.sort_values(by = ['ticker', 'date'])\n",
    "    df_price_history_complete['weighted_avg_shares_out'] = df_price_history_complete.groupby('ticker')['weighted_avg_shares_out'].transform(lambda x: x.ffill())\n",
    "    df_price_history_complete['market_cap'] = df_price_history_complete['adj_close'] * df_price_history_complete['weighted_avg_shares_out']\n",
    "\n",
    "    # # Export the complete price history dataframe to a CSV file.\n",
    "    # df_price_history_complete.to_csv(path_or_buf = my_path + \"/df_price_history_complete.csv\", index=False)\n",
    "\n",
    "    # Drop any remaining dupes and configure the final columns for loading into AWS RDS price history table.\n",
    "    df_price_history_complete_load = df_price_history_complete.copy()\n",
    "    df_price_history_complete_load = df_price_history_complete_load.dropna(subset = ['figi'])\n",
    "    df_price_history_complete_load['date'] = pd.to_datetime(df_price_history_complete_load['date'])\n",
    "    df_price_history_complete_load['intraperiod'] = 0\n",
    "    df_price_history_complete_load['last_update_date'] = pd.to_datetime(df_price_history_complete_load['date'].max()).normalize()\n",
    "    df_price_history_complete_load['last_corp_action_date'] = pd.to_datetime(df_price_history_complete_load['date'].max()).normalize()\n",
    "    df_price_history_complete_load['key_id'] = df_price_history_complete_load['ticker'] + df_price_history_complete_load['figi'] + df_price_history_complete_load['date'].dt.strftime('%Y-%m-%d')\n",
    "    df_price_history_complete_load = df_price_history_complete_load[['key_id', 'ticker', 'figi', 'date', 'open', 'high', 'low', 'close', 'volume',\n",
    "           'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume',\n",
    "           'adj_factor', 'split_ratio', 'change', 'percent_change',\n",
    "           'fifty_two_week_high', 'fifty_two_week_low', 'market_cap',\n",
    "           'weighted_avg_shares_out', 'intraperiod', 'last_update_date', 'last_corp_action_date']]\n",
    "\n",
    "    # Check primary key column again for duplicates.\n",
    "\n",
    "    df_duplicate_rows = df_price_history_complete_load[df_price_history_complete_load.duplicated(subset = ['key_id'], keep = False)]\n",
    "    if df_duplicate_rows.shape[0] != 0:\n",
    "        df_duplicate_rows.to_csv(path_or_buf = my_path + \"/df_duplicate_rows.csv\", index=False)\n",
    "        print(\"Primary key duplicates found and pushed to CSV file, df_duplicate_rows.csv.\")\n",
    "\n",
    "    print(\"Final price history dataframe completed, ready to load to DB. The final dataframe shape is \", df_price_history_complete_load.shape)\n",
    "\n",
    "    return df_price_history_complete_load\n",
    "    \n",
    "                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For backup or archive purposes, save the final dataframe to CSV and/or parquet files and push them to AWS S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the low level functional AWS client\n",
    "\n",
    "def push_data_to_S3(df_price_history_complete_load):\n",
    "    \n",
    "    client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id = aws_key,\n",
    "        aws_secret_access_key = aws_secret_key,\n",
    "        region_name = 'us-east-1'\n",
    "    )\n",
    "\n",
    "    # Export the price history dataframe to a zipped CSV file then push to AWS S3.\n",
    "    compression_opts = dict(method='zip', archive_name='df_full_price_history_complete_load.csv') \n",
    "    df_price_history_complete_load.to_csv(path_or_buf = my_path + \"/df_full_price_history_complete_load.zip\", index=False, compression=compression_opts)\n",
    "    client.upload_file(my_path + \"/df_full_price_history_complete_load.zip\", 'bns-intrinio-data', \"price-data-historical/csv_files/df_full_price_history_complete_load.zip\")\n",
    "\n",
    "\n",
    "    # Write parquet file to local drive, then push to AWS S3.\n",
    "    local_file = my_path + \"/df_full_price_history_complete_load.parquet\"\n",
    "    parquet_table = pa.Table.from_pandas(df_price_history)\n",
    "    pq.write_table(parquet_table, local_file)\n",
    "    client.upload_file(local_file, 'bns-intrinio-data', \"price-data-historical/parquet_files/df_full_price_history_complete_load.parquet\")\n",
    "\n",
    "    print(\"Data saved to S3 in zipped CSV and parquet.\")\n",
    "    S3_push_status = \"Done.\"\n",
    "    \n",
    "    return S3_push_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the complete history file if needed\n",
    "\n",
    "# file_path = my_path + \"/\" + \"df_price_history_complete.zip\"\n",
    "# df_price_history_complete_load = pd.read_csv (file_path, low_memory=False)\n",
    "# df_price_history_complete_load.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, create the MySQL price history table in RDS and push the history data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SQL libraries\n",
    "\n",
    "def create_and_fill_RDS_table(df_price_history_complete_load):\n",
    "\n",
    "    import mysql.connector \n",
    "    from mysql.connector import errorcode\n",
    "\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    # Establish the MySQL connection\n",
    "\n",
    "    connection = mysql.connector.connect(host=rds_host,\n",
    "                                 user=rds_user, \n",
    "                                 password=rds_password, \n",
    "                                 database=rds_database,\n",
    "                                 charset=rds_charset)\n",
    "\n",
    "    mycursor = connection.cursor()\n",
    "\n",
    "    # Create the data table in MySQL with MySQL Connector library\n",
    "\n",
    "    create_price_data_history_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS `price_data_historical` (\n",
    "    `key_id` varchar(30) NOT NULL,\n",
    "    `ticker` varchar(8) NOT NULL,\n",
    "    `figi` varchar(14) NOT NULL,\n",
    "    `date` datetime NOT NULL,\n",
    "    `open` float NULL,\n",
    "    `high` float NULL,\n",
    "    `low` float NULL,\n",
    "    `close` float NULL,\n",
    "    `volume` float NULL,\n",
    "    `adj_open` float NULL,\n",
    "    `adj_high` float NULL,\n",
    "    `adj_low` float NULL,\n",
    "    `adj_close` float NULL,\n",
    "    `adj_volume` float NULL,\n",
    "    `split_ratio` float NULL,\n",
    "    `adj_factor` float NULL,\n",
    "    `change` float NULL,\n",
    "    `percent_change` float NULL,\n",
    "    `fifty_two_week_high` float NULL,\n",
    "    `fifty_two_week_low` float NULL,\n",
    "    `market_cap` bigint NULL,\n",
    "    `weighted_avg_shares_out` bigint NULL,\n",
    "    `intraperiod` boolean NOT NULL DEFAULT 0,\n",
    "    `last_updated_date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "    `last_corp_action_date` timestamp NULL,\n",
    "    PRIMARY KEY (key_id)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n",
    "    \"\"\"\n",
    "\n",
    "    mycursor.execute(create_price_data_history_table)\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "    print(\"The price_data_historical table is created in RDS.\")\n",
    "\n",
    "\n",
    "    # Push the final dataframe into SQL DB on AWS RDS.\n",
    "\n",
    "    df = df_price_history_complete_load.copy()\n",
    "\n",
    "    # Set SQLAlchemy database credentials.\n",
    "    creds = {'usr': rds_user,\n",
    "             'pwd': rds_password,\n",
    "             'hst': rds_host,\n",
    "             'prt': 3306,\n",
    "             'dbn': rds_database}\n",
    "\n",
    "    # MySQL conection string.\n",
    "    connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "    # Create sqlalchemy engine for MySQL connection.\n",
    "    engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "    # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "    chunk = int(len(df) / 1000)\n",
    "    df.to_sql(name='price_data_historical', \n",
    "                                          con=engine, \n",
    "                                          if_exists='replace', \n",
    "                                          chunksize=chunk,\n",
    "                                          index=False)\n",
    "\n",
    "    # Create indexes for ticker, figi and dates\n",
    "\n",
    "    mycursor.execute(\"CREATE INDEX idx_ticker ON price_data_historical (ticker(9));\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_figi ON price_data_historical (figi(12));\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_date ON price_data_historical (date);\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_update ON price_data_historical (last_update_date);\")\n",
    "    mycursor.execute(\"CREATE INDEX idx_corp_action ON price_data_historical (last_corp_action_date);\")\n",
    "\n",
    "    print(\"The price history data is loaded and the indexes are set.\")\n",
    "    rds_table_status = \"Done.\"\n",
    "    \n",
    "    return rds_table_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ETL process.\n",
    "\n",
    "assemble_bulk_history\n",
    "download_bulk_history()\n",
    "get_shares_out_data(df_price_history)\n",
    "join_dataframes(df_price_history, df_shares_out)\n",
    "push_data_to_S3(df_price_history_complete_load)\n",
    "create_and_fill_RDS_table(df_price_history_complete_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
